{
  "topic": "Differential Equations",
  "toc": [
    "Introduction to Ordinary Differential Equations",
    "Separation of Variables and First-Order Differential Equations",
    "Higher-Order Linear Differential Equations",
    "Systems of Differential Equations and Phase Plane Analysis",
    "Numerical Methods for Solving Differential Equations, including Euler's Method and Runge-Kutta Methods"
  ],
  "resources": {
    "Introduction to Ordinary Differential Equations": [
      "https://tutorial.math.lamar.edu/classes/de/de.aspx",
      "https://www.math.purdue.edu/~torresm/ordinary-differential-equations.html",
      "https://web.uvic.ca/~tbazett/diffyqs/frontmatter-1.html"
    ],
    "Separation of Variables and First-Order Differential Equations": [
      "https://tutorial.math.lamar.edu/classes/de/separable.aspx",
      "https://www.sfu.ca/math-coursenotes/Math%20158%20Course%20Notes/sec_first_order_homogeneous_linear.html",
      "https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/core-mathematics/calculus/separable-first-order-differential-equations.html"
    ],
    "Higher-Order Linear Differential Equations": [
      "https://math.libretexts.org/Bookshelves/Differential_Equations/Differential_Equations_for_Engineers_(Lebl)/2%3A_Higher_order_linear_ODEs/2.3%3A_Higher_order_linear_ODEs",
      "https://tutorial.math.lamar.edu/classes/de/IntroHigherOrder.aspx",
      "https://tutorial.math.lamar.edu/classes/de/de.aspx"
    ],
    "Systems of Differential Equations and Phase Plane Analysis": [
      "https://tutorial.math.lamar.edu/classes/de/phaseplane.aspx",
      "https://faculty.sfasu.edu/judsontw/ode/html-snapshot/linear03.html",
      "https://en.wikipedia.org/wiki/Phase_plane"
    ],
    "Numerical Methods for Solving Differential Equations, including Euler's Method and Runge-Kutta Methods": [
      "https://onlinehw.math.ksu.edu/math340book/chap1/xc1.php",
      "https://web.mit.edu/10.001/Web/Course_Notes/Differential_Equations_Notes/node5.html",
      "https://lpsa.swarthmore.edu/NumInt/NumIntFirst.html"
    ]
  },
  "documents": {
    "Introduction to Ordinary Differential Equations": "Processing math: 20%\nPaul's Online Notes\nNOTES  QUICK NAV  DOWNLOAD  \n\nHome / Differential Equations\n Notes  Next Section \n\u00a0\nDIFFERENTIAL EQUATIONS\nHere are my notes for my differential equations course that I teach here at Lamar University. Despite the fact that these are my \u201cclass notes\u201d, they should be accessible to anyone wanting to learn how to solve differential equations or needing a refresher on differential equations.\nI\u2019ve tried to make these notes as self-contained as possible and so all the information needed to read through them is either from a Calculus or Algebra class or contained in other sections of the notes.\nHere are a couple of warnings to my students who may be here to get a copy of what happened on a day that you missed.\n\nBecause I wanted to make this a fairly complete set of notes for anyone wanting to learn differential equations have included some material that I do not usually have time to cover in class and because this changes from semester to semester it is not noted here. You will need to find one of your fellow class mates to see if there is something in these notes that wasn\u2019t covered in class.\n\n\nIn general, I try to work problems in class that are different from my notes. However, with Differential Equation many of the problems are difficult to make up on the spur of the moment and so in this class my class work will follow these notes fairly close as far as worked problems go. With that being said I will, on occasion, work problems off the top of my head when I can to provide more examples than just those in my notes. Also, I often don\u2019t have time in class to work all of the problems in the notes and so you will find that some sections contain problems that weren\u2019t worked in class due to time restrictions.\n\n\nSometimes questions in class will lead down paths that are not covered here. I try to anticipate as many of the questions as possible when writing these up, but the reality is that I can\u2019t anticipate all the questions. Sometimes a very good question gets asked in class that leads to insights that I\u2019ve not included here. You should always talk to someone who was in class on the day you missed and compare these notes to their notes and see what the differences are.\n\n\nThis is somewhat related to the previous three items, but is important enough to merit its own item. THESE NOTES ARE NOT A SUBSTITUTE FOR ATTENDING CLASS!! Using these notes as a substitute for class is liable to get you in trouble. As already noted not everything in these notes is covered in class and often material or insights not in these notes is covered in class.\n\nHere is a listing (and brief description) of the material that is in this set of notes.\n\nBasic Concepts - In this chapter we introduce many of the basic concepts and definitions that are encountered in a typical differential equations course. We will also take a look at direction fields and how they can be used to determine some of the behavior of solutions to differential equations.\nDefinitions \u2013 In this section some of the common definitions and concepts in a differential equations course are introduced including order, linear vs. nonlinear, initial conditions, initial value problem and interval of validity.\nDirection Fields \u2013 In this section we discuss direction fields and how to sketch them. We also investigate how direction fields can be used to determine some information about the solution to a differential equation without actually having the solution.\nFinal Thoughts \u2013 In this section we give a couple of final thoughts on what we will be looking at throughout this course.\n\nFirst Order Differential Equations - In this chapter we will look at several of the standard solution methods for first order differential equations including linear, separable, exact and Bernoulli differential equations. We also take a look at intervals of validity, equilibrium solutions and Euler\u2019s Method. In addition we model some physical situations with first order differential equations.\nLinear Equations \u2013 In this section we solve linear first order differential equations, i.e. differential equations in the form y\u2032+p(t)y=g(t). We give an in depth overview of the process used to solve this type of differential equation as well as a derivation of the formula needed for the integrating factor used in the solution process.\nSeparable Equations \u2013 In this section we solve separable first order differential equations, i.e. differential equations in the form N(y)y\u2032=M(x). We will give a derivation of the solution process to this type of differential equation. We\u2019ll also start looking at finding the interval of validity for the solution to a differential equation.\nExact Equations \u2013 In this section we will discuss identifying and solving exact differential equations. We will develop a test that can be used to identify exact differential equations and give a detailed explanation of the solution process. We will also do a few more interval of validity problems here as well.\nBernoulli Differential Equations \u2013 In this section we solve Bernoulli differential equations, i.e. differential equations in the form y\u2032+p(t)y=yn. This section will also introduce the idea of using a substitution to help us solve differential equations.\nSubstitutions \u2013 In this section we\u2019ll pick up where the last section left off and take a look at a couple of other substitutions that can be used to solve some differential equations. In particular we will discuss using solutions to solve differential equations of the form y\u2032=F(\ny\n\n\nx\n) and y\u2032=G(ax+by).\nIntervals of Validity \u2013 In this section we will give an in depth look at intervals of validity as well as an answer to the existence and uniqueness question for first order differential equations.\nModeling with First Order Differential Equations \u2013 In this section we will use first order differential equations to model physical situations. In particular we will look at mixing problems (modeling the amount of a substance dissolved in a liquid and liquid both enters and exits), population problems (modeling a population under a variety of situations in which the population can enter or exit) and falling objects (modeling the velocity of a falling object under the influence of both gravity and air resistance).\nEquilibrium Solutions \u2013 In this section we will define equilibrium solutions (or equilibrium points) for autonomous differential equations, y\u2032=f(y). We discuss classifying equilibrium solutions as asymptotically stable, unstable or semi-stable equilibrium solutions.\nEuler\u2019s Method \u2013 In this section we\u2019ll take a brief look at a fairly simple method for approximating solutions to differential equations. We derive the formulas used by Euler\u2019s Method and give a brief discussion of the errors in the approximations of the solutions.\n\nSecond Order Differential Equations - In this chapter we will start looking at second order differential equations. We will concentrate mostly on constant coefficient second order differential equations. We will derive the solutions for homogeneous differential equations and we will use the methods of undetermined coefficients and variation of parameters to solve non homogeneous differential equations. In addition, we will discuss reduction of order, fundamentals of sets of solutions, Wronskian and mechanical vibrations.\nBasic Concepts \u2013 In this section give an in depth discussion on the process used to solve homogeneous, linear, second order differential equations, ay\u2033. We derive the characteristic polynomial and discuss how the Principle of Superposition is used to get the general solution.\nReal Roots \u2013 In this section we discuss the solution to homogeneous, linear, second order differential equations, ay'' + by' + cy = 0, in which the roots of the characteristic polynomial, ar^{2} + br + c = 0, are real distinct roots.\nComplex Roots \u2013 In this section we discuss the solution to homogeneous, linear, second order differential equations, ay'' + by' + cy = 0, in which the roots of the characteristic polynomial, ar^{2} + br + c = 0, are real distinct roots. We will also derive from the complex roots the standard solution that is typically used in this case that will not involve complex numbers.\nRepeated Roots \u2013 In this section we discuss the solution to homogeneous, linear, second order differential equations, ay'' + by' + cy = 0, in which the roots of the characteristic polynomial, ar^{2} + br + c = 0, are repeated, i.e. double, roots. We will use reduction of order to derive the second solution needed to get a general solution in this case.\nReduction of Order \u2013 In this section we will discuss reduction of order, the process used to derive the solution to the repeated roots case for homogeneous linear second order differential equations, in greater detail. This will be one of the few times in this chapter that non-constant coefficient differential equation will be looked at.\nFundamental Sets of Solutions \u2013 In this section we will a look at some of the theory behind the solution to second order differential equations. We define fundamental sets of solutions and discuss how they can be used to get a general solution to a homogeneous second order differential equation. We will also define the Wronskian and show how it can be used to determine if a pair of solutions are a fundamental set of solutions.\nMore on the Wronskian \u2013 In this section we will examine how the Wronskian, introduced in the previous section, can be used to determine if two functions are linearly independent or linearly dependent. We will also give and an alternate method for finding the Wronskian.\nNonhomogeneous Differential Equations \u2013 In this section we will discuss the basics of solving nonhomogeneous differential equations. We define the complimentary and particular solution and give the form of the general solution to a nonhomogeneous differential equation.\nUndetermined Coefficients \u2013 In this section we introduce the method of undetermined coefficients to find particular solutions to nonhomogeneous differential equation. We work a wide variety of examples illustrating the many guidelines for making the initial guess of the form of the particular solution that is needed for the method.\nVariation of Parameters \u2013 In this section we introduce the method of variation of parameters to find particular solutions to nonhomogeneous differential equation. We give a detailed examination of the method as well as derive a formula that can be used to find particular solutions.\nMechanical Vibrations \u2013 In this section we will examine mechanical vibrations. In particular we will model an object connected to a spring and moving up and down. We also allow for the introduction of a damper to the system and for general external forces to act on the object. Note as well that while we example mechanical vibrations in this section a simple change of notation (and corresponding change in what the quantities represent) can move this into almost any other engineering field.\n\nLaplace Transforms - In this chapter we introduce Laplace Transforms and how they are used to solve Initial Value Problems. With the introduction of Laplace Transforms we will now be able to solve some Initial Value Problems that we wouldn\u2019t be able to solve otherwise. We will solve differential equations that involve Heaviside and Dirac Delta functions. We will also give brief overview on using Laplace transforms to solve nonconstant coefficient differential equations. In addition, we will define the convolution integral and show how it can be used to take inverse transforms.\nThe Definition \u2013 In this section we give the definition of the Laplace transform. We will also compute a couple Laplace transforms using the definition.\nLaplace Transforms \u2013 In this section we introduce the way we usually compute Laplace transforms that avoids needing to use the definition. We discuss the table of Laplace transforms used in this material and work a variety of examples illustrating the use of the table of Laplace transforms.\nInverse Laplace Transforms \u2013 In this section we ask the opposite question from the previous section. In other words, given a Laplace transform, what function did we originally have? We again work a variety of examples illustrating how to use the table of Laplace transforms to do this as well as some of the manipulation of the given Laplace transform that is needed in order to use the table.\nStep Functions \u2013 In this section we introduce the step or Heaviside function. We illustrate how to write a piecewise function in terms of Heaviside functions. We also work a variety of examples showing how to take Laplace transforms and inverse Laplace transforms that involve Heaviside functions. We also derive the formulas for taking the Laplace transform of functions which involve Heaviside functions.\nSolving IVPs' with Laplace Transforms - In this section we will examine how to use Laplace transforms to solve IVP\u2019s. The examples in this section are restricted to differential equations that could be solved without using Laplace transform. The advantage of starting out with this type of differential equation is that the work tends to be not as involved and we can always check our answers if we wish to.\nNonconstant Coefficient IVP\u2019s \u2013 In this section we will give a brief overview of using Laplace transforms to solve some nonconstant coefficient IVP\u2019s. We do not work a great many examples in this section. We only work a couple to illustrate how the process works with Laplace transforms.\nIVP\u2019s with Step Functions \u2013 This is the section where the reason for using Laplace transforms really becomes apparent. We will use Laplace transforms to solve IVP\u2019s that contain Heaviside (or step) functions. Without Laplace transforms solving these would involve quite a bit of work. While we do work one of these examples without Laplace transforms, we do it only to show what would be involved if we did try to solve one of the examples without using Laplace transforms.\nDirac Delta Function \u2013 In this section we introduce the Dirac Delta function and derive the Laplace transform of the Dirac Delta function. We work a couple of examples of solving differential equations involving Dirac Delta functions and unlike problems with Heaviside functions our only real option for this kind of differential equation is to use Laplace transforms. We also give a nice relationship between Heaviside and Dirac Delta functions.\nConvolution Integral \u2013 In this section we give a brief introduction to the convolution integral and how it can be used to take inverse Laplace transforms. We also illustrate its use in solving a differential equation in which the forcing function (i.e. the term without any y\u2019s in it) is not known.\nTable of Laplace Transforms \u2013 This section is the table of Laplace Transforms that we\u2019ll be using in the material. We give as wide a variety of Laplace transforms as possible including some that aren\u2019t often given in tables of Laplace transforms.\n\nSystems of Differential Equations - In this chapter we will look at solving systems of differential equations. We will restrict ourselves to systems of two linear differential equations for the purposes of the discussion but many of the techniques will extend to larger systems of linear differential equations. We also examine sketch phase planes/portraits for systems of two differential equations. In addition, we give brief discussions on using Laplace transforms to solve systems and some modeling that gives rise to systems of differential equations.\nReview : Systems of Equations \u2013 In this section we will give a review of the traditional starting point for a linear algebra class. We will use linear algebra techniques to solve a system of equations as well as give a couple of useful facts about the number of solutions that a system of equations can have.\nReview : Matrices and Vectors \u2013 In this section we will give a brief review of matrices and vectors. We will look at arithmetic involving matrices and vectors, finding the inverse of a matrix, computing the determinant of a matrix, linearly dependent/independent vectors and converting systems of equations into matrix form.\nReview : Eigenvalues and Eigenvectors \u2013 In this section we will introduce the concept of eigenvalues and eigenvectors of a matrix. We define the characteristic polynomial and show how it can be used to find the eigenvalues for a matrix. Once we have the eigenvalues for a matrix we also show how to find the corresponding eigenvalues for the matrix.\nSystems of Differential Equations \u2013 In this section we will look at some of the basics of systems of differential equations. We show how to convert a system of differential equations into matrix form. In addition, we show how to convert an n^{ \\text{th}} order differential equation into a system of differential equations.\nSolutions to Systems \u2013 In this section we will a quick overview on how we solve systems of differential equations that are in matrix form. We also define the Wronskian for systems of differential equations and show how it can be used to determine if we have a general solution to the system of differential equations.\nPhase Plane \u2013 In this section we will give a brief introduction to the phase plane and phase portraits. We define the equilibrium solution/point for a homogeneous system of differential equations and how phase portraits can be used to determine the stability of the equilibrium solution. We also show the formal method of how phase portraits are constructed.\nReal Eigenvalues \u2013 In this section we will solve systems of two linear differential equations in which the eigenvalues are distinct real numbers. We will also show how to sketch phase portraits associated with real distinct eigenvalues (saddle points and nodes).\nComplex Eigenvalues \u2013 In this section we will solve systems of two linear differential equations in which the eigenvalues are complex numbers. This will include illustrating how to get a solution that does not involve complex numbers that we usually are after in these cases. We will also show how to sketch phase portraits associated with complex eigenvalues (centers and spirals).\nRepeated Eigenvalues \u2013 In this section we will solve systems of two linear differential equations in which the eigenvalues are real repeated (double in this case) numbers. This will include deriving a second linearly independent solution that we will need to form the general solution to the system. We will also show how to sketch phase portraits associated with real repeated eigenvalues (improper nodes).\nNonhomogeneous Systems \u2013 In this section we will work quick examples illustrating the use of undetermined coefficients and variation of parameters to solve nonhomogeneous systems of differential equations. The method of undetermined coefficients will work pretty much as it does for nth order differential equations, while variation of parameters will need some extra derivation work to get a formula/process we can use on systems.\nLaplace Transforms \u2013 In this section we will work a quick example illustrating how Laplace transforms can be used to solve a system of two linear differential equations.\nModeling \u2013 In this section we\u2019ll take a quick look at some extensions of some of the modeling we did in previous chapters that lead to systems of differential equations. In particular we will look at mixing problems in which we have two interconnected tanks of water, a predator-prey problem in which populations of both are taken into account and a mechanical vibration problem with two masses, connected with a spring and each connected to a wall with a spring.\n\nSeries Solutions to Differential Equations - In this chapter we are going to take a quick look at how to represent the solution to a differential equation with a power series. We will also look at how to solve Euler\u2019s differential equation. In addition, we will do a quick review of power series and Taylor series to help with work in the chapter.\nReview : Power Series \u2013 In this section we give a brief review of some of the basics of power series. Included are discussions of using the Ratio Test to determine if a power series will converge, adding/subtracting power series, differentiating power series and index shifts for power series.\nReview : Taylor Series \u2013 In this section we give a quick reminder on how to construct the Taylor series for a function. Included are derivations for the Taylor series of {\\bf e}^{x} and \\cos(x) about x = 0 as well as showing how to write down the Taylor series for a polynomial.\nSeries Solutions \u2013 In this section we define ordinary and singular points for a differential equation. We also show who to construct a series solution for a differential equation about an ordinary point. The method illustrated in this section is useful in solving, or at least getting an approximation of the solution, differential equations with coefficients that are not constant.\nEuler Equations \u2013 In this section we will discuss how to solve Euler\u2019s differential equation, ax^{2}y'' + b x y' +c y = 0. Note that while this does not involve a series solution it is included in the series solution chapter because it illustrates how to get a solution to at least one type of differential equation at a singular point.\n\nHigher Order Differential Equations - In this chapter we will look at extending many of the ideas of the previous chapters to differential equations with order higher that 2nd order. In a few cases this will simply mean working an example to illustrate that the process doesn\u2019t really change, but in most cases there are some issues to discuss.\nBasic Concepts for n^{\\text{th}} Order Linear Equations \u2013 In this section we\u2019ll start the chapter off with a quick look at some of the basic ideas behind solving higher order linear differential equations. Included will be updated definitions/facts for the Principle of Superposition, linearly independent functions and the Wronskian.\nLinear Homogeneous Differential Equations \u2013 In this section we will extend the ideas behind solving 2nd order, linear, homogeneous differential equations to higher order. As we\u2019ll most of the process is identical with a few natural extensions to repeated real roots that occur more than twice. We will also need to discuss how to deal with repeated complex roots, which are now a possibility. In addition, we will see that the main difficulty in the higher order cases is simply finding all the roots of the characteristic polynomial.\nUndetermined Coefficients \u2013 In this section we work a quick example to illustrate that using undetermined coefficients on higher order differential equations is no different that when we used it on 2nd order differential equations with only one small natural extension.\nVariation of Parameters \u2013 In this section we will give a detailed discussion of the process for using variation of parameters for higher order differential equations. We will also develop a formula that can be used in these cases. We will also see that the work involved in using variation of parameters on higher order differential equations can be quite involved on occasion.\nLaplace Transforms \u2013 In this section we will work a quick example using Laplace transforms to solve a differential equation on a 3rd order differential equation just to say that we looked at one with order higher than 2nd. As we\u2019ll see, outside of needing a formula for the Laplace transform of y''', which we can get from the general formula, there is no real difference in how Laplace transforms are used for higher order differential equations.\nSystems of Differential Equations \u2013 In this section we\u2019ll take a quick look at extending the ideas we discussed for solving 2 \\times 2 systems of differential equations to systems of size 3 \\times 3. As we will see they are mostly just natural extensions of what we already know how to do. We will also make a couple of quick comments about 4 \\times 4 systems.\nSeries Solutions \u2013 In this section we are going to work a quick example illustrating that the process of finding series solutions for higher order differential equations is pretty much the same as that used on 2nd order differential equations.\n\nBoundary Value Problems & Fourier Series - In this chapter we will introduce two topics that are integral to basic partial differential equations solution methods. The first topic, boundary value problems, occur in pretty much every partial differential equation. The second topic, Fourier series, is what makes one of the basic solution techniques work.\nBoundary Value Problems \u2013 In this section we\u2019ll define boundary conditions (as opposed to initial conditions which we should already be familiar with at this point) and the boundary value problem. We will also work a few examples illustrating some of the interesting differences in using boundary values instead of initial conditions in solving differential equations.\nEigenvalues and Eigenfunctions \u2013 In this section we will define eigenvalues and eigenfunctions for boundary value problems. We will work quite a few examples illustrating how to find eigenvalues and eigenfunctions. In one example the best we will be able to do is estimate the eigenvalues as that is something that will happen on a fairly regular basis with these kinds of problems.\nPeriodic Functions and Orthogonal Functions \u2013 In this section we will define periodic functions, orthogonal functions and mutually orthogonal functions. We will also work a couple of examples showing intervals on which \\cos\\left(\\frac{n \\pi x}{L}\\right) and \\sin\\left(\\frac{n \\pi x}{L}\\right) are mutually orthogonal. The results of these examples will be very useful for the rest of this chapter and most of the next chapter.\nFourier Sine Series \u2013 In this section we define the Fourier Sine Series, i.e. representing a function with a series in the form \\sum\\limits_{n = 1}^\\infty {{B_n}\\sin \\left( {\\frac{{n\\pi x}}{L}} \\right)} . We will also define the odd extension for a function and work several examples finding the Fourier Sine Series for a function.\nFourier Cosine Series \u2013 In this section we define the Fourier Cosine Series, i.e. representing a function with a series in the form \\sum\\limits_{n = 0}^\\infty {{A_n}\\cos \\left( {\\frac{{n\\pi x}}{L}} \\right)} . We will also define the even extension for a function and work several examples finding the Fourier Cosine Series for a function.\nFourier Series \u2013 In this section we define the Fourier Series, i.e. representing a function with a series in the form \\sum\\limits_{n = 0}^\\infty {{A_n}\\cos \\left( {\\frac{{n\\pi x}}{L}} \\right)} + \\sum\\limits_{n = 1}^\\infty {{B_n}\\sin \\left( {\\frac{{n\\pi x}}{L}} \\right)} . We will also work several examples finding the Fourier Series for a function.\nConvergence of Fourier Series \u2013 In this section we will define piecewise smooth functions and the periodic extension of a function. In addition, we will give a variety of facts about just what a Fourier series will converge to and when we can expect the derivative or integral of a Fourier series to converge to the derivative or integral of the function it represents.\n\nPartial Differential Equations - In this chapter we introduce Separation of Variables one of the basic solution techniques for solving partial differential equations. Included are partial derivations for the Heat Equation and Wave Equation. In addition, we give solutions to examples for the heat equation, the wave equation and Laplace\u2019s equation.\nThe Heat Equation \u2013 In this section we will do a partial derivation of the heat equation that can be solved to give the temperature in a one dimensional bar of length L. In addition, we give several possible boundary conditions that can be used in this situation. We also define the Laplacian in this section and give a version of the heat equation for two or three dimensional situations.\nThe Wave Equation \u2013 In this section we do a partial derivation of the wave equation which can be used to find the one dimensional displacement of a vibrating string. In addition, we also give the two and three dimensional version of the wave equation.\nTerminology \u2013 In this section we take a quick look at some of the terminology we will be using in the rest of this chapter. In particular we will define a linear operator, a linear partial differential equation and a homogeneous partial differential equation. We also give a quick reminder of the Principle of Superposition.\nSeparation of Variables \u2013 In this section show how the method of Separation of Variables can be applied to a partial differential equation to reduce the partial differential equation down to two ordinary differential equations. We apply the method to several partial differential equations. We do not, however, go any farther in the solution process for the partial differential equations. That will be done in later sections. The point of this section is only to illustrate how the method works.\nSolving the Heat Equation \u2013 In this section we go through the complete separation of variables process, including solving the two ordinary differential equations the process generates. We will do this by solving the heat equation with three different sets of boundary conditions. Included is an example solving the heat equation on a bar of length L but instead on a thin circular ring.\nHeat Equation with Non-Zero Temperature Boundaries \u2013 In this section we take a quick look at solving the heat equation in which the boundary conditions are fixed, non-zero temperature. Note that this is in contrast to the previous section when we generally required the boundary conditions to be both fixed and zero.\nLaplace\u2019s Equation \u2013 In this section we discuss solving Laplace\u2019s equation. As we will see this is exactly the equation we would need to solve if we were looking to find the equilibrium solution (i.e. time independent) for the two dimensional heat equation with no sources. We will also convert Laplace\u2019s equation to polar coordinates and solve it on a disk of radius a.\nVibrating String \u2013 In this section we solve the one dimensional wave equation to get the displacement of a vibrating string.\nSummary of Separation of Variables \u2013 In this final section we give a quick summary of the method of separation of variables for solving partial differential equations.\n\n\n[Contact Me]\u00a0[Privacy Statement]\u00a0[Site Help & FAQ]\u00a0[Terms of Use]\n\u00a9 2003 - 2025 Paul Dawkins\nPage Last Modified : 6/26/2023\n\nHome\nPublications\nModern Real Analysis, Second Edition\u00a0\nLecture Notes: Introduction to Geometric Measure Theory\u00a0\nLecture Notes: Lebesgue Theory of Integration\u00a0\nLecture Notes: Topics in Analysis\u00a0\nLectures Notes: Topics in Vector Calculus\u00a0\nLectures Notes: Ordinary Differential Equations\u00a0\nLecture Notes: Introduction to Real Analysis\nLectures Notes: Ordinary Differential Equations\n\nClass Policies\n\n\u00a0Midterm 1: Wednesday February 19, 2025. Time: 8:00-9:00 p.m. Location: HAMP 1252\n\n\u00a0Miderm 2: Tuesday April 8, 2025. Time: 8:00-9:00 p.m. Location: LWSN B151\n\n\u00a0Office Hours: Wednesday 1:30-2:30 p.m. or by appointment\n\n\u00a0\n\nSection 1.1: Direction Fields\u00a0\n\nSection 1.2: Solutions of some differential equations\n\nSection 1.3: Linear versus nonlinear ordinary differential equations\n\nSection 2.1: Method of integrating factors to solve first order liner ODE\n\nSection 2.2: Separable equations and homegeneous equations\n\nSection 2.3: Modeling with first order differential equations\n\nSection 2.3: Modeling with first order differential equations (Continuation)\n\nSection 2.4: Existence theorems for first order linear and nonlinear equations\n\nSection 2.5: Analysis of autonomous equations\n\nSection 2.6: Method of exact equations\n\nSection 2.7: Method of the tangent line (Euler's method)\n\nSection 3.1: Second order linear homogeneous equations with constant coefficients\n\nSection 3.2: Solutions of linear homogeneous second order equations: the Wronskian\n\nSection 3.3: Complex roots of characteristic equation\n\nSection 3.4: Repeated roots; method of reduction of order\n\nSection 3.5: Non-homogeneous equations and the method of undetermined coefficients\n\nSection 3.5: Non-homogeneous equations and the method of undetermined coefficients (continuation)\n\nSection 3.6: Variation of Parameters\n\nSection 3.7: Mechanical and electrical vibrations\n\nSection 3.8: Forced Vibrations\n\nSections 4.1 and 4.2: Higher order, liner ODE\n\nSection 4.3: Method of undetermined coefficients for higher order, liner, constant coefficents, non-homegeneous equations\n\nSection 6.1: Definition of Laplace Transform\n\nSection 6.2: Solution of initial value problems\n\nSection 6.3: Step functions\n\nSection 6.4: More examples of differential equation of discontinuous forcing term\n\nSection 6.5: Impulse functions\n\nSection 6.6: The convolution integral\n\nSection 7.1: Introduction to systems of first order linear equations\n\nSection 7.2: Review of matrices\n\nSections 7.3 and 7.5: Eigenvalues and eigenvectors. System of equations: case 1 (different and real eigenvalues\n\nSection 7.6: Case 2: Complex eigenvalues\n\nSection 7.8: Case 3: Repeated eigenvalues\n\nSection 7.9: Case 3: Non-homogeneous systems\n\nHW 1: Due on Friday January 24\n\n1.1: 1, 11, 16, 18\n\n1.2: 1(a), 7, 8, 12\n\n1.3: 1, 2, 3, 4, 8, 10, 12, 14\n\n2.1: 2, 13, 21\n\nHW 2: Due on Friday January 31\n\n2.1: 22, 23\n\n2.2: 1, 4, 9, 11, 22, 27, 29\n\n2.3: 2, 3, 11, 13\n\nHW 3: Due on Friday February 7\n\n2.4: 1, 3, 7, 8, 18, 23\n\n2.5: 2, 4, 9\n\n2.6: 1, 2, 5\n\nHW 4: Due on Friday February 14\n\n2.6: 7, 14, 15, 19\n\n2.7: 12, 15\n\nHW 5: Due on Monday February 24\n\n3.1: 1, 15, 16, 17, 21\n\n3.2: 4, 8, 10, 11, 13, 14, 16, 20, 22\n\nHW 6: Due on Friday February 28\n\n3.3: 3, 5, 14, 18, 23\n\n3.4: 4, 15, 19, 24, 25\n\nHw 7: Due on Friday March 7\n\n3.5: 1, 3, 5, 13, 16(a), 18(a), 19(a), 21(a)\n\n3.6: 2, 8, 11, 13\n\nHW 8: Due on Friday March 21\n\n3.7: 3, 4, 9, 13\n\n3.8: 4, 5(a), 13\n\n4.1: 3, 4, 11\n\n4.2: 9, 13, 16, 17, 23\n\n4.3: 1, 3, 7 , 10, 11, 12\n\n\n\n\nCopyright\u00a9 2022, Purdue University, all rights reserved.\nWest Lafayette, IN 47907 USA, 765-494-1901\nAn equal access/equal opportunity university\n\nDepartment of Mathematics, Purdue University\n150 N. University Street, West Lafayette, IN 47907\nIf you have trouble accessing this page because of a disability, please contact the web manager.\n\nSkip to main content\nIntroduction to Differential Equations\n\nAdapted for Math 204 at the University of Victoria\n\n\u2630Contents\nIndex\n<\nPrev\n^\nUp\nNext\n>\n\t\n\t\n\ud83d\udd0d\nFront Matter\nColophon\nAbstract\n0\nIntroduction\nAbout this book\nWhat are Differential Equations?\nClassification of differential equations\n1\nFirst order equations\nIntegrals as solutions\nSlope fields\nSeparable equations\nLinear equations and the integrating factor\nSubstitution\nAutonomous equations\nNumerical methods: Euler\u2019s method\nExact equations\nFirst order linear PDE\n2\nHigher order linear ODEs\nSecond order linear ODEs\nConstant coefficient second order linear ODEs\nHigher order linear ODEs\nMechanical vibrations\nNonhomogeneous equations\nForced oscillations and resonance\n3\nThe Laplace transform\nThe Laplace transform\nTransforms of derivatives and ODEs\nConvolution\nDirac delta and impulse response\nSolving PDEs with the Laplace transform\n4\nPower series methods\nPower series\nSeries solutions of linear second order ODEs\nSingular points and the method of Frobenius\n5\nFourier series and PDEs\nBoundary value problems\nThe trigonometric series\nMore on the Fourier series\nSine and cosine series\nApplications of Fourier series\nPDEs, separation of variables, and the heat equation\nOne-dimensional wave equation\nD\u2019Alembert solution of the wave equation\nSteady state temperature and the Laplacian\nDirichlet problem in the circle and the Poisson kernel\n6\nMore on eigenvalue problems\nSturm\u2013Liouville problems\nHigher order eigenvalue problems\nSteady periodic solutions\n7\nSystems of ODEs\nIntroduction to systems of ODEs\nMatrices and linear systems\nLinear systems of ODEs\nEigenvalue method\nTwo-dimensional systems and their vector fields\nSecond order systems and applications\nMultiple eigenvalues\nMatrix exponentials\nNonhomogeneous systems\nBack Matter\nLinear algebra\nTable of Laplace Transforms\nFurther Reading\nIndex\nIntroduction to Differential Equations\nAdapted for Math 204 at the University of Victoria\nAbstractThis book consists of an introduction to Differential Equations, primarily focusing on Ordinary Differential Equations (ODEs). It is used specifically in Math 204 at the University of Victoria, but covers a fairly typical one-semester introductory course for students who have taken Calculus.\nThis book is free and open source, which means that anyone can modify the source code for this book, please see the Colophon for more information. Specicfically, this version of the book, maintained by Trefor Bazett at the University of Victoria, has been forked from the original text by Ji\u0159\u00ed Lebl.\u20096\u2009\nWhat are differential equations? Check out the first video from the book for an overview!\nColophon\n<\nPrev\n^\nTop\nNext\n>",
    "Separation of Variables and First-Order Differential Equations": "Loading [MathJax]/jax/output/CommonHTML/fonts/TeX/fontdata.js\nPaul's Online Notes\nNOTES  QUICK NAV  DOWNLOAD  \n\nHome / Differential Equations / First Order DE's / Separable Equations\n Prev. Section  Notes  Next Section \n\u00a0\nSection 2.2 : Separable Equations\n\nWe are now going to start looking at nonlinear first order differential equations. The first type of nonlinear first order differential equations that we will look at is separable differential equations.\n\nA separable differential equation is any differential equation that we can write in the following form.\n\nN(y)\ndy\n\n\ndx\n=M(x)\n\nNote that in order for a differential equation to be separable all the y's in the differential equation must be multiplied by the derivative and all the x's in the differential equation must be on the other side of the equal sign.\n\nTo solve this differential equation we first integrate both sides with respect to x to get,\n\n\u222bN(y)\ndy\n\n\ndx\ndx=\u222bM(x)dx\n\nNow, remember that y is really y(x) and so we can use the following substitution,\n\nu=y(x)du=y\u2032(x)dx=\ndy\n\n\ndx\ndx\n\nApplying this substitution to the integral we get,\n\n\u222bN(u)du=\u222bM(x)dx\n\nAt this point we can (hopefully) integrate both sides and then back substitute for the u on the left side. Note, that as implied in the previous sentence, it might not actually be possible to evaluate one or both of the integrals at this point. If that is the case, then there won\u2019t be a lot we can do to proceed using this method to solve the differential equation.\n\nNow, the process above is the mathematically correct way of solving this differential equation. Note however, that if we \u201cseparate\u201d the derivative as well we can write the differential equation as,\n\nN(y)dy=M(x)dx\n\nWe obviously can\u2019t separate the derivative like that, but let\u2019s pretend we can for a bit and we\u2019ll see that we arrive at the answer with less work.\n\nNow we integrate both sides of this to get,\n\n\u222bN(y)dy=\u222bM(x)dx\n\nSo, if we compare (2) and (3) we can see that the only difference is on the left side and even then the only real difference is (2) has the integral in terms of u and (3) has the integral in terms of y. Outside of that there is no real difference. The integral on the left is exactly the same integral in each equation. The only difference is the letter used in the integral. If we integrate (2) and then back substitute in for u we would arrive at the same thing as if we\u2019d just integrated (3) from the start.\n\nTherefore, to make the work go a little easier, we\u2019ll just use (3) to find the solution to the differential equation. Also, after doing the integrations, we will have an implicit solution that we can hopefully solve for the explicit solution, y(x). Note that it won't always be possible to solve for an explicit solution.\n\nRecall from the Definitions section that an implicit solution is a solution that is not in the form y=y(x) while an explicit solution has been written in that form.\n\nWe will also have to worry about the interval of validity for many of these solutions. Recall that the interval of validity was the range of the independent variable, x in this case, on which the solution is valid. In other words, we need to avoid division by zero, complex numbers, logarithms of negative numbers or zero, etc. Most of the solutions that we will get from separable differential equations will not be valid for all values of x.\n\nLet\u2019s start things off with a fairly simple example so we can see the process without getting lost in details of the other issues that often arise with these problems.\n\nExample 1 Solve the following differential equation and determine the interval of validity for the solution.\ndy\n\n\ndx\n=6y2xy(1)=\n1\n\n\n25\nShow Solution \nExample 2 Solve the following IVP and find the interval of validity for the solution.\ny\u2032=\n3x2+4x\u22124\n\n\n2y\u22124\ny(1)=3\nShow Solution \nExample 3 Solve the following IVP and find the interval of validity of the solution.\ny\u2032=\nxy3\n\n\n\u221a\n1+x2\ny(0)=\u22121\nShow Solution \nExample 4 Solve the following IVP and find the interval of validity of the solution.\ny\u2032=e\u2212y(2x\u22124)y(5)=0\nShow Solution \nExample 5 Solve the following IVP and find the interval of validity for the solution.\ndr\n\n\nd\u03b8\n=\nr2\n\n\n\u03b8\nr(1)=2\nShow Solution \nExample 6 Solve the following IVP.\ndy\n\n\ndt\n=ey\u2212tsec(y)(1+t2)y(0)=0\nShow Solution \n\nAs this last example showed it is not always possible to find explicit solutions so be on the lookout for those cases.\n\n[Contact Me]\u00a0[Privacy Statement]\u00a0[Site Help & FAQ]\u00a0[Terms of Use]\n\u00a9 2003 - 2025 Paul Dawkins\nPage Last Modified : 2/6/2023\n\nLoading [MathJax]/jax/output/CommonHTML/fonts/TeX/fontdata.js\nSkip to main content\nCalculus Early Transcendentals: Integral & Multi-Variable Calculus for Social Sciences\n\nContents\nIndex\nCalc\nPrev\nUp\nNext\nFront Matter\nCopyright\nAcknowledgements\nOpen Educational Resources (OER) Support: Corrections and Suggestions\nDedication\nIntroduction\nProblem Solving Strategies\n1\nIntegration\nAntiderivatives\nDisplacement and Area\nRiemann Sums\nThe Definite Integral and FTC\nIndefinite Integrals\n2\nTechniques of Integration\nSubstitution Rule\nPowers of Trigonometric Functions\nTrigonometric Substitutions\nIntegration by Parts\nPartial Fraction Method for Rational Functions\nNumerical Integration\nImproper Integrals\nAdditional Exercises\n3\nApplications of Integration\nAverage Value and Area Revisited\nApplications to Business and Economics\nVolume of Revolution: Disk Method\nVolume of Revolution: Shell Method\n4\nMultiple Integration\nFunctions of Several Variables\nDouble Integrals: Volume and Average Value\nTriple Integrals: Volume and Average Value\nProbability\n5\nDifferential Equations\nClassifying Differential Equations\nFirst Order Differential Equations\nFirst Order Linear Differential Equations\nApproximation\n6\nSequences and Series\nSequences\nSeries\nIntegral Test\nAlternating Series\nComparison Test\nAbsolute and Conditional Convergence\nRatio and Root Tests\nPower Series and Polynomial Approximation\nBack Matter\nIndex\nAuthored in PreTeXt\n5.3 First Order Linear Differential Equations \u00b6\n5.3.1 Homogeneous DEs \u00b6\n\nA simple, but important and useful, type of separable equation is the first order homogeneous linear equation:\n\nDefinition 5.21. First Order Homogeneous Linear DE.  \n\nA first order homogeneous linear differential equation is one of the form \\ds y' + p(t)y=0 or equivalently \\ds y' = -p(t)y\\text{.}\n\nWe have already seen a first order homogeneous linear differential equation, namely the simple growth and decay model y'=ky\\text{.}\n\nSince first order homogeneous linear equations are separable, we can solve them in the usual way:\n\n\\begin{align*} y' \\amp = -p(t)y\\\\ \\int {1\\over y}\\,dy \\amp = \\int -p(t)\\,dt\\\\ \\ln|y| \\amp = P(t)+C\\\\ y\\amp = \\pm\\,e^{P(t)+C}\\\\ y\\amp = Ae^{P(t)}\\text{,} \\end{align*}\n\nwhere P(t) is an antiderivative of -p(t)\\text{.} As in previous examples, if we allow A=0 we get the constant solution y=0\\text{.}\n\nExample5.22. Solving an IVP I.\n\nSolve the initial value problem\n\n\\begin{equation*} \\ds y' + y\\cos t =0\\text{,} \\end{equation*}\n\nsubject to\n\ny(0)=1/2\n\ny(2)=1/2\n\nSolution\nExample5.23. Solving an IVP II.\n\nSolve the initial value problem ty'+3y=0\\text{,} y(1)=2\\text{,} assuming t>0\\text{.}\n\nSolution\n5.3.2 Non-Homogeneous DEs \u00b6\n\nAs you might guess, a first order non-homogeneous linear differential equation has the form \\ds y' + p(t)y = f(t)\\text{.} Not only is this closely related in form to the first order homogeneous linear equation, we can use what we know about solving homogeneous equations to solve the general linear equation.\n\nDefinition 5.24. First Order Non-Homogeneous Linear DE.  \n\nA first order non-homogeneous linear differential equation is one of the form\n\n\\begin{equation*} y' + p(t)y = f(t)\\text{.} \\end{equation*}\n\nNote: When the coefficient of the first derivative is one in the first order non-homogeneous linear differential equation as in the above definition, then we say the DE is in standard form.\n\nLet us now discuss how we can find all solutions to a first order non-homogeneous linear differential equation. Suppose that y_1(t) and y_2(t) are solutions to \\ds y' + p(t)y = f(t)\\text{.} Let \\ds g(t)=y_1-y_2\\text{.} Then\n\n\\begin{align*} g'(t)+p(t)g(t)\\amp = y_1'-y_2'+p(t)(y_1-y_2)\\\\ \\amp = (y_1'+p(t)y_1)-(y_2'+p(t)y_2)\\\\ \\amp = f(t)-f(t)=0\\text{.} \\end{align*}\n\nIn other words, \\ds g(t)=y_1-y_2 is a solution to the homogeneous equation \\ds y' + p(t)y = 0\\text{.} Turning this around, any solution to the linear equation \\ds y' + p(t)y = f(t)\\text{,} call it y_1\\text{,} can be written as y_2+g(t)\\text{,} for some particular y_2 and some solution g(t) of the homogeneous equation \\ds y' + p(t)y = 0\\text{.} Since we already know how to find all solutions of the homogeneous equation, finding just one solution to the equation \\ds y' + p(t)y = f(t) will give us all of them.\n\nTheorem 5.25. General Solution of First Order Non-Homogeneous Linear DE.  \n\nGiven a first order non-homogeneous linear differential equation\n\n\\begin{equation*} y' + p(t)y=f(t)\\text{,} \\end{equation*}\n\nlet h(t) be a particular solution, and let g(t) be the general solution to the corresponding homogeneous DE\n\n\\begin{equation*} y' + p(t)y=0\\text{.} \\end{equation*}\n\nThen the general solution to the non-homogeneous DE is constructed as the sum of the above two solutions:\n\n\\begin{equation*} y(t)=g(t)+h(t)\\text{.} \\end{equation*}\n5.3.2.1 Variation of Parameters\n\nWe now introduce the first one of two methods discussed in these notes to solve a first order non-homogeneous linear differential equation. Again, it turns out that what we already know helps. We know that the general solution to the homogeneous equation \\ds y' + p(t)y = 0 looks like \\ds Ae^{P(t)}\\text{,} where P(t) is an antiderivative of -p(t)\\text{.} We now make an inspired guess: Consider the function \\ds v(t)e^{P(t)}\\text{,} in which we have replaced the constant parameter A with the function v(t)\\text{.} This technique is called variation of parameters. For convenience write this as s(t)=v(t)h(t)\\text{,} where \\ds h(t)=e^{P(t)} is a solution to the homogeneous equation. Now let's compute a bit with s(t)\\text{:}\n\n\\begin{align*} s'(t)+p(t)s(t)\\amp = v(t)h'(t)+v'(t)h(t)+p(t)v(t)h(t)\\\\ \\amp = v(t)(h'(t)+p(t)h(t)) + v'(t)h(t)\\\\ \\amp = v'(t)h(t)\\text{.} \\end{align*}\n\nThe last equality is true because \\ds h'(t)+p(t)h(t)=0\\text{,} since h(t) is a solution to the homogeneous equation. We are hoping to find a function s(t) so that \\ds s'(t)+p(t)s(t)=f(t)\\text{;} we will have such a function if we can arrange to have \\ds v'(t)h(t)=f(t)\\text{,} that is, \\ds v'(t)=f(t)/h(t)\\text{.} But this is as easy (or hard) as finding an antiderivative of \\ds f(t)/h(t)\\text{.} Putting this all together, the general solution to \\ds y' + p(t)y = f(t) is\n\n\\begin{equation*} v(t)h(t)+Ae^{P(t)} = v(t)e^{P(t)}+Ae^{P(t)}\\text{.} \\end{equation*}\nMethod of Variation of Parameters.\n\nGiven a first order non-homogeneous linear differential equation\n\n\\begin{equation*} y' + p(t)y=f(t)\\text{,} \\end{equation*}\n\nusing variation of parameters the general solution is given by\n\n\\begin{equation*} y(t)=v(t)e^{P(t)} + Ae^{P(t)}\\text{,} \\end{equation*}\n\nwhere v'(t)=e^{-P(t)}f(t) and P(t) is an antiderivative of -p(t)\\text{.}\n\nNote: The method of variation of parameters makes more sense after taking linear algebra since the method uses determinants. We therefore restrict ourselves to just one example to illustrate this method.\n\nExample5.26. Solving an IVP Using Variation of Parameters.\n\nFind the solution of the initial value problem \\ds y'+3y/t=t^2\\text{,} y(1)=1/2\\text{.}\n\nSolution\n5.3.2.2 Integrating Factor\n\nAnother common method for solving such a differential equation is by means of an integrating factor . In the differential equation \\ds y'+p(t)y=f(t)\\text{,} we note that if we multiply through by a function I(t) to get \\ds I(t)y'+I(t)p(t)y=I(t)f(t)\\text{,} the left hand side looks like it could be a derivative computed by the Product Rule:\n\n\\begin{equation*} \\frac{d}{dt}(I(t)y)=I(t)y'+I'(t)y\\text{.} \\end{equation*}\n\nNow if we could choose I(t) so that I'(t)=I(t)p(t)\\text{,} this would be exactly the left hand side of the differential equation. But this is just a first order homogeneous linear equation, and we know a solution is \\ds I(t)=e^{Q(t)}\\text{,} where \\ds Q(t)=\\int p(t)\\,dt\\text{.} Note that Q(t)=-P(t)\\text{,} where P(t) appears in the variation of parameters method and P'(t)=-p(t)\\text{.} Now the modified differential equation is\n\n\\begin{align*} e^{-P(t)}y'+e^{-P(t)}p(t)y\\amp = e^{-P(t)}f(t)\\\\ {d\\over dt}(e^{-P(t)}y)\\amp = e^{-P(t)}f(t)\\text{.} \\end{align*}\n\nIntegrating both sides gives\n\n\\begin{align*} e^{-P(t)}y\\amp = \\int e^{-P(t)}f(t)\\,dt\\\\ y\\amp = e^{P(t)}\\int e^{-P(t)}f(t)\\,dt\\text{.} \\end{align*}\nNote: If you look carefully, you will see that this is exactly the same solution we found by variation of parameters, because \\ds e^{-P(t)}f(t)=f(t)/h(t)\\text{.} Some people find it easier to remember how to use the integrating factor method, rather than variation of parameters. Since ultimately they require the same calculation, you should use whichever of the two methods appeals to you more.\nDefinition 5.27. Integrating Factor.  \n\nGiven a first order non-homogeneous linear differential equation\n\n\\begin{equation*} y'+p(t)y=f(t)\\text{,} \\end{equation*}\n\nthe integrating factor is given by\n\n\\begin{equation*} I(t) = e^{\\int p(t)\\,dt}\\text{.} \\end{equation*}\nMethod of Integrating Factor.\n\nGiven a first order non-homogeneous linear differential equation\n\n\\begin{equation*} y'+p(t)y=f(t)\\text{,} \\end{equation*}\n\nfollow these steps to determine the general solution y(t) using an integrating factor:\n\nCalculate the integrating factor I(t)\\text{.}\n\nMultiply the standard form equation by I(t)\\text{.}\n\nSimplify the left-hand side to\n\n\\begin{equation*} \\frac{d}{dt}\\left[I(t)y\\right]\\text{.} \\end{equation*}\n\nIntegrate both sides of the equation.\n\nSolve for y(t)\\text{.}\n\nThe solution can be compactly written as\n\n\\begin{equation*} y(t)=e^{-\\int p(t)\\,dt}\\left[\\int e^{\\int p(t)\\,dt} f(t)\\,dt + C\\right]\\text{.} \\end{equation*}\n\nUsing this method, the solution of the previous example would look just a bit different.\n\nExample5.28. Solving an IVP Using Integrating Factor.\n\nFind the solution of the initial value problem \\ds y'+3y/t=t^2\\text{,} y(1)=1/2\\text{.}\n\nSolution\nExample5.29. General Solution Using Integrating Factor.\n\nDetermine the general solution of the differential equation\n\n\\begin{equation*} \\frac{dy}{dt} +3t^2y = 6t^2\\text{.} \\end{equation*}\nSolution\nExercises for Section\u00a05.3.\nExercise 5.3.1.\nExercise 5.3.2.\nExercise 5.3.3.\nExercise 5.3.4.\nExercise 5.3.5.\nExercise 5.3.6.\nExercise 5.3.7.\nExercise 5.3.8.\nExercise 5.3.9.\n\nSeparable First Order Differential Equations\n\nContentsToggle Main Menu 1 Definition 2 Solving Separable Equations 3 Worked Examples 4 Video Examples 5 Workbook 6 See Also 7 External Resources\n\nDefinition\n\nA first order differential equation is separable if it can be written in one of the following forms:\n\n\\[\\begin{align} \\frac{\\mathrm{d} y}{\\mathrm{d} x} &= f(x,y) = \\frac{g(x)}{h(y)}, \\\\ \\frac{\\mathrm{d} y}{\\mathrm{d} x} &= f(x,y) = \\frac{h(y)}{g(x)}. \\end{align}\\]\n\nSolving Separable Equations\n\nA separable equation is solved by separating the variables, that is, rearranging the equation so that everything involving $y$ appears on one side of the equation, and everything involving $x$ appears on the other. The equation can then be integrated directly.\n\nFor an equation in the form:\n\n\\[\\frac{\\mathrm{d} y}{\\mathrm{d} x} = \\frac{g(x)}{h(y)},\\]\n\nmultiplying both sides by $h(y)\\mathrm{d} x$ gives:\n\n\\[h(y)\\mathrm{d} y = g(x) \\mathrm{d} x,\\]\n\nwhich can be integrated directly:\n\n\\[\\int h(y) \\mathrm{d} y = \\int g(x) \\mathrm{d} x.\\]\n\nThis will yield a solution for $y(x)$.\n\nSimilarly, an equation in the form:\n\n\\[\\frac{\\mathrm{d} y}{\\mathrm{d} x} = \\frac{h(y)}{g(x)}\\]\n\ncan be multiplied by $\\dfrac{\\mathrm{d} x}{h(y)}$ and then integrated:\n\n\\[\\int \\frac{\\mathrm{d} y}{h(y)} = \\int \\frac{\\mathrm{d} x}{g(x)},\\]\n\nthus yielding a solution for $y(x)$.\n\nNote: The solution obtained for $y$ by computing these integrals may be implicitly defined. Rearranging the solution may be necessary to obtain an explicit solution for $y$, although in some cases it may not be possible to express $y$ explicitly.\n\nWorked Examples\nExample 1\n\nSolve the differential equation\n\n\\[\\frac{\\mathrm{d} y}{\\mathrm{d} x} = \\frac{3x^2}{y}\\]\n\nsubject to the condition $y(0)=2$.\n\nSolution\n\nThe equation can be rearranged so that everything involving $y$ appears on the left-hand side of the equation, and everything involving $x$ appears on the right.\n\nFirst multiply both sides by $y$:\n\n\\[y\\frac{\\mathrm{d} y}{\\mathrm{d} x} = 3x^2,\\]\n\nthen multiply both sides by $\\mathrm{d} x$ to obtain\n\n\\[y \\; \\mathrm{d} y = 3x^2 \\; \\mathrm{d} x.\\]\n\nThis can be integrated directly using standard integrals:\n\n\\begin{align} \\int y \\; \\mathrm{d} y &= \\int 3x^2 \\; \\mathrm{d} x, \\\\ \\frac{y^2}{2} &= x^3 + C. \\end{align}\n\nNote: A constant of integration arises for both the integral on the left-hand side and the integral on the right-hand side. Since these constants are both arbitrary, then can be absorbed into one constant, denoted here by $C$, which is conventionally included on the side involving the independent variable $x$.\n\nRearranging the above equation gives an explicit solution for $y$:\n\n\\[\\begin{align} \\frac{y^2}{2} &= x^3 + C \\\\ y^2 &= 2x^3 + C' & (C' &= 2C)\\\\ y &= \\pm\\sqrt{2x^3 + C'}. \\end{align}\\]\n\nNote: Since multiplying an arbitrary constant by a number gives an arbitrary constant, the quantity $2C$ can be relabelled $C'$, another arbitrary constant.\n\nTo find the solution that satisfies $y(0)=2$, substitute $x=0$ and $y=2$ into the solution and solve for $C'$:\n\n\\[\\begin{align} 2 &= \\pm\\sqrt{0+C'} \\\\ 2 &= \\pm\\sqrt{C'} \\end{align}.\\]\n\nClearly this can only be satisfied by taking the positive square root. Then:\n\n\\[2 = \\sqrt{C'} \\Rightarrow C'=4,\\]\n\nand the solution satisfying the condition $y(0)=2$ is:\n\n\\[y=\\sqrt{2x^3+4}.\\]\n\nNote: Since the condition $y(0)=2$ is only satisfied by taking the positive square root, the solution is only valid when the positive square root is taken, so the $\\pm$ sign is no longer included.\n\nExample 2\n\nSolve the differential equation\n\n\\[\\frac{\\mathrm{d} y}{\\mathrm{d} x} = \\frac{1}{x+3}\\]\n\nsubject to the condition $y(0)=1$.\n\nSolution\n\nMultiplying both sides of the equation by $\\mathrm{d} x$ gives\n\n\\[\\frac{\\mathrm{d} y}{y} = \\frac{\\mathrm{d} x}{x+3}.\\]\n\nEvery term involving $x$ now appears on the right-hand side, and every term involving $y$ now appears on the left-hand side. Each side of the equation can therefore be integrated directly:\n\n\\[\\begin{align} \\int \\frac{\\mathrm{d} y}{y} &= \\int\\frac{\\mathrm{d} x}{x+3}, \\\\ \\ln\\lvert y\\rvert &= \\ln\\lvert x+3 \\rvert + C. \\end{align}\\]\n\nTo find a solution which gives $y$ explicitly, exponentiate both sides to obtain:\n\n\\[y=A(x+3),\\]\n\nwhere $A$ is an arbitrary constant.\n\nNote: The constant $A$ comes from applying the laws of logarithms and powers:\n\n\\[e^{\\large{\\ln\\mid x+3\\mid+C} } = e^C e^{\\large{\\ln\\mid x+3\\mid} } = e^C(x+3) = A(x+3),\\]\n\nwhere $A= e^C$. Since $ e$ is a number, $ e$ raised to a constant power is also a constant, and it is permissible to denote that constant by a single letter $A$.\n\nHence, the general solution to the differential equation is\n\n\\[y=A(x+3).\\]\n\nTo find the solution that satisfies $y(0)=1$, substitute $x=0$ and $y=1$ into the solution and solve for $A$:\n\n\\[1=A(0+3) \\Rightarrow 3A=1 \\Rightarrow A=\\frac{1}{3}.\\]\n\nHence the solution to the given differential equation that satisfies the condition $y(0)=1$ is\n\n\\[y=\\frac{1}{3}(x+3).\\]\n\nVideo Examples\nExample 1\n\nProf. Robin Johnson solves $\\dfrac{\\mathrm{d}y}{\\mathrm{d}x}=-2y^3$, subject to the condition $y(0)=1$.\n\nExample 2\n\nProf. Robin Johnson solves $\\dfrac{\\mathrm{d}y}{\\mathrm{d}x}=\\dfrac{1+y^2}{2+3x}$, subject to the condition $y\\left(-\\dfrac{1}{3}\\right)=0$.\n\nWorkbook\n\nThis workbook produced by HELM is a good revision aid, containing key points for revision and many worked examples.\n\nFirst order differential equations\nSee Also\nIntegrating Factor\nHomogeneous First Order Differential Equations\nExternal Resources\nSolving differential equations by separating variables workbook at mathcentre.",
    "Higher-Order Linear Differential Equations": "Processing math: 0%\nSkip to main content\nchrome_reader_mode\nEnter Reader Mode\nSearch \n Search\nSign in\n Expand/collapse global hierarchy\n Home   Bookshelves   Differential Equations   Differential Equations for Engineers (Lebl)   2: Higher order linear ODEs   2.3: Higher order linear ODEs\n Expand/collapse global location\n2.3: Higher order linear ODEs\n2.2: Constant coefficient second order linear ODEs\n \n2.4: Mechanical Vibrations\nJi\u0159\u00ed Lebl\nOklahoma State University\n\nEquations that appear in applications tend to be second order, although higher order equations do appear from time to time. Hence, it is a generally assumed that the world is \u201csecond order\u201d from a modern physics perspective. The basic results about linear ODEs of higher order are essentially the same as for second order equations, with 2 replaced by n. The important concept of linear independence is somewhat more complicated when more than two functions are involved.\n\nFor higher order constant coefficient ODEs, the methods are also somewhat harder to apply, but we will not dwell on these complications. We can always use the methods for systems of linear equations to solve higher order constant coefficient equations. So let us start with a general homogeneous linear equation:\n\ny^{(n)} + p_{n-1}(x)y^{(n-1)} + \\, ... + p_1(x)y' + p_o(x)y = f(x) \\label{2.3.1}\n\nTheorem \\PageIndex{1}\n\nSuperposition\n\nSuppose y_1, y_2, \\dots , y_n are solutions of the homogeneous equation (Equation \\ref{2.3.1}). Then\n\ny(x) = C_1 y_1(x) + C_2 y_2(x) + ... + C_n y_n(x) \\nonumber\n\nalso solves Equation \\ref{2.3.1} for arbitrary constants C_1, .... C_n .\n\nIn other words, a linear combination of solutions to Equation \\ref{2.3.1} is also a solution to Equation \\ref{2.3.1}. We also have the existence and uniqueness theorem for nonhomogeneous linear equations.\n\nTheorem \\PageIndex{2}\n\nExistence and Uniqueness\n\nSuppose p_o through p_{n-1}, and f are continuous functions on some interval I , a is a number in I, and b_0, b_1, \\dots , b_{n-1} are constants. The equation\n\ny^{(n)} + p_{n-1}(x)y^{(n-1)} + \\, ... + p_1(x)y' + p_o(x)y = f(x) \\nonumber\n\nhas exactly one solution y(x) defined on the same interval I satisfying the initial conditions\n\ny(a) = b_0, \\quad y'(a) = b_1,\\quad \\dots ,\\quad y^{(n -1)} (a) = b_{n - 1} \\nonumber\n\nLinear Independence\n\nWhen we had two functions y_1 and y_2 we said they were linearly independent if one was not the multiple of the other. Same idea holds for n functions. In this case it is easier to state as follows. The functions y_1, y_2, \\dots , y_n are linearly independent if\n\nc_1y_1 + c_2y_2 + \\dots + c_ny_n = 0 \\nonumber\n\nhas only the trivial solution c_1 = c_2 = \\dots = c_n = 0 , where the equation must hold for all x. If we can solve equation with some constants where for example c_1 \\ne 0 , then we can solve for y_1 as a linear combination of the others. If the functions are not linearly independent, they are linearly dependent.\n\nExample \\PageIndex{1}\n\nShow that e^x, e^{2x}, and e^{3x} are linearly independent functions.\n\nSolution\n\nLet us give several ways to show this fact. Many textbooks introduce Wronskians, but that is really not necessary to solve this example. Let us write down\n\nc_1e^x + c_2e^{2x} + c_3e^{3x} = 0 \\nonumber\n\nWe use rules of exponentials and write z = e^x . Then we have\n\nc_1z + c_2z^2 + c_3z^3 = 0 \\nonumber\n\nThe left hand side is a third degree polynomial in z. It can either be identically zero, or it can have at most 3 zeros. Therefore, it is identically zero, c_1 = c_2 = c_3 = 0 , and the functions are linearly independent.\n\nLet us try another way. As before we write\n\nc_1e^x + c_2e^{2x} + c_3e^{3x} = 0 \\nonumber\n\nThis equation has to hold for all x. What we could do is divide through by e^{3x} to get\n\nc_1e^{-2x} + c_2e^{-x} + c_3 = 0 \\nonumber\n\nAs the equation is true for all x, let x \\rightarrow \\infty . After taking the limit we see that c_3 = 0 . Hence our equation becomes\n\nc_1e^x + c_2e^{2x} = 0 \\nonumber\n\nRinse, repeat!\n\nHow about yet another way. We again write\n\nc_1e^x + c_2e^{2x} + c_3e^{3x} = 0 \\nonumber\n\nWe can evaluate the equation and its derivatives at different values of x to obtain equations for c_1, c_2, and c_3. Let us first divide by e^x for simplicity.\n\nc_1 + c_2e^x + c_3e^{2x} = 0 \\nonumber\n\nWe set x = 0 to get the equation c_1 + c_2 + c_3 = 0 . Now differentiate both sides\n\nc_2 e^x + 2c_3e^{2x} = 0 \\nonumber\n\nWe set x = 0 to get c_2 + 2c_3 = 0 . We divide by e^x again and differentiate to get 2c_3e^x = 0 . It is clear that c_3 is zero. Then c_2 must be zero as c_2 = -2c_3 , and c_1 must be zero because c_1 + c_2 + c_3 = 0 .\n\nThere is no one best way to do it. All of these methods are perfectly valid. The important thing is to understand why the functions are linearly independent.\n\nExample \\PageIndex{2}\n\nOn the other hand, the functions e^x, e^{-x} and \\cosh x are linearly dependent. Simply apply definition of the hyperbolic cosine:\n\n\\cosh x = \\frac {e^x + e^{-x}}{2} \\quad\\text{or}\\quad 2 \\cosh x - e^x - e^{-x} = 0 \\nonumber\n\nConstant Coefficient Higher Order ODEs\n\nWhen we have a higher order constant coefficient homogeneous linear equation, the song and dance is exactly the same as it was for second order. We just need to find more solutions. If the equation is n^{th} order we need to find n linearly independent solutions. It is best seen by example.\n\nExample \\PageIndex{3}: Third order ODE with Constant Coefficients\n\nFind the general solution to\n\n\\label{eq:15}y''' - 3'' - y' + 3y =0\n\nSolution\n\nTry: y = e^{rx} . We plug in and get\n\n\\underbrace{r^3 e^{rx}}_{y'''} - 3 \\underbrace{r^2 e^{rx}}_{y''} - \\underbrace{r e^{rx}}_{y'} + 3 \\underbrace{e^{rx}}_{y} = 0 . \\nonumber\n\nWe divide through by e^{rx}. Then\n\nr^3 - 3r^2 - r +3 =0 \\nonumber\n\nThe trick now is to find the roots. There is a formula for the roots of degree 3 and 4 polynomials, but it is very complicated. There is no formula for higher degree polynomials. That does not mean that the roots do not exist. There are always n roots for an n^{th} degree polynomial. They may be repeated and they may be complex. Computers are pretty good at finding roots approximately for reasonable size polynomials.\n\nA good place to start is to plot the polynomial and check where it is zero. We can also simply try plugging in. We just start plugging in numbers r = -2, -1, 0, 1, 2, \\dots and see if we get a hit (we can also try complex numbers). Even if we do not get a hit, we may get an indication of where the root is. For example, we plug r = -2 into our polynomial and get -15; we plug in r = 0 and get 3. That means there is a root between r = -2 and r = 0 , because the sign changed. If we find one root, say r_1, then we know (r - r_1) is a factor of our polynomial. Polynomial long division can then be used.\n\nA good strategy is to begin with r = -1, 1, or 0. These are easy to compute. Our polynomial happens to have two such roots, r_1 = -1 and r_2 = 1 and. There should be three roots and the last root is reasonably easy to find. The constant term in a monic^{1} polynomial such as this is the multiple of the negations of all the roots because r^3 - 3r^2 - r + 3 = (r - r_1)(r - r_2)(r - r_3). So\n\n3 = (-r_1)(-r_2)(-r_3) = (1)(-1)(-r_3) = r_3 \\nonumber\n\nYou should check that r_3 = 3 really is a root. Hence we know that e^{-x}, e^{x}, and e^{3x} are solutions to \\eqref{eq:15}. They are linearly independent as can easily be checked, and there are three of them, which happens to be exactly the number we need. Hence the general solution is\n\ny = C_1e^{-x} + C_2e^{x} + C_3e^{3x} \\nonumber\n\nSuppose we were given some initial conditions y(0) = 1, y'(0) = 2, and y''(0) = 3 . Then\n\n\\begin{align}\\begin{aligned} 1 &= y(0) = C_1 + C_2 + C_3 \\\\ 2 &= y'(0) = -C_1 + C_2 + 3C_3 \\\\ 3 &= y''(0) = C_1 + C_2 + 9C_3 \\end{aligned}\\end{align} \\nonumber\n\nIt is possible to find the solution by high school algebra, but it would be a pain. The sensible way to solve a system of equations such as this is to use matrix algebra, see Section 3.2 or Appendix A. For now we note that the solution is C_1 = - \\frac {1}{4}, C_2 = 1, and C_3 = \\frac {1}{4} . The specific solution to the ODE is\n\ny = - \\frac {1}{4} e^{-x} + e^{x} + \\frac {1}{4} e^{3x} \\nonumber\n\nNext, suppose that we have real roots, but they are repeated. Let us say we have a root r repeated k times. In the spirit of the second order solution, and for the same reasons, we have the solutions\n\ne^{rx}, xe^{rx}, x^2e^{rx}, \\dots , x^{k-1}e^{rx} \\nonumber\n\nWe take a linear combination of these solutions to find the general solution.\n\nExample \\PageIndex{4}\n\nSolve\n\ny^{(4)} - 3y''' + 3y'' - y' = 0 \\nonumber\n\nSolution\n\nWe note that the characteristic equation is\n\nr^4 - 3r^3 + 3r^2 - r = 0 \\nonumber\n\nBy inspection we note that r^4 - 3r^3 + 3r^2 - r = r{(r - 1)}^3 . Hence the roots given with multiplicity are r = 0, 1, 1, 1 . Thus the general solution is\n\ny = \\underbrace { (C_1 + C_2 + C_3x^2)e^x}_{\\text {terms coming from r = 1}} + \\underbrace { C_4}_{ \\text {from r = 0} } \\nonumber\n\nThe case of complex roots is similar to second order equations. Complex roots always come in pairs r = \\alpha \\pm i\\beta . Suppose we have two such complex roots, each repeated k times. The corresponding solution is\n\n(C_0 + C_1x + \\dots + C_{k-1} x^{k-1})e^{ax} \\cos ( \\beta x) + ( D_0 + D_1x + \\dots + D_{k - 1}x^{k - 1} ) e^{ax} \\sin ( \\beta x) \\nonumber\n\nwhere C_0, \\dots , C_{k-1} , D_0, \\dots, D_{k-1} are arbitrary constants.\n\nExample \\PageIndex{5}\n\nSolve\n\ny^{(4)} - 4y''' + 8y'' - 8y' + 4y = 0 \\nonumber\n\nSolution\n\nThe characteristic equation is\n\n\\begin{align}\\begin{aligned} r^4 - 4r^3 + 8r^2 - 8r + 4 &= 0 \\\\ {(r^2 - 2r + 2)}^2 &= 0 \\\\ {({( r - 1)}^2 + 1 )}^2 &= 0 \\end{aligned}\\end{align} \\nonumber\n\nHence the roots are 1 \\pm i, both with multiplicity 2. Hence the general solution to the ODE is\n\ny = ( C_1 + C_2x)e^x \\cos x + ( C_3 + C_4 x ) e^x \\sin x \\nonumber\n\nThe way we solved the characteristic equation above is really by guessing or by inspection. It is not so easy in general. We could also have asked a computer or an advanced calculator for the roots.\n\nFootnotes\n\n[1] The word monic means that the coefficient of the top degree r^{d}, in our case r^{3}, is 1.\n\nOutside Links\nAfter reading this lecture, it may be good to try Project III from the IODE website: www.math.uiuc.edu/iode/.\n\nThis page titled 2.3: Higher order linear ODEs is shared under a CC BY-SA 4.0 license and was authored, remixed, and/or curated by Ji\u0159\u00ed Lebl via source content that was edited to the style and standards of the LibreTexts platform.\n\nBack to top\n2.2: Constant coefficient second order linear ODEs 2.4: Mechanical Vibrations\nWas this article helpful?\nYes\nNo\n?\n\nThe LibreTexts libraries are\u00a0Powered by NICE CXone Expert\u00a0and are supported by the Department of Education Open Textbook Pilot Project, the UC Davis Office of the Provost, the UC Davis Library, the California State University Affordable Learning Solutions Program, and Merlot. We also acknowledge previous National Science Foundation support under grant numbers 1246120, 1525057, and 1413739. Legal. Accessibility Statement\u00a0For more information contact us at\u00a0info@libretexts.org.\n\nProcessing math: 100%\nPaul's Online Notes\nNOTES  QUICK NAV  DOWNLOAD  \n\nHome / Differential Equations / Higher Order Differential Equations\n Prev. Section  Notes  Next Section \n\u00a0\nChapter 7 : Higher Order Differential Equations\n\nIn this chapter we\u2019re going to take a look at higher order differential equations. This chapter will actually contain more than most text books tend to have when they discuss higher order differential equations.\n\nWe will definitely cover the same material that most text books do here. However, in all the previous chapters all of our examples were 2nd order differential equations or 2\u00d72 systems of differential equations. So, in this chapter we\u2019re also going to do a couple of examples here dealing with 3rd order or higher differential equations with Laplace transforms and series as well as a discussion of some larger systems of differential equations.\n\nHere is a brief listing of the topics in this chapter.\n\nBasic Concepts for nth Order Linear Equations \u2013 In this section we\u2019ll start the chapter off with a quick look at some of the basic ideas behind solving higher order linear differential equations. Included will be updated definitions/facts for the Principle of Superposition, linearly independent functions and the Wronskian.\n\nLinear Homogeneous Differential Equations \u2013 In this section we will extend the ideas behind solving 2nd order, linear, homogeneous differential equations to higher order. As we\u2019ll most of the process is identical with a few natural extensions to repeated real roots that occur more than twice. We will also need to discuss how to deal with repeated complex roots, which are now a possibility. In addition, we will see that the main difficulty in the higher order cases is simply finding all the roots of the characteristic polynomial.\n\nUndetermined Coefficients \u2013 In this section we work a quick example to illustrate that using undetermined coefficients on higher order differential equations is no different that when we used it on 2nd order differential equations with only one small natural extension.\n\nVariation of Parameters \u2013 In this section we will give a detailed discussion of the process for using variation of parameters for higher order differential equations. We will also develop a formula that can be used in these cases. We will also see that the work involved in using variation of parameters on higher order differential equations can be quite involved on occasion.\n\nLaplace Transforms \u2013 In this section we will work a quick example using Laplace transforms to solve a differential equation on a 3rd order differential equation just to say that we looked at one with order higher than 2nd. As we\u2019ll see, outside of needing a formula for the Laplace transform of y\u2034, which we can get from the general formula, there is no real difference in how Laplace transforms are used for higher order differential equations.\n\nSystems of Differential Equations \u2013 In this section we\u2019ll take a quick look at extending the ideas we discussed for solving 2\u00d72 systems of differential equations to systems of size 3\u00d73. As we will see they are mostly just natural extensions of what we already know how to do. We will also make a couple of quick comments about 4\u00d74 systems.\n\nSeries Solutions \u2013 In this section we are going to work a quick example illustrating that the process of finding series solutions for higher order differential equations is pretty much the same as that used on 2nd order differential equations.\n\n[Contact Me]\u00a0[Privacy Statement]\u00a0[Site Help & FAQ]\u00a0[Terms of Use]\n\u00a9 2003 - 2025 Paul Dawkins\nPage Last Modified : 6/26/2023\n\nPaul's Online Notes\nNOTES  QUICK NAV  DOWNLOAD  \n\nHome / Differential Equations\n Notes  Next Section \n\u00a0\nDIFFERENTIAL EQUATIONS\nHere are my notes for my differential equations course that I teach here at Lamar University. Despite the fact that these are my \u201cclass notes\u201d, they should be accessible to anyone wanting to learn how to solve differential equations or needing a refresher on differential equations.\nI\u2019ve tried to make these notes as self-contained as possible and so all the information needed to read through them is either from a Calculus or Algebra class or contained in other sections of the notes.\nHere are a couple of warnings to my students who may be here to get a copy of what happened on a day that you missed.\n\nBecause I wanted to make this a fairly complete set of notes for anyone wanting to learn differential equations have included some material that I do not usually have time to cover in class and because this changes from semester to semester it is not noted here. You will need to find one of your fellow class mates to see if there is something in these notes that wasn\u2019t covered in class.\n\n\nIn general, I try to work problems in class that are different from my notes. However, with Differential Equation many of the problems are difficult to make up on the spur of the moment and so in this class my class work will follow these notes fairly close as far as worked problems go. With that being said I will, on occasion, work problems off the top of my head when I can to provide more examples than just those in my notes. Also, I often don\u2019t have time in class to work all of the problems in the notes and so you will find that some sections contain problems that weren\u2019t worked in class due to time restrictions.\n\n\nSometimes questions in class will lead down paths that are not covered here. I try to anticipate as many of the questions as possible when writing these up, but the reality is that I can\u2019t anticipate all the questions. Sometimes a very good question gets asked in class that leads to insights that I\u2019ve not included here. You should always talk to someone who was in class on the day you missed and compare these notes to their notes and see what the differences are.\n\n\nThis is somewhat related to the previous three items, but is important enough to merit its own item. THESE NOTES ARE NOT A SUBSTITUTE FOR ATTENDING CLASS!! Using these notes as a substitute for class is liable to get you in trouble. As already noted not everything in these notes is covered in class and often material or insights not in these notes is covered in class.\n\nHere is a listing (and brief description) of the material that is in this set of notes.\n\nBasic Concepts - In this chapter we introduce many of the basic concepts and definitions that are encountered in a typical differential equations course. We will also take a look at direction fields and how they can be used to determine some of the behavior of solutions to differential equations.\nDefinitions \u2013 In this section some of the common definitions and concepts in a differential equations course are introduced including order, linear vs. nonlinear, initial conditions, initial value problem and interval of validity.\nDirection Fields \u2013 In this section we discuss direction fields and how to sketch them. We also investigate how direction fields can be used to determine some information about the solution to a differential equation without actually having the solution.\nFinal Thoughts \u2013 In this section we give a couple of final thoughts on what we will be looking at throughout this course.\n\nFirst Order Differential Equations - In this chapter we will look at several of the standard solution methods for first order differential equations including linear, separable, exact and Bernoulli differential equations. We also take a look at intervals of validity, equilibrium solutions and Euler\u2019s Method. In addition we model some physical situations with first order differential equations.\nLinear Equations \u2013 In this section we solve linear first order differential equations, i.e. differential equations in the form \\(y' + p(t) y = g(t)\\). We give an in depth overview of the process used to solve this type of differential equation as well as a derivation of the formula needed for the integrating factor used in the solution process.\nSeparable Equations \u2013 In this section we solve separable first order differential equations, i.e. differential equations in the form \\(N(y) y' = M(x)\\). We will give a derivation of the solution process to this type of differential equation. We\u2019ll also start looking at finding the interval of validity for the solution to a differential equation.\nExact Equations \u2013 In this section we will discuss identifying and solving exact differential equations. We will develop a test that can be used to identify exact differential equations and give a detailed explanation of the solution process. We will also do a few more interval of validity problems here as well.\nBernoulli Differential Equations \u2013 In this section we solve Bernoulli differential equations, i.e. differential equations in the form \\(y' + p(t) y = y^{n}\\). This section will also introduce the idea of using a substitution to help us solve differential equations.\nSubstitutions \u2013 In this section we\u2019ll pick up where the last section left off and take a look at a couple of other substitutions that can be used to solve some differential equations. In particular we will discuss using solutions to solve differential equations of the form \\(y' = F(\\frac{y}{x})\\) and \\(y' = G(ax + by)\\).\nIntervals of Validity \u2013 In this section we will give an in depth look at intervals of validity as well as an answer to the existence and uniqueness question for first order differential equations.\nModeling with First Order Differential Equations \u2013 In this section we will use first order differential equations to model physical situations. In particular we will look at mixing problems (modeling the amount of a substance dissolved in a liquid and liquid both enters and exits), population problems (modeling a population under a variety of situations in which the population can enter or exit) and falling objects (modeling the velocity of a falling object under the influence of both gravity and air resistance).\nEquilibrium Solutions \u2013 In this section we will define equilibrium solutions (or equilibrium points) for autonomous differential equations, \\(y' = f(y)\\). We discuss classifying equilibrium solutions as asymptotically stable, unstable or semi-stable equilibrium solutions.\nEuler\u2019s Method \u2013 In this section we\u2019ll take a brief look at a fairly simple method for approximating solutions to differential equations. We derive the formulas used by Euler\u2019s Method and give a brief discussion of the errors in the approximations of the solutions.\n\nSecond Order Differential Equations - In this chapter we will start looking at second order differential equations. We will concentrate mostly on constant coefficient second order differential equations. We will derive the solutions for homogeneous differential equations and we will use the methods of undetermined coefficients and variation of parameters to solve non homogeneous differential equations. In addition, we will discuss reduction of order, fundamentals of sets of solutions, Wronskian and mechanical vibrations.\nBasic Concepts \u2013 In this section give an in depth discussion on the process used to solve homogeneous, linear, second order differential equations, \\(ay'' + by' + cy = 0\\). We derive the characteristic polynomial and discuss how the Principle of Superposition is used to get the general solution.\nReal Roots \u2013 In this section we discuss the solution to homogeneous, linear, second order differential equations, \\(ay'' + by' + cy = 0\\), in which the roots of the characteristic polynomial, \\(ar^{2} + br + c = 0\\), are real distinct roots.\nComplex Roots \u2013 In this section we discuss the solution to homogeneous, linear, second order differential equations, \\(ay'' + by' + cy = 0\\), in which the roots of the characteristic polynomial, \\(ar^{2} + br + c = 0\\), are real distinct roots. We will also derive from the complex roots the standard solution that is typically used in this case that will not involve complex numbers.\nRepeated Roots \u2013 In this section we discuss the solution to homogeneous, linear, second order differential equations, \\(ay'' + by' + cy = 0\\), in which the roots of the characteristic polynomial, \\(ar^{2} + br + c = 0\\), are repeated, i.e. double, roots. We will use reduction of order to derive the second solution needed to get a general solution in this case.\nReduction of Order \u2013 In this section we will discuss reduction of order, the process used to derive the solution to the repeated roots case for homogeneous linear second order differential equations, in greater detail. This will be one of the few times in this chapter that non-constant coefficient differential equation will be looked at.\nFundamental Sets of Solutions \u2013 In this section we will a look at some of the theory behind the solution to second order differential equations. We define fundamental sets of solutions and discuss how they can be used to get a general solution to a homogeneous second order differential equation. We will also define the Wronskian and show how it can be used to determine if a pair of solutions are a fundamental set of solutions.\nMore on the Wronskian \u2013 In this section we will examine how the Wronskian, introduced in the previous section, can be used to determine if two functions are linearly independent or linearly dependent. We will also give and an alternate method for finding the Wronskian.\nNonhomogeneous Differential Equations \u2013 In this section we will discuss the basics of solving nonhomogeneous differential equations. We define the complimentary and particular solution and give the form of the general solution to a nonhomogeneous differential equation.\nUndetermined Coefficients \u2013 In this section we introduce the method of undetermined coefficients to find particular solutions to nonhomogeneous differential equation. We work a wide variety of examples illustrating the many guidelines for making the initial guess of the form of the particular solution that is needed for the method.\nVariation of Parameters \u2013 In this section we introduce the method of variation of parameters to find particular solutions to nonhomogeneous differential equation. We give a detailed examination of the method as well as derive a formula that can be used to find particular solutions.\nMechanical Vibrations \u2013 In this section we will examine mechanical vibrations. In particular we will model an object connected to a spring and moving up and down. We also allow for the introduction of a damper to the system and for general external forces to act on the object. Note as well that while we example mechanical vibrations in this section a simple change of notation (and corresponding change in what the quantities represent) can move this into almost any other engineering field.\n\nLaplace Transforms - In this chapter we introduce Laplace Transforms and how they are used to solve Initial Value Problems. With the introduction of Laplace Transforms we will now be able to solve some Initial Value Problems that we wouldn\u2019t be able to solve otherwise. We will solve differential equations that involve Heaviside and Dirac Delta functions. We will also give brief overview on using Laplace transforms to solve nonconstant coefficient differential equations. In addition, we will define the convolution integral and show how it can be used to take inverse transforms.\nThe Definition \u2013 In this section we give the definition of the Laplace transform. We will also compute a couple Laplace transforms using the definition.\nLaplace Transforms \u2013 In this section we introduce the way we usually compute Laplace transforms that avoids needing to use the definition. We discuss the table of Laplace transforms used in this material and work a variety of examples illustrating the use of the table of Laplace transforms.\nInverse Laplace Transforms \u2013 In this section we ask the opposite question from the previous section. In other words, given a Laplace transform, what function did we originally have? We again work a variety of examples illustrating how to use the table of Laplace transforms to do this as well as some of the manipulation of the given Laplace transform that is needed in order to use the table.\nStep Functions \u2013 In this section we introduce the step or Heaviside function. We illustrate how to write a piecewise function in terms of Heaviside functions. We also work a variety of examples showing how to take Laplace transforms and inverse Laplace transforms that involve Heaviside functions. We also derive the formulas for taking the Laplace transform of functions which involve Heaviside functions.\nSolving IVPs' with Laplace Transforms - In this section we will examine how to use Laplace transforms to solve IVP\u2019s. The examples in this section are restricted to differential equations that could be solved without using Laplace transform. The advantage of starting out with this type of differential equation is that the work tends to be not as involved and we can always check our answers if we wish to.\nNonconstant Coefficient IVP\u2019s \u2013 In this section we will give a brief overview of using Laplace transforms to solve some nonconstant coefficient IVP\u2019s. We do not work a great many examples in this section. We only work a couple to illustrate how the process works with Laplace transforms.\nIVP\u2019s with Step Functions \u2013 This is the section where the reason for using Laplace transforms really becomes apparent. We will use Laplace transforms to solve IVP\u2019s that contain Heaviside (or step) functions. Without Laplace transforms solving these would involve quite a bit of work. While we do work one of these examples without Laplace transforms, we do it only to show what would be involved if we did try to solve one of the examples without using Laplace transforms.\nDirac Delta Function \u2013 In this section we introduce the Dirac Delta function and derive the Laplace transform of the Dirac Delta function. We work a couple of examples of solving differential equations involving Dirac Delta functions and unlike problems with Heaviside functions our only real option for this kind of differential equation is to use Laplace transforms. We also give a nice relationship between Heaviside and Dirac Delta functions.\nConvolution Integral \u2013 In this section we give a brief introduction to the convolution integral and how it can be used to take inverse Laplace transforms. We also illustrate its use in solving a differential equation in which the forcing function (i.e. the term without any y\u2019s in it) is not known.\nTable of Laplace Transforms \u2013 This section is the table of Laplace Transforms that we\u2019ll be using in the material. We give as wide a variety of Laplace transforms as possible including some that aren\u2019t often given in tables of Laplace transforms.\n\nSystems of Differential Equations - In this chapter we will look at solving systems of differential equations. We will restrict ourselves to systems of two linear differential equations for the purposes of the discussion but many of the techniques will extend to larger systems of linear differential equations. We also examine sketch phase planes/portraits for systems of two differential equations. In addition, we give brief discussions on using Laplace transforms to solve systems and some modeling that gives rise to systems of differential equations.\nReview : Systems of Equations \u2013 In this section we will give a review of the traditional starting point for a linear algebra class. We will use linear algebra techniques to solve a system of equations as well as give a couple of useful facts about the number of solutions that a system of equations can have.\nReview : Matrices and Vectors \u2013 In this section we will give a brief review of matrices and vectors. We will look at arithmetic involving matrices and vectors, finding the inverse of a matrix, computing the determinant of a matrix, linearly dependent/independent vectors and converting systems of equations into matrix form.\nReview : Eigenvalues and Eigenvectors \u2013 In this section we will introduce the concept of eigenvalues and eigenvectors of a matrix. We define the characteristic polynomial and show how it can be used to find the eigenvalues for a matrix. Once we have the eigenvalues for a matrix we also show how to find the corresponding eigenvalues for the matrix.\nSystems of Differential Equations \u2013 In this section we will look at some of the basics of systems of differential equations. We show how to convert a system of differential equations into matrix form. In addition, we show how to convert an \\(n^{ \\text{th}}\\) order differential equation into a system of differential equations.\nSolutions to Systems \u2013 In this section we will a quick overview on how we solve systems of differential equations that are in matrix form. We also define the Wronskian for systems of differential equations and show how it can be used to determine if we have a general solution to the system of differential equations.\nPhase Plane \u2013 In this section we will give a brief introduction to the phase plane and phase portraits. We define the equilibrium solution/point for a homogeneous system of differential equations and how phase portraits can be used to determine the stability of the equilibrium solution. We also show the formal method of how phase portraits are constructed.\nReal Eigenvalues \u2013 In this section we will solve systems of two linear differential equations in which the eigenvalues are distinct real numbers. We will also show how to sketch phase portraits associated with real distinct eigenvalues (saddle points and nodes).\nComplex Eigenvalues \u2013 In this section we will solve systems of two linear differential equations in which the eigenvalues are complex numbers. This will include illustrating how to get a solution that does not involve complex numbers that we usually are after in these cases. We will also show how to sketch phase portraits associated with complex eigenvalues (centers and spirals).\nRepeated Eigenvalues \u2013 In this section we will solve systems of two linear differential equations in which the eigenvalues are real repeated (double in this case) numbers. This will include deriving a second linearly independent solution that we will need to form the general solution to the system. We will also show how to sketch phase portraits associated with real repeated eigenvalues (improper nodes).\nNonhomogeneous Systems \u2013 In this section we will work quick examples illustrating the use of undetermined coefficients and variation of parameters to solve nonhomogeneous systems of differential equations. The method of undetermined coefficients will work pretty much as it does for nth order differential equations, while variation of parameters will need some extra derivation work to get a formula/process we can use on systems.\nLaplace Transforms \u2013 In this section we will work a quick example illustrating how Laplace transforms can be used to solve a system of two linear differential equations.\nModeling \u2013 In this section we\u2019ll take a quick look at some extensions of some of the modeling we did in previous chapters that lead to systems of differential equations. In particular we will look at mixing problems in which we have two interconnected tanks of water, a predator-prey problem in which populations of both are taken into account and a mechanical vibration problem with two masses, connected with a spring and each connected to a wall with a spring.\n\nSeries Solutions to Differential Equations - In this chapter we are going to take a quick look at how to represent the solution to a differential equation with a power series. We will also look at how to solve Euler\u2019s differential equation. In addition, we will do a quick review of power series and Taylor series to help with work in the chapter.\nReview : Power Series \u2013 In this section we give a brief review of some of the basics of power series. Included are discussions of using the Ratio Test to determine if a power series will converge, adding/subtracting power series, differentiating power series and index shifts for power series.\nReview : Taylor Series \u2013 In this section we give a quick reminder on how to construct the Taylor series for a function. Included are derivations for the Taylor series of \\({\\bf e}^{x}\\) and \\(\\cos(x)\\) about \\(x = 0\\) as well as showing how to write down the Taylor series for a polynomial.\nSeries Solutions \u2013 In this section we define ordinary and singular points for a differential equation. We also show who to construct a series solution for a differential equation about an ordinary point. The method illustrated in this section is useful in solving, or at least getting an approximation of the solution, differential equations with coefficients that are not constant.\nEuler Equations \u2013 In this section we will discuss how to solve Euler\u2019s differential equation, \\(ax^{2}y'' + b x y' +c y = 0\\). Note that while this does not involve a series solution it is included in the series solution chapter because it illustrates how to get a solution to at least one type of differential equation at a singular point.\n\nHigher Order Differential Equations - In this chapter we will look at extending many of the ideas of the previous chapters to differential equations with order higher that 2nd order. In a few cases this will simply mean working an example to illustrate that the process doesn\u2019t really change, but in most cases there are some issues to discuss.\nBasic Concepts for \\(n^{\\text{th}}\\) Order Linear Equations \u2013 In this section we\u2019ll start the chapter off with a quick look at some of the basic ideas behind solving higher order linear differential equations. Included will be updated definitions/facts for the Principle of Superposition, linearly independent functions and the Wronskian.\nLinear Homogeneous Differential Equations \u2013 In this section we will extend the ideas behind solving 2nd order, linear, homogeneous differential equations to higher order. As we\u2019ll most of the process is identical with a few natural extensions to repeated real roots that occur more than twice. We will also need to discuss how to deal with repeated complex roots, which are now a possibility. In addition, we will see that the main difficulty in the higher order cases is simply finding all the roots of the characteristic polynomial.\nUndetermined Coefficients \u2013 In this section we work a quick example to illustrate that using undetermined coefficients on higher order differential equations is no different that when we used it on 2nd order differential equations with only one small natural extension.\nVariation of Parameters \u2013 In this section we will give a detailed discussion of the process for using variation of parameters for higher order differential equations. We will also develop a formula that can be used in these cases. We will also see that the work involved in using variation of parameters on higher order differential equations can be quite involved on occasion.\nLaplace Transforms \u2013 In this section we will work a quick example using Laplace transforms to solve a differential equation on a 3rd order differential equation just to say that we looked at one with order higher than 2nd. As we\u2019ll see, outside of needing a formula for the Laplace transform of \\(y'''\\), which we can get from the general formula, there is no real difference in how Laplace transforms are used for higher order differential equations.\nSystems of Differential Equations \u2013 In this section we\u2019ll take a quick look at extending the ideas we discussed for solving \\(2 \\times 2\\) systems of differential equations to systems of size \\(3 \\times 3\\). As we will see they are mostly just natural extensions of what we already know how to do. We will also make a couple of quick comments about \\(4 \\times 4\\) systems.\nSeries Solutions \u2013 In this section we are going to work a quick example illustrating that the process of finding series solutions for higher order differential equations is pretty much the same as that used on 2nd order differential equations.\n\nBoundary Value Problems & Fourier Series - In this chapter we will introduce two topics that are integral to basic partial differential equations solution methods. The first topic, boundary value problems, occur in pretty much every partial differential equation. The second topic, Fourier series, is what makes one of the basic solution techniques work.\nBoundary Value Problems \u2013 In this section we\u2019ll define boundary conditions (as opposed to initial conditions which we should already be familiar with at this point) and the boundary value problem. We will also work a few examples illustrating some of the interesting differences in using boundary values instead of initial conditions in solving differential equations.\nEigenvalues and Eigenfunctions \u2013 In this section we will define eigenvalues and eigenfunctions for boundary value problems. We will work quite a few examples illustrating how to find eigenvalues and eigenfunctions. In one example the best we will be able to do is estimate the eigenvalues as that is something that will happen on a fairly regular basis with these kinds of problems.\nPeriodic Functions and Orthogonal Functions \u2013 In this section we will define periodic functions, orthogonal functions and mutually orthogonal functions. We will also work a couple of examples showing intervals on which \\(\\cos\\left(\\frac{n \\pi x}{L}\\right)\\) and \\(\\sin\\left(\\frac{n \\pi x}{L}\\right)\\) are mutually orthogonal. The results of these examples will be very useful for the rest of this chapter and most of the next chapter.\nFourier Sine Series \u2013 In this section we define the Fourier Sine Series, i.e. representing a function with a series in the form \\(\\sum\\limits_{n = 1}^\\infty {{B_n}\\sin \\left( {\\frac{{n\\pi x}}{L}} \\right)} \\). We will also define the odd extension for a function and work several examples finding the Fourier Sine Series for a function.\nFourier Cosine Series \u2013 In this section we define the Fourier Cosine Series, i.e. representing a function with a series in the form \\(\\sum\\limits_{n = 0}^\\infty {{A_n}\\cos \\left( {\\frac{{n\\pi x}}{L}} \\right)} \\). We will also define the even extension for a function and work several examples finding the Fourier Cosine Series for a function.\nFourier Series \u2013 In this section we define the Fourier Series, i.e. representing a function with a series in the form \\(\\sum\\limits_{n = 0}^\\infty {{A_n}\\cos \\left( {\\frac{{n\\pi x}}{L}} \\right)} + \\sum\\limits_{n = 1}^\\infty {{B_n}\\sin \\left( {\\frac{{n\\pi x}}{L}} \\right)} \\). We will also work several examples finding the Fourier Series for a function.\nConvergence of Fourier Series \u2013 In this section we will define piecewise smooth functions and the periodic extension of a function. In addition, we will give a variety of facts about just what a Fourier series will converge to and when we can expect the derivative or integral of a Fourier series to converge to the derivative or integral of the function it represents.\n\nPartial Differential Equations - In this chapter we introduce Separation of Variables one of the basic solution techniques for solving partial differential equations. Included are partial derivations for the Heat Equation and Wave Equation. In addition, we give solutions to examples for the heat equation, the wave equation and Laplace\u2019s equation.\nThe Heat Equation \u2013 In this section we will do a partial derivation of the heat equation that can be solved to give the temperature in a one dimensional bar of length L. In addition, we give several possible boundary conditions that can be used in this situation. We also define the Laplacian in this section and give a version of the heat equation for two or three dimensional situations.\nThe Wave Equation \u2013 In this section we do a partial derivation of the wave equation which can be used to find the one dimensional displacement of a vibrating string. In addition, we also give the two and three dimensional version of the wave equation.\nTerminology \u2013 In this section we take a quick look at some of the terminology we will be using in the rest of this chapter. In particular we will define a linear operator, a linear partial differential equation and a homogeneous partial differential equation. We also give a quick reminder of the Principle of Superposition.\nSeparation of Variables \u2013 In this section show how the method of Separation of Variables can be applied to a partial differential equation to reduce the partial differential equation down to two ordinary differential equations. We apply the method to several partial differential equations. We do not, however, go any farther in the solution process for the partial differential equations. That will be done in later sections. The point of this section is only to illustrate how the method works.\nSolving the Heat Equation \u2013 In this section we go through the complete separation of variables process, including solving the two ordinary differential equations the process generates. We will do this by solving the heat equation with three different sets of boundary conditions. Included is an example solving the heat equation on a bar of length \\(L\\) but instead on a thin circular ring.\nHeat Equation with Non-Zero Temperature Boundaries \u2013 In this section we take a quick look at solving the heat equation in which the boundary conditions are fixed, non-zero temperature. Note that this is in contrast to the previous section when we generally required the boundary conditions to be both fixed and zero.\nLaplace\u2019s Equation \u2013 In this section we discuss solving Laplace\u2019s equation. As we will see this is exactly the equation we would need to solve if we were looking to find the equilibrium solution (i.e. time independent) for the two dimensional heat equation with no sources. We will also convert Laplace\u2019s equation to polar coordinates and solve it on a disk of radius \\(a\\).\nVibrating String \u2013 In this section we solve the one dimensional wave equation to get the displacement of a vibrating string.\nSummary of Separation of Variables \u2013 In this final section we give a quick summary of the method of separation of variables for solving partial differential equations.\n\n\n[Contact Me]\u00a0[Privacy Statement]\u00a0[Site Help & FAQ]\u00a0[Terms of Use]\n\u00a9 2003 - 2025 Paul Dawkins\nPage Last Modified : 6/26/2023",
    "Systems of Differential Equations and Phase Plane Analysis": "Loading [MathJax]/jax/output/CommonHTML/fonts/TeX/fontdata.js\nPaul's Online Notes\nNOTES  QUICK NAV  DOWNLOAD  \n\nHome / Differential Equations / Systems of DE's / Phase Plane\n Prev. Section  Notes  Next Section \n\u00a0\nSection 5.6 : Phase Plane\n\nBefore proceeding with actually solving systems of differential equations there\u2019s one topic that we need to take a look at. This is a topic that\u2019s not always taught in a differential equations class but in case you\u2019re in a course where it is taught we should cover it so that you are prepared for it.\n\nLet\u2019s start with a general homogeneous system,\n\n\u2192\nx\n\u2032=A\n\u2192\nx\n\nNotice that\n\n\u2192\nx\n=\n\u2192\n0\n\nis a solution to the system of differential equations. What we\u2019d like to ask is, do the other solutions to the system approach this solution as t increases or do they move away from this solution? We did something similar to this when we classified equilibrium solutions in a previous section. In fact, what we\u2019re doing here is simply an extension of this idea to systems of differential equations.\n\nThe solution \n\u2192\nx\n=\n\u2192\n0\n is called an equilibrium solution for the system. As with the single differential equations case, equilibrium solutions are those solutions for which\n\nA\n\u2192\nx\n=\n\u2192\n0\n\nWe are going to assume that A is a nonsingular matrix and hence will have only one solution,\n\n\u2192\nx\n=\n\u2192\n0\n\nand so we will have only one equilibrium solution.\n\nBack in the single differential equation case recall that we started by choosing values of y and plugging these into the function f(y) to determine values of y\u2032. We then used these values to sketch tangents to the solution at that particular value of y. From this we could sketch in some solutions and use this information to classify the equilibrium solutions.\n\nWe are going to do something similar here, but it will be slightly different as well. First, we are going to restrict ourselves down to the 2\u00d72 case. So, we\u2019ll be looking at systems of the form,\n\nx\u20321\t=ax1+bx2\nx\u20322\t=cx1+dx2\t\u21d2\n\u2192\nx\n\u2032=(a\tb\nc\td)\n\u2192\nx\n\nSolutions to this system will be of the form,\n\n\u2192\nx\n=(x1(t)\nx2(t))\n\nand our single equilibrium solution will be,\n\n\u2192\nx\n=(0\n0)\n\nIn the single differential equation case we were able to sketch the solution, y(t) in the y-t plane and see actual solutions. However, this would somewhat difficult in this case since our solutions are actually vectors. What we\u2019re going to do here is think of the solutions to the system as points in the x1x2 plane and plot these points. Our equilibrium solution will correspond to the origin of x1x2. plane and the x1x2 plane is called the phase plane.\n\nTo sketch a solution in the phase plane we can pick values of t and plug these into the solution. This gives us a point in the x1x2 or phase plane that we can plot. Doing this for many values of t will then give us a sketch of what the solution will be doing in the phase plane. A sketch of a particular solution in the phase plane is called the trajectory of the solution. Once we have the trajectory of a solution sketched we can then ask whether or not the solution will approach the equilibrium solution as t increases.\n\nWe would like to be able to sketch trajectories without actually having solutions in hand. There are a couple of ways to do this. We\u2019ll look at one of those here and we\u2019ll look at the other in the next couple of sections.\n\nOne way to get a sketch of trajectories is to do something similar to what we did the first time we looked at equilibrium solutions. We can choose values of \n\u2192\nx\n (note that these will be points in the phase plane) and compute A\n\u2192\nx\n. This will give a vector that represents \n\u2192\nx\n\u2032at that particular solution. As with the single differential equation case this vector will be tangent to the trajectory at that point. We can sketch a bunch of the tangent vectors and then sketch in the trajectories.\n\nThis is a fairly work intensive way of doing these and isn\u2019t the way to do them in general. However, it is a way to get trajectories without doing any solution work. All we need is the system of differential equations. Let\u2019s take a quick look at an example.\n\nExample 1 Sketch some trajectories for the system,\nx\u20321\t=x1+2x2\nx\u20322\t=3x1+2x2\t\u21d2\n\u2192\nx\n\u2032=(1\t2\n3\t2)\n\u2192\nx\nShow Solution \n\nAs we noted earlier this is not generally the way that we will sketch trajectories. All we really need to get the trajectories are the eigenvalues and eigenvectors of the matrix A. We will see how to do this over the next couple of sections as we solve the systems.\n\nHere are a few more phase portraits so you can see some more possible examples. We\u2019ll actually be generating several of these throughout the course of the next couple of sections.\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\nNot all possible phase portraits have been shown here. These are here to show you some of the possibilities. Make sure to notice that several kinds can be either asymptotically stable or unstable depending upon the direction of the arrows.\n\nNotice the difference between stable and asymptotically stable. In an asymptotically stable node or spiral all the trajectories will move in towards the equilibrium point as t increases, whereas a center (which is always stable) trajectory will just move around the equilibrium point but never actually move in towards it.\n\n[Contact Me]\u00a0[Privacy Statement]\u00a0[Site Help & FAQ]\u00a0[Terms of Use]\n\u00a9 2003 - 2025 Paul Dawkins\nPage Last Modified : 11/16/2022\n\nSkip to main content\nThe Ordinary Differential Equations Project\n\nThomas W. Judson\n\nContents\nIndex\nPrev\nUp\nNext\nFront Matter\nColophon\nDedication\nPreface\nAcknowledgements\n1\nA First Look at Differential Equations\nModeling with Differential Equations\nSeparable Differential Equations\nGeometric and Quantitative Analysis\nAnalyzing Equations Numerically\nFirst-Order Linear Equations\nExistence and Uniqueness of Solutions\nBifurcations\nProjects for First-Order Differential Equations\n2\nSystems of Differential Equations\nModeling with Systems\nThe Geometry of Systems\nNumerical Techniques for Systems\nSolving Systems Analytically\nProjects for Systems of Differential Equations\n3\nLinear Systems\nLinear Algebra in a Nutshell\nPlanar Systems\nPhase Plane Analysis of Linear Systems\nComplex Eigenvalues\nRepeated Eigenvalues\nChanging Coordinates\nThe Trace-Determinant Plane\nLinear Systems in Higher Dimensions\nThe Matrix Exponential\nProjects Systems of Linear Differential Equations\n4\nSecond-Order Linear Equations\nHomogeneous Linear Equations\nForcing\nSinusoidal Forcing\nForcing and Resonance\nProjects for Second-Order Differential Equations\n5\nNonlinear Systems\nLinearization\nHamiltonian Systems\nMore Nonlinear Mechanics\nThe Hopf Bifurcation\nProjects\n6\nThe Laplace Transform\nThe Laplace Transform\nSolving Initial Value Problems\nDelta Functions and Forcing\nConvolution\nProjects for Laplace Transforms\nReference\nA\nGNU Free Documentation License\nB\nHints and Answers to Selected Exercises\nReadings and References\nC\nNotation\nIndex\nColophon\nAuthored in PreTeXt\n3.3 Phase Plane Analysis of Linear Systems\nObjectives\n\nTo understand that given a system of linear differential equations\n\n\\begin{equation*} \\begin{pmatrix} dx/dt \\\\ dy/dt \\end{pmatrix} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = A \\begin{pmatrix} x \\\\ y \\end{pmatrix}, \\end{equation*}\n\nwith distinct real eigenvalues, we can classify the origin as sink, saddle, or source depending on the signs of the eigenvalues.\n\nIn Section\u00a03.2, we learned how to solve the system\n\n\\begin{equation*} \\begin{pmatrix} dx/dt \\\\ dy/dt \\end{pmatrix} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = A \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\end{equation*}\n\nprovided the system has distinct real eigenvalues. If \\(A\\) has distinct real eigenvalues \\(\\lambda\\) and \\(\\mu\\) with eigenvectors \\(\\mathbf u\\) and \\(\\mathbf v\\text{,}\\) respectively, then the general solution of the system is\n\n\\begin{equation*} \\mathbf x(t) = c_1 e^{\\lambda t} \\mathbf u + c_2 e^{\\mu t} \\mathbf v. \\end{equation*}\n\nFurthermore, we can use the general solution of such a system to find the straight-line solutions to the system. If \\(c_2 = 0\\text{,}\\) then all solutions will lie along the line in the \\(xy\\)-plane that contains the vector \\(\\mathbf u\\text{.}\\) Similarly, if \\(c_1 = 0\\text{,}\\) then all solutions will lie along the line in the \\(xy\\)-plane that contains the vector \\(\\mathbf v\\text{.}\\)\n\n3.3.1 The Case \\(\\lambda_1 \\lt 0 \\lt \\lambda_2\\)\nExample 3.3.1. \n\nLet us consider the special case of the system \\({\\mathbf x}' = A {\\mathbf x}\\text{,}\\) where \\(\\lambda_1 \\lt 0 \\lt \\lambda_2\\) and\n\n\\begin{equation*} A = \\begin{pmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{pmatrix}. \\end{equation*}\n\nSince this is a decoupled system,\n\n\\begin{align*} \\frac{dx}{dt} & = \\lambda_1 x\\\\ \\frac{dy}{dt} & = \\lambda_2 y, \\end{align*}\n\nwe already know how to find the solutions. However, in keeping with the spirit of our investigation, we will find the eigenvalues of \\(A\\text{.}\\) The characteristic equation of \\(A\\) is\n\n\\begin{equation*} (\\lambda - \\lambda_1)(\\lambda - \\lambda_2) = 0, \\end{equation*}\n\nand our eigenvalues are \\(\\lambda_1\\) and \\(\\lambda_2\\text{.}\\) It is easy to see that we can associate eigenvectors \\((1,0)\\) and \\((0, 1)\\) to \\(\\lambda_1\\) and \\(\\lambda_2\\text{,}\\) respectively. Thus, our general solution is\n\n\\begin{equation*} {\\mathbf x}(t) = c_1 e^{\\lambda_1 t} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_2 e^{\\lambda_2 t} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}. \\end{equation*}\nFigure 3.3.3. Saddle phase portrait\n\nSince \\(\\lambda_1 \\lt 0\\text{,}\\) the straight-line solutions of the form \\(c_1 e^{\\lambda_1 t} (1, 0)\\) lie on the \\(x\\)-axis. These solutions approach zero as \\(t \\to \\infty\\text{.}\\) On the other hand, the solutions \\(c_2 e^{\\lambda_2 t} (0, 1)\\) lie on the \\(y\\)-axis and approach infinity as \\(t \\to \\infty\\text{.}\\) The \\(x\\)-axis is a stable line of solutions, while the \\(y\\)-axis is an unstable line of solutions. All other solutions\n\n\\begin{equation*} {\\mathbf x}(t) = c_1 e^{\\lambda_1 t} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_2 e^{\\lambda_2 t} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\end{equation*}\n\n(with \\(c_1, c_2 \\neq 0\\)) tend to infinity in the direction of the unstable line, since \\({\\mathbf x}(t)\\) approaches \\((0, c_2 e^{\\lambda_2 t} )\\) as \\(t \\to \\infty\\text{.}\\) The phase portrait for the system\n\n\\begin{align*} x' & = -x\\\\ y' & = y \\end{align*}\n\nis given in Figure\u00a03.3.3. The equilibrium point of such systems is called a saddle.\n\nIn general, a straight-line solution is called a stable line of solutions if all solutions approach \\((0,0)\\text{.}\\) A straight-line solution is called an unstable line if all of the non-zero solutions approach infinity.\n\nExample 3.3.4. \n\nFor the general case, where \\(A\\) has eigenvalues \\(\\lambda_1 \\lt 0 \\lt \\lambda_2\\text{,}\\) we always have a stable line of solutions and an unstable line of solutions. All other solutions approach the unstable line as \\(t \\to \\infty\\) and the stable line as \\(t \\to - \\infty\\text{.}\\)\n\nActivity 3.3.1. Planar Systems with Eigenvalues of Different Signs.  \n\nConsider the system \\(d\\mathbf x/dt = A \\mathbf x\\text{,}\\) where\n\n\\begin{equation*} A = \\begin{pmatrix} 8 \\amp -3 \\\\ 18 \\amp -7 \\end{pmatrix} \\end{equation*}\n(a) \n\nFind the eigenvalues of \\(A\\text{.}\\) You should find distinct real eigenvalues \\(\\lambda\\) and \\(\\mu\\text{.}\\)\n\n(b) \n\nFind eigenvectors \\(\\mathbf v_1\\) and \\(\\mathbf v_2\\) for the eigenvalues \\(\\lambda\\) and \\(\\mu\\text{,}\\) respectively.\n\n(c) \n\nFind the straight-line solutions of \\(d\\mathbf x/dt = A \\mathbf x\\text{.}\\) Plot the solutions in the \\(xy\\)-plane.\n\n(d) \n\nSketch several solution curves for the system \\(d\\mathbf x/dt = A \\mathbf x\\text{.}\\) What do you notice about the solution curves, especially with respect to the straight-line solutions?\n\n3.3.2 The Case \\(\\lambda_1 \\lt \\lambda_2 \\lt 0\\)\n\nSuppose \\(\\lambda_1 \\lt \\lambda_2 \\lt 0\\) and consider the diagonal system\n\n\\begin{equation*} \\begin{pmatrix} x'(t) \\\\ y'(t) \\end{pmatrix} = \\begin{pmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{pmatrix} \\begin{pmatrix} x(t) \\\\ y(t) \\end{pmatrix}. \\end{equation*}\n\nThe general solution of this system is\n\n\\begin{equation*} {\\mathbf x}(t) = c_1 e^{\\lambda_1 t} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_2 e^{\\lambda_2 t} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\end{equation*}\n\nbut unlike the case of the saddle, all solutions tend towards the origin as \\(t \\to \\infty\\text{.}\\) To see how the solutions approach the origin, we will compute \\(dy/dx\\) for \\(c_2 \\neq 0\\text{.}\\) If\n\n\\begin{align*} x(t) & = c_1 e^{\\lambda_1 t}\\\\ y(t) & = c_2 e^{\\lambda_2 t}, \\end{align*}\n\nthen\n\n\\begin{equation*} \\frac{dy}{dx} = \\frac{y'(t)}{x'(t)} = \\frac{\\lambda_2 c_2 e^{\\lambda_2 t}}{\\lambda_1c_1 e^{\\lambda_1 t}} = \\frac{\\lambda_2 c_2}{\\lambda_1c_1 } e^{(\\lambda_2 - \\lambda_1) t}. \\end{equation*}\n\nSince \\(\\lambda_2 - \\lambda_1 \\gt 0\\text{,}\\) the derivative, \\(dy/dx\\text{,}\\) must approach \\(\\pm \\infty\\text{,}\\) provided \\(c_2 \\neq 0\\text{.}\\) Therefore, the solutions tend towards the origin tangentially to the \\(y\\)-axis (Figure\u00a03.3.6). We say that the equilibrium point for this system is a sink.\n\nFigure 3.3.6. Sink phase portrait\n\nSince \\(\\lambda_1 \\lt \\lambda_2 \\lt 0\\text{,}\\) we say that \\(\\lambda_1\\) is the dominant eigenvalue. The \\(x\\)-coordinates of the solutions approach the origin much faster than the \\(y\\)-coordinates.\n\nTo see what happens in the general case, suppose that \\(\\lambda_1 \\lt \\lambda_2 \\lt 0\\text{,}\\) the eigenvectors associated with \\(\\lambda_1\\) and \\(\\lambda_2\\) are \\((u_1, u_2)\\) and \\((v_1, v_2)\\text{,}\\) respectively. The general solution of our system is\n\n\\begin{equation*} {\\mathbf x}(t) = c_1 e^{\\lambda_1 t} \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix} + c_2 e^{\\lambda_2 t} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}. \\end{equation*}\n\nThe slope of a solution curve at \\((x, y)\\) is given by\n\n\\begin{align*} \\frac{dy}{dx} & = \\frac{\\lambda_1 c_1 e^{\\lambda_1 t} u_2 + \\lambda_2 c_2 e^{\\lambda_2 t} v_2} {\\lambda_1 c_1 e^{\\lambda_1 t} u_1 + \\lambda_2 c_2 e^{\\lambda_2 t} v_1}\\\\ & = \\left( \\frac{\\lambda_1 c_1 e^{\\lambda_1 t} u_2 + \\lambda_2 c_2 e^{\\lambda_2 t} v_2} {\\lambda_1 c_1 e^{\\lambda_1 t} u_1 + \\lambda_2 c_2 e^{\\lambda_2 t} v_1} \\right) \\frac{e^{-\\lambda_2 t}}{e^{-\\lambda_2 t}}\\\\ & = \\frac{\\lambda_1 c_1 e^{(\\lambda_1 - \\lambda_2) t} u_2 + \\lambda_2 c_2 v_2} {\\lambda_1 c_1 e^{(\\lambda_1 - \\lambda_2) t} u_1 + \\lambda_2 c_2 v_1}. \\end{align*}\n\nThis last expression tends toward the slope \\(v_2/v_1\\) of the eigenvector of \\(\\lambda_2\\) (unless \\(c_2 = 0\\)). If \\(c_2 = 0\\text{,}\\) then we have the straight-line solution corresponding to the eigenvalue \\(\\lambda_1\\text{.}\\) Hence, all the solutions for this case (except those on the straight-line belonging to the dominant eigenvalue) tend toward the origin tangentially to the straight-line solution corresponding to the weaker eigenvalue, \\(\\lambda_2\\text{.}\\)\n\nExample 3.3.7. \nActivity 3.3.2. Planar Systems with Two Negative Eigenvalues.  \n\nConsider the system \\(d\\mathbf x/dt = A \\mathbf x\\text{,}\\) where\n\n\\begin{equation*} A = \\begin{pmatrix} 6 \\amp 14 \\\\ -4 \\amp -9 \\end{pmatrix} \\end{equation*}\n(a) \n\nFind the eigenvalues of \\(A\\text{.}\\) You should find distinct real eigenvalues \\(\\lambda\\) and \\(\\mu\\text{.}\\)\n\n(b) \n\nFind eigenvectors \\(\\mathbf v_1\\) and \\(\\mathbf v_2\\) for the eigenvalues \\(\\lambda\\) and \\(\\mu\\text{,}\\) respectively.\n\n(c) \n\nFind the straight-line solutions of \\(d\\mathbf x/dt = A \\mathbf x\\text{.}\\) Plot the solutions in the \\(xy\\)-plane.\n\n(d) \n\nSketch several solution curves for the system \\(d\\mathbf x/dt = A \\mathbf x\\text{.}\\) What do you notice about the solution curves, especially with respect to the straight-line solutions?\n\n(e) \n\nWhich of the two eigenvalues is the dominant eigenvalue? Why?\n\n3.3.3 The Case \\(\\lambda_1 \\gt \\lambda_2 \\gt 0\\)\n\nIf \\(\\lambda_1 \\gt \\lambda_2 \\gt 0\\text{,}\\) we can regard our direction field as the negative of the direction field of the previous case. The general solution and the direction field are the same, but the arrows are reversed (Figure\u00a03.3.9). In this case, we say that the equilibrium point is a source.\n\nFigure 3.3.9. Source phase portrait\nExample 3.3.10. \n3.3.4 Important Lessons\n\nGiven a system of linear differential equations\n\n\\begin{equation*} \\begin{pmatrix} dx/dt \\\\ dy/dt \\end{pmatrix} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = A \\begin{pmatrix} x \\\\ y \\end{pmatrix}, \\end{equation*}\n\nwe can use the eigenvalues of \\(A\\) to find and classify the solutions of the system.\n\nIf\n\n\\begin{equation*} A = \\begin{pmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{pmatrix}, \\end{equation*}\n\nthen \\(A\\) has two distinct real eigenvalues. The general solution to the system \\({\\mathbf x}' = A {\\mathbf x}\\) is\n\n\\begin{equation*} {\\mathbf x}(t) = \\alpha e^{\\lambda_1 t} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\beta e^{\\lambda_2 t} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}. \\end{equation*}\n\nFor the case \\(\\lambda_1 \\lt 0 \\lt \\lambda_2\\text{,}\\) the equilibrium point of the system \\({\\mathbf x}' = A {\\mathbf x}\\) is a saddle.\n\nFor the case \\(\\lambda_1 \\lt \\lambda_2 \\lt 0\\text{,}\\) the equilibrium point of the system \\({\\mathbf x}' = A {\\mathbf x}\\) is a sink.\n\nFor the case \\(0 \\lt \\lambda_1 \\lt \\lambda_2\\text{,}\\) the equilibrium point of the system \\({\\mathbf x}' = A {\\mathbf x}\\) is a source.\n\n3.3.5 Reading Questions\n1. \n\nWhat is a stable line of solutions?\n\n2. \n\nFor a \\(2 \\times 2\\) linear system with distinct real eigenvalues, what are the three different possibilities for the phase plane of the system?\n\n3.3.6 Exercises\nPhase Plane Analysis of Linear Systems with Distinct Real Eigenvalues. \n\nFor each of the linear systems \\(d\\mathbf x/dt = A \\mathbf x\\) in Exercise Group\u00a03.3.6.1\u20134\n\nFind the eigenvalues of \\(A\\text{.}\\)\n\nWhat is the dominant eigenvalue?\n\nFind the eigenvectors for each eigenvalue of \\(A\\text{.}\\)\n\nWhat are the straight-line solutions of \\(d\\mathbf x/dt = A \\mathbf x\\text{?}\\)\n\nDescribe the nature of the equilibrium solution at \\(\\mathbf 0\\text{.}\\)\n\nSketch the phase plane and several solution curves.\n\n1.\n\\begin{equation*} A = \\begin{pmatrix} -1 \\amp 2 \\\\ -6 \\amp 6 \\end{pmatrix} \\end{equation*}\n2.\n\\begin{equation*} A = \\begin{pmatrix} -12 \\amp 30 \\\\ -5 \\amp 13 \\end{pmatrix} \\end{equation*}\n3.\n\\begin{equation*} A = \\begin{pmatrix} -9 \\amp -2 \\\\ 10 \\amp 0 \\end{pmatrix} \\end{equation*}\n4.\n\\begin{equation*} A = \\begin{pmatrix} 11 \\amp 8 \\\\ -12 \\amp -9 \\end{pmatrix} \\end{equation*}\n5. \n\nSolve each linear systems \\(d\\mathbf x/dt = A \\mathbf x\\) in Exercise Group\u00a03.3.6.1\u20134 for the initial condition \\(\\mathbf x(0) = (2,2)\\text{.}\\)\n\n6. \n\nConsider the linear system \\(d \\mathbf x/dt = A \\mathbf x\\text{,}\\) where\n\n\\begin{equation*} A = \\begin{pmatrix} 3 \\amp 2 \\\\ 3 \\amp -2 \\end{pmatrix}. \\end{equation*}\n\nSuppose the initial conditions for the solution curve are \\(x(0) = 1\\) and \\(y(0) = 1\\text{.}\\) We can use the following Sage code to plot the phase portrait of this system, including the straight-line solutions and a solution curve.\n\n1\nx, y, t = var('x y t') #declare the variables\n2\nF = [3*x + 2*y, 3*x - 2*y] #declare the system\n3\n# normalize the vector fields so that all of the arrows are the same length\n4\nn = sqrt(F[0]^2 + F[1]^2)\n5\n# plot the vector field\n6\np = plot_vector_field((F[0]/n, F[1]/n), (x, -4, 4), (y, -4, 4), aspect_ratio = 1)\n7\n# solve the system for the initial condition t = 0, x = 1, y = 1\n8\nP1 = desolve_system_rk4(F, [x, y], ics=[0, 1, 1], ivar = t, end_points = 5, step = 0.01)\n9\n# grab the x and y values\n10\nS1 = [ [j, k] for i, j, k in P1]\n11\n# plot the solution\n12\n# Setting xmin, xmax, ymin, ymax will clip the window\n13\n# Try plotting without doing this to see what happens\n14\np += line(S1, thickness = 2, axes_labels=['$x(t)$','$y(t)$'], xmin = -4, xmax = 4, ymin = -4, ymax = 4) \n15\n# plot the straight-line solutions\n16\np += line([(-4, -2), (4, 2)], thickness = 2, color = \"red\") \n17\np += line([(-4/3, 4), (4/3, -4)], thickness = 2, color = \"red\")\n18\np\nEvaluate (Sage)\n\nUse Sage to graph the direction field for the system linear systems \\(d\\mathbf x/dt = A \\mathbf x\\) in Exercise Group\u00a03.3.6.1\u20134. Plot a solution curve for the initial condition \\(\\mathbf x(0) = (2,2)\\text{.}\\) Be sure to show the corresponding straight-line solutions on your graph.\n\n1\n\u200b\nEvaluate (Sage)\n\nJump to content\nMain menu\nSearch\nDonate\nCreate account\nLog in\nPersonal tools\nContents hide\n(Top)\nExample of a linear system\nToggle Example of a linear system subsection\nSolving using eigenvalues\nEigenvectors and nodes\nRepeated eigenvalues\nComplex eigenvalues\nSee also\nReferences\nExternal links\nPhase plane\n8 languages\nArticle\nTalk\nRead\nEdit\nView history\nTools\nAppearance hide\nFrom Wikipedia, the free encyclopedia\nDifferential equations\n\n\nScope\n\n\nFields\n\n\nClassification\n\n\nTypes\n\n\nRelation to processes\n\n\nSolution\n\n\nExistence and uniqueness\n\n\nGeneral topics\n\n\nSolution methods\n\n\nPeople\n\n\nList\n\nvte\n\nIn applied mathematics, in particular the context of nonlinear system analysis, a phase plane is a visual display of certain characteristics of certain kinds of differential equations; a coordinate plane with axes being the values of the two state variables, say (x, y), or (q, p) etc. (any pair of variables). It is a two-dimensional case of the general n-dimensional phase space.\n\nThe phase plane method refers to graphically determining the existence of limit cycles in the solutions of the differential equation.\n\nThe solutions to the differential equation are a family of functions. Graphically, this can be plotted in the phase plane like a two-dimensional vector field. Vectors representing the derivatives of the points with respect to a parameter (say time t), that is (dx/dt, dy/dt), at representative points are drawn. With enough of these arrows in place the system behaviour over the regions of plane in analysis can be visualized and limit cycles can be easily identified.\n\nThe entire field is the phase portrait, a particular path taken along a flow line (i.e. a path always tangent to the vectors) is a phase path. The flows in the vector field indicate the time-evolution of the system the differential equation describes.\n\nIn this way, phase planes are useful in visualizing the behaviour of physical systems; in particular, of oscillatory systems such as predator-prey models (see Lotka\u2013Volterra equations). In these models the phase paths can \"spiral in\" towards zero, \"spiral out\" towards infinity, or reach neutrally stable situations called centres where the path traced out can be either circular, elliptical, or ovoid, or some variant thereof. This is useful in determining if the dynamics are stable or not.[1]\n\nOther examples of oscillatory systems are certain chemical reactions with multiple steps, some of which involve dynamic equilibria rather than reactions that go to completion. In such cases one can model the rise and fall of reactant and product concentration (or mass, or amount of substance) with the correct differential equations and a good understanding of chemical kinetics.[2]\n\nExample of a linear system[edit]\n\nA two-dimensional system of linear differential equations can be written in the form:[1]\n\n\ud835\udc51\n\ud835\udc65\n\ud835\udc51\n\ud835\udc61\n\t\n=\n\ud835\udc34\n\ud835\udc65\n+\n\ud835\udc35\n\ud835\udc66\n\n\n\ud835\udc51\n\ud835\udc66\n\ud835\udc51\n\ud835\udc61\n\t\n=\n\ud835\udc36\n\ud835\udc65\n+\n\ud835\udc37\n\ud835\udc66\n\nwhich can be organized into a matrix equation:\n\n\t\n\ud835\udc51\n\ud835\udc51\n\ud835\udc61\n[\n\ud835\udc65\n\n\n\ud835\udc66\n]\n=\n[\n\ud835\udc34\n\t\n\ud835\udc35\n\n\n\ud835\udc36\n\t\n\ud835\udc37\n]\n[\n\ud835\udc65\n\n\n\ud835\udc66\n]\n\n\t\n\ud835\udc51\n\ud835\udc63\n\ud835\udc51\n\ud835\udc61\n=\n\ud835\udc34\n\ud835\udc63\n.\n\nwhere A is the 2 \u00d7 2 coefficient matrix above, and v = (x, y) is a coordinate vector of two independent variables.\n\nSuch systems may be solved analytically, for this case by integrating:[3]\n\n\ud835\udc51\n\ud835\udc66\n\ud835\udc51\n\ud835\udc65\n=\n\ud835\udc36\n\ud835\udc65\n+\n\ud835\udc37\n\ud835\udc66\n\ud835\udc34\n\ud835\udc65\n+\n\ud835\udc35\n\ud835\udc66\n\nalthough the solutions are implicit functions in x and y, and are difficult to interpret.[1]\n\nSolving using eigenvalues[edit]\n\nMore commonly they are solved with the coefficients of the right hand side written in matrix form using eigenvalues \u03bb, given by the determinant:\n\ndet\n(\n[\n\ud835\udc34\n\t\n\ud835\udc35\n\n\n\ud835\udc36\n\t\n\ud835\udc37\n]\n\u2212\n\ud835\udf06\n\ud835\udc3c\n)\n=\n0\n\nand eigenvectors:\n\n[\n\ud835\udc34\n\t\n\ud835\udc35\n\n\n\ud835\udc36\n\t\n\ud835\udc37\n]\n\ud835\udc65\n=\n\ud835\udf06\n\ud835\udc65\n\nThe eigenvalues represent the powers of the exponential components and the eigenvectors are coefficients. If the solutions are written in algebraic form, they express the fundamental multiplicative factor of the exponential term. Due to the nonuniqueness of eigenvectors, every solution arrived at in this way has undetermined constants c1, c2, \u2026, cn.\n\nThe general solution is:\n\n\ud835\udc65\n=\n[\n\ud835\udc58\n1\n\n\n\ud835\udc58\n2\n]\n\ud835\udc50\n1\n\ud835\udc52\n\ud835\udf06\n1\n\ud835\udc61\n+\n[\n\ud835\udc58\n3\n\n\n\ud835\udc58\n4\n]\n\ud835\udc50\n2\n\ud835\udc52\n\ud835\udf06\n2\n\ud835\udc61\n.\n\nwhere \u03bb1 and \u03bb2 are the eigenvalues, and (k1, k2), (k3, k4) are the basic eigenvectors. The constants c1 and c2 account for the nonuniqueness of eigenvectors and are not solvable unless an initial condition is given for the system.\n\nThe above determinant leads to the characteristic polynomial:\n\n\ud835\udf06\n2\n\u2212\n(\n\ud835\udc34\n+\n\ud835\udc37\n)\n\ud835\udf06\n+\n(\n\ud835\udc34\n\ud835\udc37\n\u2212\n\ud835\udc35\n\ud835\udc36\n)\n=\n0\n\nwhich is just a quadratic equation of the form:\n\n\ud835\udf06\n2\n\u2212\n\ud835\udc5d\n\ud835\udf06\n+\n\ud835\udc5e\n=\n0\n\nwhere\n\ud835\udc5d\n=\n\ud835\udc34\n+\n\ud835\udc37\n=\nt\nr\n(\n\ud835\udc34\n)\n,\n(\"tr\" denotes trace) and\n\ud835\udc5e\n=\n\ud835\udc34\n\ud835\udc37\n\u2212\n\ud835\udc35\n\ud835\udc36\n=\ndet\n(\n\ud835\udc34\n)\n.\n\nThe explicit solution of the eigenvalues are then given by the quadratic formula:\n\n\ud835\udf06\n=\n1\n2\n(\n\ud835\udc5d\n\u00b1\n\u0394\n)\n\nwhere\n\u0394\n=\n\ud835\udc5d\n2\n\u2212\n4\n\ud835\udc5e\n.\n\nEigenvectors and nodes[edit]\n\nThe eigenvectors and nodes determine the profile of the phase paths, providing a pictorial interpretation of the solution to the dynamical system, as shown next.\n\nClassification of equilibrium points of a linear autonomous system.[1] These profiles also arise for non-linear autonomous systems in linearized approximations.\n\nThe phase plane is then first set-up by drawing straight lines representing the two eigenvectors (which represent stable situations where the system either converges towards those lines or diverges away from them). Then the phase plane is plotted by using full lines instead of direction field dashes. The signs of the eigenvalues indicate the phase plane's behaviour:\n\nIf the signs are opposite, the intersection of the eigenvectors is a saddle point.\nIf the signs are both positive, the eigenvectors represent stable situations that the system diverges away from, and the intersection is an unstable node.\nIf the signs are both negative, the eigenvectors represent stable situations that the system converges towards, and the intersection is a stable node.\n\nThe above can be visualized by recalling the behaviour of exponential terms in differential equation solutions.\n\nRepeated eigenvalues[edit]\n\nThis example covers only the case for real, separate eigenvalues. Real, repeated eigenvalues require solving the coefficient matrix with an unknown vector and the first eigenvector to generate the second solution of a two-by-two system. However, if the matrix is symmetric, it is possible to use the orthogonal eigenvector to generate the second solution.\n\nComplex eigenvalues[edit]\n\nComplex eigenvalues and eigenvectors generate solutions in the form of sines and cosines as well as exponentials. One of the simplicities in this situation is that only one of the eigenvalues and one of the eigenvectors is needed to generate the full solution set for the system.\n\nSee also[edit]\nPhase line, 1-dimensional case\nPhase space, n-dimensional case\nPhase portrait\nReferences[edit]\n^ a b c d D.W. Jordan; P. Smith (2007). Non-Linear Ordinary Differential Equations: Introduction for Scientists and Engineers (4th\u00a0ed.). Oxford University Press. ISBN\u00a0978-0-19-920825-8.\n^ K.T. Alligood; T.D. Sauer; J.A. Yorke (1996). Chaos: An Introduction to Dynamical Systems. Springer. ISBN\u00a0978-0-38794-677-1.\n^ W.E. Boyce; R.C. Diprima (1986). Elementary Differential Equations and Boundary Value Problems (4th\u00a0ed.). John Wiley & Sons. ISBN\u00a00-471-83824-1.\nExternal links[edit]\nLamar University, Online Math Notes - Phase Plane, P. Dawkins\nLamar University, Online Math Notes - Systems of Differential Equations, P. Dawkins\nOverview of the phase plane method\nCategories: Nonlinear controlOrdinary differential equations\nThis page was last edited on 20 April 2024, at 06:52\u00a0(UTC).\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\nPrivacy policy\nAbout Wikipedia\nDisclaimers\nContact Wikipedia\nCode of Conduct\nDevelopers\nStatistics\nCookie statement\nMobile view",
    "Numerical Methods for Solving Differential Equations, including Euler's Method and Runge-Kutta Methods": "Kansas State University\nSearch K-State web, people, directories\nBrowse A-Z\nSign in\nMathematics Department\nMath 340 Home, Textbook Contents, Online Homework Home\n\nRunge-Kutta Methods\nDiscussion\nEuler's method and the improved Euler's method are the simplest examples of a whole family of numerical methods to approximate the solutions of differential equations called Runge-Kutta methods. In this section we will give third and fourth order Runge-Kutta methods and discuss how Runge-Kutta methods are developed.\n\nEuler's method and the improved Euler's method both try to approximate $y(x_0+h)$ by approximating the slope $m$ of the secant line from $(x_0,y(x_0))$ to $(x_0+h,y(x_0+h))$ and using the formula $y(x_0+h)=y(x_0)+mh$ (see figure 1).\n\nEuler's method approximates the slope of the secant line by the slope of the tangent line at the left endpoint $(x_0,y(x_0))$. The improved Euler's method uses the average of the slopes at the left endpoint and the approximate right endpoint (that is the right endpoint as computed by Euler's method) to approximate the slope of the secant line. We don't have to stop there either. We can keep finding slopes at different points and computing weighted averages to approximate the slope of the tangent line. Numerical methods to approximate the solution of differential equations in this fashion are called Runge-Kutta methods (after the mathematicians Runge and Kutta).\nThird Order Runge-Kutta Method\n\nTo approximate the solution of $$ \\frac{dy}{dx}=f(x,y),\\qquad y(x_0)=y_0 $$ compute $$ \\begin{align} x_1&=x_0+h \\\\ k_1&=f(x_0,y_0) \\\\ k_2&=f(x_0+h,y_0+hk_1) \\\\ k_3&=f(x_0+h/2,y_0+(h/2)(k_1+k_2)/2) \\\\ y_1&=y_0+\\frac{k_1+k_2+4k_3}{6}h \\end{align} $$ Then $y(x_1)\\approx y_1$.\n\nThis method uses the improved Euler method to find an approximate midpoint of the secant line and then takes a weighted average of the slopes at the left and right endpoints and the midpoint. Note that if $f(x,y)$ is just $f(x)$, a function of $x$ alone, then solving the differential equation $\\displaystyle\\frac{dy}{dx}=f(x)$ is just evaluating the integral $\\displaystyle\\int_{x_0}^{x_1} f(x)\\quad dx$. In this case, the third order Runge-Kutta method is the same as Simpson's rule for numerical approximation of integrals from Calculus 2. Just as for Euler's method and the improved Euler's method, the RK3 method can be easily put in a spreadsheet. The first part of a spreadsheet for the RK3 method applied to $\\displaystyle\\frac{dy}{dx}=2xy$, $y(0)=1$ is shown below. Note that this is the problem for exercise 1, so you can use this spreadsheet to check your work.\n\nFourth Order Runge-Kutta method\nTo approximate the solution of $$ \\frac{dy}{dx}=f(x,y),\\qquad y(x_0)=y_0 $$ compute $$ \\begin{align} x_1&=x_0+h \\\\ k_1&=f(x_0,y_0) \\\\ k_2&=f(x_0+h/2,y_0+(h/2)k_1) \\\\ k_3&=f(x_0+h/2,y_0+(h/2)k_2) \\\\ k_4&=f(x_0+h,y_0+hk_3) \\\\ y_1&=y_0+\\frac{k_1+2k_2+2k_3+k_4}{6}h \\end{align} $$ Then $y(x_1)\\approx y_1$.\n\nJust as with Euler's method and the improved Euler's method, we can repeat the process to find an $x_2=x_1+h=x_0+2h$ and $y_2\\approx y(x_2)=y(x_0+2h)$ and so on. The fourth order Runge-Kutta method listed above is powerful enough that it was a popular technique for practical computation (and not just for educational purposes). The steps are well suited to programming into a computer, just loop through the basic algorithm as many times as needed to get to the values you are interested in. The first part of a spreadsheet for the RK4 method applied to $\\displaystyle\\frac{dy}{dx}=2xy$, $y(0)=1$ is shown below. Note that this is the problem for exercise 2, so you can use this spreadsheet to check your work.\n\nExercises\nUse the third order Runge-Kutta method with step sizes $h=0.1$ and $h=0.05$ to approximate $y(2)$ for the initial value problem $\\displaystyle\\frac{dy}{dx}=2xy$, $y(0)=1$. Find the error in both approximations.\n\nUse the fourth order Runge-Kutta method with step sizes $h=0.1$ and $h=0.05$ to approximate $y(2)$ for the initial value problem $\\displaystyle\\frac{dy}{dx}=2xy$, $y(0)=1$. Find the error in both approximations.\n\nShow that approximating $y(b)$ using one step of the third order Runge-Kutta method for the initial value problem $\\displaystyle\\frac{dy}{dx}=f(x)$, $y(a)=y_0$ gives the same formula as Simpson's rule for $\\displaystyle\\int_a^b f(x)\\quad dx.$ Simpson's rule is covered in your calculus textbook (or, if you don't have your book any more, either Wikipedia or Wolfram MathWorld have reasonable explanations).\n\n\u00a0\n\nRunge-Kutta-Fehlberg Methods\nDiscussion\nThe trickiest part of using Runge-Kutta methods to approximate the solution of a differential equation is choosing the right step-size. Too large a step-size and the error is too large and the approximation is inaccurate. Too small a step-size and the process will take too long and possibly have too much roundoff error to be accurate. Furthermore, the appropriate step-size may change during the course of a single problem. Many problems in celestial mechanics, chemical reaction kinematics, and other areas have long periods of time where nothing much is happening (and for which large step-sizes are appropriate) mixed in with periods of intense activity where a small step-size is vital. What we need is an algorithm which includes a method for choosing the appropriate step-size at each step. The Runge-Kutta-Fehlberg methods do just this, which is why they have largely replaced the Runge-Kutta methods in practice.\n\nFehlberg's improvement to the Runge-Kutta method is based on two assumptions.\n\nThe error in a single step of the improved Euler's method is about $C'h^3$ and the error in a single step of the third order Runge-Kutta method is about $C''h^4$ where $C'$ and $C''$ are constants that depend on the problem but not the step size. (The error in a single step is the error in $y(x_0+h)$ computed with a step-size of $h$. It is also called the local error.)\n$C''h^4$ is much smaller than $C'h^3$.\nLet us consider these assumptions in reverse order. Provided $h$ is small, it seems reasonable to hope that $C''h^4<<C'h^3$ (the $<<$ means much less than). The danger is that $C''$ might be much larger than $C'$, but in practice that usually doesn't happen. The first assumption can actually be proved as a theorem for $h$ sufficiently small using Taylor series approximations, but we won't go into all that. In the exercises you are asked to check that this assumption fits with observations made about the accuracy of the different Runge-Kutta methods in the previous section as well as the lab about numerical methods.\n\nTo see how Fehlberg used these assumptions to compute a good step-size, consider the following equations where $y_1'$ is the approximation from the improved Euler's method, $y_1''$ is the approximation from the third order Runge-Kutta method, and $y(h)$ is the true value. $$\\begin{align} y_1'&\\approx y(h)+C'h^3 \\\\ y_1''&\\approx y(h)+C''h^4 \\\\ |y_1'-y_1''|&\\approx|C'h^3-C''h^4|\\approx |C'|h^3 \\end{align} $$ where the last line uses the assumption that $C''h^4$ is very small compared to $C'h^3$. Now we can solve this last line for the constant $C'$ to get $$ |C'|\\approx \\frac{|y_1'-y_1''|}{h^3} $$ Once we have this approximation for $C'$, we can pick a step-size $h_1$ to get the local error of the size we want. If we want the local error to be about size $T$, we just take a step-size $h_{\\text{new}}$ where $$ h_{\\text{new}}=h\\left(\\frac{T}{|y_1'-y_1''|}\\right)^{1/3} $$ and then the error should be about $$ \\begin{align} |\\text{error}|&\\approx |C'h_{\\text{new}}^3| \\\\ &\\approx\\frac{|y_1'-y_1''|}{h^3}h^3\\frac{T}{|y_1'-y_1''|} \\\\ &\\approx T \\end{align} $$ You might be a little worried about how all the errors in the different approximations mount up as we carry out all these computations to get our new step-size $h_{\\text{new}}$. This is a serious consideration and is dealt with by introducing a chicken factor, usually taken to be 0.9. We actually use a step-size $$ h_1=0.9h\\left(\\frac{T}{|y_1'-y_1''|}\\right)^{1/3}. $$ This is a bit smaller than the approximation we computed above, and thus a bit safer too.\n\nThe Runge-Kutta-Fehlberg 2(3) method uses exactly this technique to pick the right step-size. Suppose the initial value problem we want to solve is $$ \\frac{dy}{dx}=f(x,y),\\qquad y(x_0)=y_0 $$ We have an initial step-size $h$ (taken to be whatever value you fancy, we will update it automatically as needed). We compute the improved Euler's and RK3 estimates in the usual fashion. $$\\begin{align} k1&=f(x_0,y_0) \\\\ k2&=f(x_0+h,y_0+h*k_1) \\\\ k3&=f(x_0+h/2,y_0+h*(k1+k2)/4) \\\\ y_1'&=y_0+h*(k1+k2)/2 \\\\ y_1''&=y_0+h\\frac{k1+k2+4k3}{6} \\end{align} $$ Next we compute the estimated error $$ |\\text{error}|\\approx|y_1'-y_1''| $$ If this error is small enough, say within a tolerance of $T=.001*\\max(|y_0|,1)$ (which is the tolerance used by the matlab program ode23), then we accept this step-size for the current step and let $$\\begin{align} x_1&=x_0+h\\\\ y_1&=y_1'' \\end{align}$$ If the error is greater than $T$, we reject this step-size for the current step and leave $x_0$ and $y_0$ as they are. In either case, we choose a new step-size $$ h_{\\text{new}}=.9h\\left(\\frac{T}{|y_1'-y_1''|}\\right)^{1/3} $$ We then either compute the next step with the new step-size (if our error was less than $T$) or we repeat the current step with the new step-size $h_{\\text{new}}$ (if the error was greater than $T$) and try again to find $x_1$ and $y_1$.\n\nExample\nApproximate $y(1)$ if $dy/dx=x+y$, $y(0)=0$ using the RK2(3) method with tolerance $T=0.01$. (This tolerance is rather large, but I want to work things out by hand so I don't want to take too many steps.)\n\nWe need to pick an initial step-size to get things started. We have to go from $x_0=0$ to $x=1$ so why not go for it all in one shot and guess $h=1$ initially. Actually, the reason not to use that large a step-size for the beginning is that it is usually too large and often takes a couple of iterations before you get the step-size down to a reasonable size and actually get the process off and running. Choosing a step-size of about the length of the interval divided by 16 or 32 is more typical. But I wanted to be sure I had to reject the estimate sometime in the course of the example so I decided to start off wrong with too large a step size to be sure that happened. We carry out the following computations. $$\\begin{align} k1&=0\\\\ k2&=1\\\\ k3&=.75\\\\ y_1'&=.5\\\\ y_1''&=.666666\\ldots\\\\ |y_1'-y_1''|&=.166666\\ldots>.01 \\end{align}$$ The estimated error is greater than the tolerance $.01$ so we reject the initial step-size of $h=1$. We compute a new step-size and try again. $$ \\begin{align} h_{\\text{new}}&=.9*1*(.01/.166666\\ldots)^{1/3}=.3523380877\\\\ k1&=0\\\\ k2&=.35233880877\\\\ k3&=.2072045759\\\\ y_1'&=.062071064\\\\ y_1''&=.069361064\\\\ |y_1'-y_1''|&=.00729<.01 \\end{align}$$ This time the estimated error is less than the tolerance so we accept the step-size and estimate and compute $$\\begin{align} x_1&=x_0+h=.3523380877\\\\ y_1&=y_1''=.069361064 \\end{align}$$ We now compute a new step-size $h_{\\text{new}}$ and go on to the next step (Twice in this problem, we compute a new step-size and it turns out to exactly equal the old step-size. This is a freak accident. I couldn't have planned that to happen if I tried.) $$ \\begin{align} h_{\\text{new}}&=.9*.3523380877*(.01/.00729\\ldots)^{1/3}=.3523380877\\\\ k1&=.4216991517\\\\ k2&=.9276179121\\\\ k3&=.7162817215\\\\ y_2'&=.3061881158\\\\ y_2''&=.3165523026\\\\ |y_2'-y_2''|&=.0103641868>.01 \\end{align}$$ This time the estimated error is greater than the tolerance so we reject the step-size and try again with the same $x_1$ and $y_1$. $$ \\eqalign{ h_{\\text{new}}&=.9*.3523380877*(.01/.0103641868)^{1/3}=.3133456655\\cr k1&=.4216991515\\cr k2&=.8671824185\\cr k3&=.6793383478\\cr y_2'&=.2712937907\\cr y_2''&=.2785837907\\cr |y_2'-y_2''|&=.00729<.01\\cr }$$ The estimated error is less than the tolerance so we accept the step-size and compute $$\\eqalign{ x_2&=x_1+h=.6656837532\\cr y_2&=y_2''=.2785837907\\cr }$$ We now compute the new step-size for the next step and repeat the process. $$ \\eqalign{ h_{\\text{new}}&=.9*.3133456655*(.01/.00729)^{1/3}=.3133456655\\cr k1&=.9442675439\\cr k2&=1.553495351\\cr k3&=1.296606171\\cr y_3'&=.669915379\\cr y_3''&=.679849358\\cr |y_3'-y_3''|&=.0099695568<.01\\cr }$$ The estimated error is less than the tolerance so we accept the estimate and compute $$\\eqalign{ x_3&=x_2+h=.9790294187\\cr y_3&=y_3''=.679849358\\cr }$$ We now compute the step-size for the next step $$h_{\\text{new}}=.9*.3133456655*(.01/.0099695568)^{1/3} =.2822978588$$ But this step-size is too large since $x_3+h=1.261327277>1$ and so it would put us past our final value for $x$. Therefore we shrink $h$ to hit $x=1$ exactly. $$ \\eqalign{ h_{\\text{new}}&=1-x_3=.0209705813\\cr k1&=1.658914354\\cr k2&=1.714673334\\cr k3&=1.687086169\\cr y_4'&=.7152579833\\cr y_4''&=.7152620701\\cr |y_4'-y_4''|&=.00000408681<.01\\cr }$$ The estimated error is less than the tolerance so we accept this estimate and get the final computations $$\\eqalign{ x_4&=x_3+h=1\\cr y_4&=y_4''=.7152620701\\cr }$$ So our approximation for $y(1)=y(x_4)\\approx y_4=.7152620701$. The true value is $y(1)=.7182818285$ and the error is .0030197584 or a percent error of 0.42%. You may be wondering how we got a relative error of less than 1% when we had a tolerance of 1% error in each individual step. The trick is that we estimate the error using the less accurate improved Euler's method but use the estimates from the more accurate RK3 method. That way we have a safety margin for our estimate of the error. There are many different Runge-Kutta-Fehlberg methods, all of which involve comparing two different Runge-Kutta approximations to get an estimated error and step-size. The RK2(3) method is a low order method which gives moderately accurate results with only 3 function evaluations at each step. If high accuracy is important, you should use a higher order method. On the other hand, if you just need an answer to within 1% for a problem that doesn't cover too large a range of $x$ values, the RK2(3) method is quick, cheap and effective (with the recommended tolerance of $T=.001$).\n\nExercises\nIn the lab you discovered halving the step size reduced the error in the improved Euler method by a factor of about 4. Explain why this fits with the global error being proportional to $h^2$ for the improved Euler method. Note, this is why it is called a second order method and denoted with the 2 in naming the RKF23 method)\n\nIn the ingredients for the Runge-Kutta-Fehlberg method, we assume the local error of the improved Euler's method is about $C'h^3$. How does this fit with the error of the approximation for $y(1)$ being about $k'h^2$ (which is what it means for the error to be proportional to $h^2$)? Hint: How many steps of size $h$ does it take to get from $x=0$ to $x=1$?\n\nExplain why you are guaranteed that if the estimated error is greater than the tolerance, so you reject the estimate, the revised step-size will be smaller than the previous step-size (i.e. $ h_{\\text{new}}<h$). If the estimated error is less than the tolerance, so you accept the estimate, will the revised step-size necessarily increase for the next step?\n\nFor the next two problems you may want to program the RK2(3) method into a calculator or computer. Unfortunately, unlike the first extra credit assignment, this method isn't well suited to a spreadsheet because you keep having to test the step-size and possibly change it (perhaps more than once) at each step, though some students have used a spreadsheet successfully anyway.\n\nApproximate $y(2)$ using the RK2(3) method for $y'=2xy$, $y(0)=1$ and a tolerance of $T=.001$. You can use whatever initial step-size you want. Then compute the exact answer for the initial value problem and find the percent error in the RK2(3) method.\n\nApproximate $y(2)$ using the RK2(3) method for $y'=x+y$, $y(0)=0$ with a tolerance of $T=.001$.\n\nIf you have any problems with this page, please contact bennett@math.ksu.edu.\n\u00a92010, 2014 Andrew G. Bennett\nCanvas\nHome\nKSIS\nMathematics Department\n1228 North 17th Street\n138 Cardwell Hall\nManhattan, KS 66506\n(785) 532-6750\n\nNext: Adams Methods Up: Higher Order Methods Previous: Higher Order Methods\n\n\nRunge-Kutta Methods\nIn the forward Euler method, we used the information on the slope or the derivative of y at the given time step to extrapolate the solution to the next time-step. The LTE for the method is O(h2), resulting in a first order numerical technique. Runge-Kutta methods are a class of methods which judiciously uses the information on the 'slope' at more than one point to extrapolate the solution to the future time step. Let's discuss first the derivation of the second order RK method where the LTE is O(h3).\n\nGiven the IVP of Eq. 6, and a time step h, and the solution yn at the nth time step, let's say that we wish to compute yn+1 in the following fashion:\n\n\n\u00a0\t\u00a0\tk1 = hf(yn,tn)\t\u00a0\n\u00a0\t\u00a0\t\t\u00a0\n\u00a0\t\u00a0\tyn+1 = yn + ak1 + bk2,\t(12)\n\n\n\nwhere the constants , , a and b have to be evaluated so that the resulting method has a LTE O(h3). Note that if k2=0 and a=1, then Eq. 13 reduces to the forward Euler method.\n\nNow, let's write down the Taylor series expansion of y in the neighborhood of tn correct to the h2 term i.e.,\n\n\n\t(13)\n\n\n\nHowever, we know from the IVP (Eq. 6) that dy/dt = f(y,t) so that\n\n\n\t(14)\n\n\n\nSo from the above analysis, i.e., Eqs. 14 and 15, we get\n\n\n\t(15)\n\n\n\nHowever, the term k2 in the proposed RK method of Eq. 13 can be expanded correct to O(h3) as\n\n\u00a0\t\u00a0\t\t\u00a0\n\u00a0\t\u00a0\t\t(16)\n\n\n\nNow, substituting for k2 from Eq. 17 in Eq. 13, we get\n\n\n\t(17)\n\n\n\nComparing the terms with identical coefficients in Eqs. 16 and 18 gives us the following system of equations to determine the constants:\n\n\n\u00a0\t\u00a0\ta+b=1\t\u00a0\n\u00a0\t\u00a0\t\t\u00a0\n\u00a0\t\u00a0\t\t(18)\n\n\n\nThere are infinitely many choices of a, b,  and  which satisfy Eq. 19, we can choose for instance  and a=b=1/2. With this choice, we have the classical second order accurate Runge-Kutta method (RK2) which is summarized as follows.\n\n\u00a0\t\u00a0\tk1 = hf(yn,tn)\t\u00a0\n\u00a0\t\u00a0\tk2 = hf(yn+k1, tn + h)\t\u00a0\n\u00a0\t\u00a0\t\t(19)\n\n\n\nIn a similar fashion Runge-Kutta methods of higher order can be developed. One of the most widely used methods for the solution of IVPs is the fourth order Runge-Kutta (RK4) technique. The LTE of this method is order h5. The method is given below.\n\n\n\u00a0\t\u00a0\tk1 = hf(yn,tn)\t\u00a0\n\u00a0\t\u00a0\tk2 = hf(yn+k1/2, tn + h/2)\t\u00a0\n\u00a0\t\u00a0\t\t\u00a0\n\u00a0\t\u00a0\tk4 = h(yn+k3, tn + h)\t\u00a0\n\u00a0\t\u00a0\tyn+1 = yn + (k1 + 2k2 + 2k3 + k4)/6.\t(20)\n\n\n\nNote that the RK methods are explicit techniques, hence they are only conditionally stable.\n\n  \nNext: Adams Methods Up: Higher Order Methods Previous: Higher Order Methods\nMichael Zeltkevic\n1998-04-15\n\nEuler's Method (First Order Runge-Kutta)\nIntro First Order Second Fourth Printable\nContents\nProblem Statement\nEuler's Method (Intuitive)\nA First Order Linear Differential Equation with No Input\nExample 1: Approximation of First Order Differential Equation with No Input Using MATLAB\nKey Concept: First Order Runge-Kutta Algorithm\nA First Order Linear Differential Equation with Input\nExample 2: Approximation of First Order Differential Equation with Input Using MATLAB\nA First Order Non-Linear Differential Equation\nExample 3: Approximation of First Order Nonlinear Differential Equation with Input Using MATLAB\nA Higher Order Linear Differential Equation\nExample 4: Approximation of Third Order Differential Equation Using MATLAB\nEuler's Method (The Math)\nKey Concept: Error of First Order Runge Kutta\nMoving on\nProblem Statement\n\nConsider the situation in which the solution, y(t), to a differential equation\n\n$${{dy(t)} \\over {dt}} = y'\\left( t \\right) = f(y(t),t)$$\n\nis to be approximated by computer starting from some known initial condition, y(t0)=y0 (note that the tick mark denotes differentiation). The following text develops an intuitive technique for doing so, and then presents several examples. This technique is known as \"Euler's Method\" or \"First Order Runge-Kutta\".\n\nEuler's Method (Intuitive)\nA First Order Linear Differential Equation with No Input\n\nConsider the following case: we wish to use a computer to approximate the solution of the differential equation\n\n$${{dy(t)} \\over {dt}} + 2y(t) = 0$$\n\nor\n\n$${{dy(t)} \\over {dt}} = - 2y(t)$$\n\nwith the initial condition set as y(0)=3. For this case the exact solution can be determined to be (y(t)=3e-2t, t\u22650) and is shown below. Since we know the exact solution in this case we will be able to use it to check the accuracy of our approximate solution.\n\nThere are several ways to develop an approximate solution, we will do so using the Taylor Series for y(t) expanded about t=0 (in general we expand around t=t0).\n\n$$y(t) = y(0) + y'(0)t + y''(0){{{t^2}} \\over 2} + \\cdots $$\n\nWe now restrict our solution to a short time step, h, after t=0 and truncate the Taylor series after the first derivative\n\n$$\\eqalign{\n& y(h) = y(0) + y'(0)h + y''(0){{{h^2}} \\over 2} + \\cdots \\cr\n& y(h) \\approx y(0) + y'(0)h \\cr} $$\n\nWe call the value of the approximation y*(h), and we call the derivative y'(0)=k_1.\n\n$$y^*(h) = y(0) + {k_1}h$$\n\nThis is shown on the graph below for h=0.2\n\nThere are several important details to observe:\n\nThe graph on the right is simply a detail of the shaded area of the graph on the left.\nThe first order 4aylor Approximation is just a straight line starting at the initial value with a slope of -6 (i.e., k1=y'(0)=-2y(0)=-6).\nThe approximation at t=h=0.2 is just the initial value plus the slope multiplied by the time step, h; y*(h)=y*(0.2)=y(0)+k1y'(0)=1.8 or about 10% error.\nObviously a smaller h would result in a better approximation, and a larger h in a worse approximation.\n\nTo find the value of the approximation after the next time step, y*(2h), we simply repeat the process using our approximation, y*(h) to estimate the derivative at time h (we don't know y(h) exactly, so we can only estimate the derivative - we call this estimate k_1).\n\n$$\\eqalign{\ny'(t) &= - 2y(t)\\quad \\quad &{\\rm{exact}}\\;{\\rm{expression}}\\;{\\rm{for}}\\;{\\rm{derivative}} \\cr\n{k_1} &= - 2y^*(h)\\quad \\quad &{\\rm{approximation}}\\;{\\rm{for}}\\;{\\rm{derivative}} \\cr\ny(2h) &= y(h) + y'(h)h + y''(h){{{h^2}} \\over 2} + \\cdots \\quad \\quad &{\\rm{4aylor}}\\;{\\rm{Series}}\\;{\\rm{around}}\\;{\\rm{t = h}} \\cr\ny(2h) &\\approx y(h) + y'(0)h\\quad \\quad &{\\rm{Truncated}}\\;{\\rm{4aylor}}\\;{\\rm{Series}} \\cr\ny^*(2h) &= y^*(h) + {k_1}h\\quad \\quad &{\\rm{Approximate Solution}} \\cr} $$\n\nIn general we move forward one step in time from t0 to t0+h\n\n$$\\eqalign{\ny'({t_0}) &= - 2y({t_0})\\quad \\quad &{\\rm{exact}}\\;{\\rm{expression}}\\;{\\rm{for}}\\;{\\rm{derivative\\;at\\;t=t_0}} \\cr\n{k_1} &= - 2y^*({t_0})\\quad \\quad &{\\rm{Previous\\;approx\\;for\\;y(t)\\;gives\\;approx\\;for\\;derivative}} \\cr\ny({t_0} + h) &= y({t_0}) + y'({t_0})h + y''({t_0}){{{h^2}} \\over 2} + \\cdots \\quad \\quad &{\\rm{4aylor}}\\;{\\rm{Series}}\\;{\\rm{around}}\\;{\\rm{t = }}{t_0} \\cr\ny({t_0} + h) &\\approx y({t_0}) + y'({t_0})h\\quad \\quad &{\\rm{Truncated}}\\;{\\rm{4aylor}}\\;{\\rm{Series}} \\cr\ny^*({t_0} + h) &= y^*({t_0}) + {k_1}h\\quad \\quad &\\rm{Approximate\\;Solution\\;at\\;next\\;value\\;of\\;y} \\cr} $$\nExample 1: Approximation of First Order Differential Equation with No Input Using MATLAB\n\nWe can use MATLAB to perform the calculation described above. A simple loop accomplishes this:\n\n%% Example 1\n% Solve y'(t)=-2y(t) with y0=3\ny0 = 3;                  % Initial Condition\nh = 0.2;% Time step\nt = 0:h:2;               % t goes from 0 to 2 seconds.\nyexact = 3*exp(-2*t)     % Exact solution (in general we won't know this)\nystar = zeros(size(t));  % Preallocate array (good coding practice)\n\nystar(1) = y0;           % Initial condition gives solution at t=0.\nfor i=1:(length(t)-1)\n    k1 = -2*ystar(i);  % Previous approx for y gives approx for derivative\n    ystar(i+1) = ystar(i) + k1*h; % Approximate solution for next value of y\nend\nplot(t,yexact,t,ystar);\nlegend('Exact','Approximate');\n\nThe MATLAB commands match up easily with the code. A slight variation of the code was used to show the effect of the size of hon the accuracy of the solution (see image below). Note that larger values of h result in poorer approximations (including bad oscillations with h=0.8).\n\nKey Concept: First Order Runge-Kutta Algorithm\n\nFor a first order ordinary differential equation defined by\n\n$${{dy(t)} \\over {dt}} = f(y(t),t)$$\n\nto progress from a point at t=t0, y*(t0), by one time step, h, follow these steps (repetitively).\n\n$$\\eqalign{\n{k_1} &= f({y^*}{\\rm{(}}{{\\rm{t}}_0}),{t_0})\\quad &{\\rm{approximation}}\\;{\\rm{for}}\\;{\\rm{derivative}} \\cr\n{y^*}({t_0} + h) &= {y^*}({t_0}) + {k_1}h\\quad &{\\rm{approximate\\;solution}}\\;{\\rm{at}}\\;{\\rm{next}}\\;{\\rm{time}}\\;{\\rm{step}} \\cr} $$\n\nNotes:\n\nan initial value of the function must be given to start the algorithm.\nsee the MATLAB programs on this page for examples that implement this algorithm.\nA First Order Linear Differential Equation with Input\n\nAdding an input creates no particular difficulty to our approximate solution (but makes the exact solution significantly harder to find). Consider an input of cos(4t).\n\n$$\\eqalign{ y'(t) + 2y(t) &= \\cos (4t) \\quad {\\rm{or}} \\quad y'(t) &= - 2y(t) + \\cos (4t) \\cr} $$\n\nso\n\n$$\\eqalign{\ny'({t_0}) &= - 2y({t_0}) + \\cos (4{t_0})\\quad \\quad &{\\rm{exact}}\\;{\\rm{expression}}\\;{\\rm{for}}\\;{\\rm{derivative\\;at\\;t=t_0}} \\cr\n{k_1} &= - 2y^*({t_0}) + \\cos (4{t_0})\\quad \\quad &{\\rm{approximation}}\\;{\\rm{for}}\\;{\\rm{derivative}} \\cr\ny({t_0} + h) &= y({t_0}) + y'({t_0})h + y''({t_0}){{{h^2}} \\over 2} + \\cdots \\quad \\quad &{\\rm{4aylor}}\\;{\\rm{Series}}\\;{\\rm{around}}\\;{\\rm{t = }}{t_0} \\cr\ny({t_0} + h) &\\approx y({t_0}) + y'({t_0})h\\quad \\quad &{\\rm{Truncated}}\\;{\\rm{4aylor}}\\;{\\rm{Series}} \\cr\ny^*({t_0} + h) &= y^*({t_0}) + {k_1}h\\quad \\quad &{\\rm{Approximate\\;Solution}} \\cr} $$\n\nNote: the exact solution in this case is $y(t) = 2.9{e^{ - 2{\\kern 1pt} t}} + 0.1\\cos (4t) + 0.2\\sin (4t)$.\n\nExample 2: Approximation of First Order Differential Equation with Input Using MATLAB\n\nWe can use MATLAB to perform the calculation described above. To perform this new approximation all that is necessary is to change the calculation of k1 (the value of the exact solution is also changed, for plotting).\n\n%% Example 2\n% Solve y'(t)=-2y(t)+cos(4t) with y0=3\ny0 = 3;                  % Initial Condition\nh = 0.2;% Time step\nt = 0:h:2;               % t goes from 0 to 2 seconds.\n% Exact solution, hard to find in this case (in general we won't have it)\nyexact = 0.1*cos(4*t)+0.2*sin(4*t)+2.9*exp(-2*t);\nystar = zeros(size(t));  % Preallocate array (good coding practice)\n\nystar(1) = y0;           % Initial condition gives solution at t=0.\nfor i=1:(length(t)-1)\n    k1 = -2*ystar(i)+cos(4*t(i));  % Previous approx for y gives approx for deriv\n    ystar(i+1) = ystar(i) + k1*h; % Approximate solution for next value of y\nend\nplot(t,yexact,t,ystar);\nlegend('Exact','Approximate');\n\nA modified version of the program (that uses several values of h) generated the plot below. As before, the solution is better with smaller values of h.\n\nA First Order Non-Linear Differential Equation\n\nNon-linear differential equations can be very difficulty to solve analytically, but pose no particular problems for our approximate method. Consider the differential equation given by\n\n$${{dy(t)} \\over {dt}} - y(t)(1 - 2t) = 0,\\quad \\quad \\quad \\quad y(0) = 1$$\n\nthe solution is (described here)\n\n$$y(t) = {e^{t - {t^2}}}$$\n\nTo obtain the approximate solution, we write an expression for y'(t) and use it to find k1.\n\n$$\\eqalign{\ny'({t_0}) &= y({t_0})(1 - 2{t_0})\\quad \\quad &{\\rm{exact}}\\;{\\rm{expression}}\\;{\\rm{for}}\\;{\\rm{derivative\\; at\\; t=t_0}} \\cr\n{k_1} &= y^*({t_0})(1 - 2{t_0})\\quad \\quad &{\\rm{approximation}}\\;{\\rm{for}}\\;{\\rm{derivative}} \\cr\ny^*({t_0} + h) &= y^*({t_0}) + {k_1}h\\quad \\quad &{\\rm{Approximate\\;Solution}} \\cr} $$\nExample 3: Approximation of First Order Nonlinear Differential Equation with Input Using MATLAB\n\nAs before, to perform this new approximation all that is necessary is to change the calculation of k1 and the initial condition (the value of the exact solution is also changed, for plotting).\n\n%% Example 3\n% Solve y'(t)=y(t)(1-2t) with y0=1\ny0 = 1;                  % Initial Condition\nh=0.2;                   % Time step\nt = 0:h:2;               % t goes from 0 to 2 seconds.\n% Exact solution, hard to find in this case (in general we won't have it)\nyexact = exp(t-t.^2);\nystar = zeros(size(t));  % Preallocate array (good coding practice)\n\nystar(1) = y0;           % Initial condition gives solution at t=0.\nfor i=1:(length(t)-1)\n  k1 = ystar(i)*(1-2*t(i));   % Approx for y gives approx for deriv\n  ystar(i+1) = ystar(i) + k1*h; % Approximate solution at next value of y\nend\nplot(t,yexact,t,ystar);\nlegend('Exact','Approximate');\n\nA modified version of the program (that uses several values of h) generated the plot below. As before, the solution is better with smaller values of h.\n\nA Higher Order Linear Differential Equation\n\nThough the techniques introduced here are only applicable to first order differential equations, the technique can be use on higher order differential equations if we reframe the problem as a first order matrix differential equation. Consider the 3rd order equation (with initial conditions)\n\n$$\\displaylines{\n{{{d^3}y(t)} \\over {dt}} + 4{{{d^2}y(t)} \\over {dt}} + 6{{dy(t)} \\over {dt}} + 4y(t) = \\gamma (t)\\quad \\quad \\quad \\gamma (t) = {\\rm{unit\\ step\\ function}} \\cr\n{\\left. {{{{d^2}y(t)} \\over {dt}}} \\right|_{t = {0^ + }}} = 0,\\quad \\quad {\\left. {{{dy(t)} \\over {dt}}} \\right|_{t = {0^ + }}} = - 1,\\quad \\quad y({0^ + }) = 0\\quad \\cr} $$\n\nIf we introduce new variables, q1, q2, and q3, we can write three first order differential equations\n\n$$\\eqalign{ {q_1}(t) &= y(t) \\cr {q_2}(t) &= y'(t) \\cr {q_3}(t) &= y''(t) \\cr \\cr\n{q_1}'(t) &= y'(t) &= {q_2}(t) \\cr {q_2}'(t) &= y''(t) &= {q_3}(t) \\cr {q_3}'(t) &= y'''(t) &= \\gamma (t) - 4y''(t) - 6y'(t) - 4y(t) \\cr &&= \\gamma (t) - 4{q_3}(t) - 6{q_2}(t) - 4{q_1}(t) \\cr} $$\n\nWe can further simplify by writing this as a single first order matrix differential equation.\n\n$$\\eqalign{ {\\bf{q}}'(t) &= \\left[ {\\matrix{ {{q_1}'(t)} \\cr {{q_2}'(t)} \\cr {{q_3}'(t)} \\cr} } \\right] = \\left[ {\\matrix{ 0 & 1 & 0 \\cr 0 & 0 & 1 \\cr\n{ - 4} & { - 6} & { - 4} \\cr} } \\right]\\left[ {\\matrix{ {{q_1}(t)} \\cr {{q_2}(t)} \\cr {{q_3}(t)} \\cr} } \\right] + \\left[ {\\matrix{ 0 \\cr 0 \\cr 1 \\cr} } \\right]\\gamma (t) \\cr\n{\\bf{q}}'(t) &= {\\bf{Aq}}(t) + {\\bf{B\\gamma }}(t) \\cr {\\bf{A}} &= \\left[ {\\matrix{ 0 & 1 & 0 \\cr 0 & 0 & 1 \\cr\n{ - 4} & { - 6} & { - 4} \\cr} } \\right],\\quad \\quad \\quad {\\bf{B}} = \\left[ {\\matrix{ 0 \\cr 0 \\cr 1 \\cr } } \\right] \\cr} $$\n\nA more in depth discussion of the procedure of going from nth order differential equation to first order nxn matrix differential equation is here.\n\nWe now proceed as before except the variable being integrated and the quantity k1 are both vectors.\n\n$$\\eqalign{\n{\\bf{q}}'({t_0}) &= {\\bf{Aq}}({t_0}) + {\\bf{B\\gamma }}({t_0})\\quad \\quad &{\\rm{exact}}\\;{\\rm{expression}}\\;{\\rm{for}}\\;{\\rm{derivative\\;at\\;t=t_0}} \\cr\n{{\\bf{k}}_1} &= {\\bf{A}}{{\\bf{q}}^*}({t_0}) + {\\bf{B\\gamma }}({t_0})\\quad \\quad &{\\rm{approximation}}\\;{\\rm{for}}\\;{\\rm{derivative}} \\cr\n{\\bf{q}}({t_0} + h) &= {\\bf{q}}({t_0}) + {\\bf{q}}'({t_0})h + {\\bf{q}}''({t_0}){{{h^2}} \\over 2} + \\cdots \\quad &{\\rm{Taylor}}\\;{\\rm{Series}}\\;{\\rm{around}}\\;{\\rm{t = }}{t_0} \\cr\n{\\bf{q}}({t_0} + h) &\\approx {\\bf{q}}({t_0}) + {\\bf{q}}'({t_0})h\\quad \\quad &{\\rm{Truncated}}\\;{\\rm{Taylor}}\\;{\\rm{Series}} \\cr\n{\\bf{q}}^*({t_0} + h) &= {\\bf{q}}^*({t_0}) + {{\\bf{k}}_1}h\\quad \\quad &{\\rm{Approximate\\;Solution}} \\cr} $$\n\nNote: the exact solution to this problem is $y(t) = {1 \\over 4}+{{\\rm{e}}^{ - t}}{\\mkern 1mu} \\left( {\\cos (t) - {5 \\over 2}\\sin (t)} \\right) - {5 \\over 4}{{\\rm{e}}^{ - 2{\\kern 1pt} t}}$, for t\u22650.\n\nExample 4: Approximation of Third Order Differential Equation Using MATLAB\n\nTo perform this new approximation all that is necessary is to change the appropriate variables from scalars to vectors or matrices, and to define the A and B matrices. Note that in this case we multiply the B matrix by 1 since the input is a unit step (\u03b3(t)=1 for t\u22650). If the input were a sine wave, the sine would multiply B; if there is no input, it is not necessary to include the B matrix at all (it is multiplied by 0).\n\n%% Example 4\n% Solve y'''(t)+4y''(t)+6y'(t)+4y(t)=gamma(t),  y''(0)=0, y'(0)=-1, y(0)=0\nq0 = [0; -1; 0];         % Initial Condition (vector)\nh = 0.2;% Time step\nt = 0:h:5;               % t goes from 0 to 2 seconds.\nA = [0 1 0; 0 0 1; -4 -6 -4];  % A Matrix\nB = [0; 0; 1];                 % B Matrix\n\nyexact = 1/4 + exp(-t).*(cos(t) - 5*sin(t)/2) - 5*exp(-2*t)/4;  % Exact soln\nqstar = zeros(3,length(t));  % Preallocate array (good coding practice)\n\nqstar(:,1) = q0;             % Initial condition gives solution at t=0.\nfor i=1:(length(t)-1)\n  k1 = A*qstar(:,i)+B*1;            % Approx for y gives approx for deriv\n  qstar(:,i+1) = qstar(:,i) + k1*h; % Approximate solution at next value of q\nend\nplot(t,yexact,t,qstar(1,:));        % ystar = first row of qstar\nlegend('Exact','Approximate');\nhold off\n\nA modified version of the program (that uses several values of h) generated the plot below. As expected, the solution is better with smaller values of h.\n\nEuler's Method (The Math)\n\nThe math for this method, the first order Runge-Kutta (or Euler's Method) is fairly simple to understand, and has been discussed before. If we write the differential equation as\n\n$${{dy(t)} \\over {dt}} = y'\\left( t \\right) = f(y(t),t)$$\n\nand write the approximation to the derivative as\n\n$$k_1 = y'\\left( t \\right) = f(y^*(t),t)$$\n\nWe expand y(t) around t0 assuming a time step h\n\n$$y({t_0} + h) = y({t_0}) + y'({t_0})h + y''({t_0}){{{h^2}} \\over 2} + \\cdots $$\n\nand drop all terms after the linear term. Because all of the dropped terms are multiplied by h2 or greater, we say that the algorithm is accurate to order h2 locally, or O(h2) (if h is small the other terms that are multiplied by h3, h4... which will be even smaller, and can be dropped as well).\n\n$$y({t_0} + h) \\approx y({t_0}) + y'({t_0})h$$\n\nThis gives us our approximate solution at the next time step.\n\n$$y^*({t_0} + h) = y^*({t_0}) + {k_1}h$$\n\nSinc the number of steps over the whole interval is proportional to 1/h (or O(h-1)) we might expect the overall accuracy to be the O(h2)\u00b7O(h-1)=O(h). A rigourous analysis proves that this is true. Such an analysis can be found in references about numerical methods such as the book Applied Numerical Methods, by Carnahan, Luther and Wilkes. Because the error is O(h), where h is raised to the power 1, this is called a first order algorithm.\n\nKey Concept: Error of First Order Runge Kutta\n\n\n\nThe global error of the First Order Runge-Kutta (i.e., Euler's) algorithm is O(h).\n\nMoving on\n\nWith a little more work we can develop an algorithm that is accurate to higher order than O(h). The next page describes just such a method.\n\nReferences\n\n\u00a9 Copyright 2005 to 2022 Erik Cheever\u00a0\u00a0\u00a0 This page may be freely used for educational purposes, but the url must be referenced.\n\nComments? \u00a0\u00a0\u00a0\u00a0\u00a0 Questions? \u00a0\u00a0\u00a0\u00a0\u00a0 Suggestions? \u00a0\u00a0\u00a0\u00a0\u00a0 Corrections?\nErik Cheever \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Department of Engineering \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Swarthmore College"
  },
  "content": {
    "Introduction to Ordinary Differential Equations": "Introduction to Ordinary Differential Equations is a comprehensive topic that covers various aspects of differential equations, including first-order and higher-order linear equations, numerical methods, and power series methods. The topic is introduced with a definition of differential equations and their classification. First-order equations are discussed, including linear, separable, exact, and Bernoulli differential equations, as well as intervals of validity, equilibrium solutions, and Euler's Method. Higher-order linear ODEs, including second-order linear ODEs and constant coefficient second-order linear ODEs, are also covered. Additionally, the topic includes discussions on mechanical vibrations, nonhomogeneous equations, forced oscillations, and resonance. The Laplace transform and power series methods are also introduced, along with boundary value problems and Fourier series. Overall, the topic provides a thorough understanding of ordinary differential equations and their applications.",
    "Separation of Variables and First-Order Differential Equations": "Separation of Variables and First-Order Differential Equations is a fundamental concept in mathematics that involves solving differential equations by separating the variables. The general form of a separable differential equation is N(y)dy/dx = M(x), where N(y) is a function of y and M(x) is a function of x. To solve this type of equation, we integrate both sides with respect to x, which gives us \u222bN(y)dy = \u222bM(x)dx. This method is used to solve various types of first-order differential equations, including linear, separable, exact, and Bernoulli equations.\n\nThe solution process for separable differential equations involves rearranging the equation so that everything involving y appears on one side, and everything involving x appears on the other. The equation can then be integrated directly, yielding a solution for y(x). The separation of variables method is a powerful tool for solving differential equations, and it has numerous applications in physics, engineering, and other fields.\n\nIn addition to separable differential equations, the topic of First-Order Differential Equations covers other important concepts, such as intervals of validity, equilibrium solutions, and Euler's Method. Intervals of validity refer to the range of values for which a solution to a differential equation is valid, while equilibrium solutions are solutions that do not change over time. Euler's Method is a numerical method for solving differential equations, which involves approximating the solution at a given point using the derivative of the function at that point.\n\nOverall, Separation of Variables and First-Order Differential Equations is a crucial concept in mathematics that has numerous applications in various fields. The separation of variables method is a powerful tool for solving differential equations, and it is an essential technique for anyone working in mathematics, physics, engineering, or other related fields.",
    "Higher-Order Linear Differential Equations": "Higher-order linear differential equations are extensions of the concepts and methods used to solve 2nd order linear differential equations. These equations appear in various applications, although 2nd order equations are more common. The basic results and concepts, such as the Principle of Superposition, linearly independent functions, and the Wronskian, are similar to those for 2nd order equations, but with some natural extensions. \n\nTo solve higher-order linear differential equations, one can use methods such as undetermined coefficients, which is similar to the method used for 2nd order equations, with only one small natural extension. The process of solving higher-order linear homogeneous differential equations is also similar to solving 2nd order equations, with some extensions to repeated real roots that occur more than twice and repeated complex roots. \n\nThe main difficulty in solving higher-order linear differential equations is finding all the roots of the characteristic polynomial. Overall, higher-order linear differential equations can be solved using similar methods and concepts as 2nd order equations, with some extensions and additional considerations.",
    "Systems of Differential Equations and Phase Plane Analysis": "Systems of Differential Equations and Phase Plane Analysis is a topic in applied mathematics that deals with the analysis of systems of differential equations and their behavior in the phase plane. The phase plane is a graphical representation of the solutions of a system of differential equations, where the x and y axes represent the state variables of the system. \n\nThe phase plane method is used to graphically determine the existence of limit cycles in the solutions of the differential equation. Phase portraits are used to visualize the behavior of the system, including the stability of equilibrium solutions. Equilibrium solutions are solutions for which the derivative of the state variables is zero, and they can be classified as asymptotically stable, unstable, or semi-stable.\n\nThe stability of equilibrium solutions can be determined by analyzing the eigenvalues of the system matrix. For a 2x2 linear system with distinct real eigenvalues, the equilibrium point can be a sink, source, or saddle point. The phase plane can also be used to visualize the behavior of physical systems, including oscillatory systems such as predator-prey models and chemical reactions.\n\nIn summary, Systems of Differential Equations and Phase Plane Analysis is a powerful tool for analyzing and visualizing the behavior of complex systems, and it has numerous applications in physics, biology, chemistry, and other fields. \n\nKey concepts include:\n\n* Phase plane: a graphical representation of the solutions of a system of differential equations\n* Phase portraits: a graphical representation of the behavior of a system in the phase plane\n* Equilibrium solutions: solutions for which the derivative of the state variables is zero\n* Stability of equilibrium solutions: can be classified as asymptotically stable, unstable, or semi-stable\n* Eigenvalues: used to determine the stability of equilibrium solutions\n\nApplications include:\n\n* Physics: modeling of oscillatory systems, such as pendulums and springs\n* Biology: modeling of population dynamics, such as predator-prey models\n* Chemistry: modeling of chemical reactions, such as the Lotka-Volterra equations\n\nOverall, Systems of Differential Equations and Phase Plane Analysis is a fundamental topic in applied mathematics, and it has numerous applications in a wide range of fields.",
    "Numerical Methods for Solving Differential Equations, including Euler's Method and Runge-Kutta Methods": "Numerical methods for solving differential equations, including Euler's method and Runge-Kutta methods, are essential tools in various fields. These methods provide a way to approximate the solution of differential equations, which is crucial in understanding the behavior of complex systems. Euler's method and the improved Euler's method are the simplest examples of Runge-Kutta methods, which approximate the solution of differential equations by approximating the slope of the secant line. The third-order Runge-Kutta method and the Runge-Kutta-Fehlberg methods are popular methods used to approximate the solution of differential equations."
  },
  "slides": [
    [
      {
        "title": "Introduction to Ordinary Differential Equations",
        "content": "Ordinary Differential Equations (ODEs) are equations that involve an unknown function of one independent variable and its derivatives. They are used to model various physical phenomena, such as population growth, chemical reactions, and mechanical vibrations.",
        "code": ""
      },
      {
        "title": "Classification of Differential Equations",
        "content": "Differential equations can be classified into two main categories: ordinary differential equations (ODEs) and partial differential equations (PDEs). ODEs involve functions of one independent variable, while PDEs involve functions of multiple independent variables.",
        "code": ""
      },
      {
        "title": "First-Order Linear Equations",
        "content": "A first-order linear equation is of the form dy/dx + P(x)y = Q(x), where P(x) and Q(x) are functions of x. These equations can be solved using an integrating factor.",
        "code": "import sympy as sp\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x) + 2*x*f(x), 3*x)\nsol = sp.dsolve(eq)\nprint(sol)"
      },
      {
        "title": "Separable, Exact, and Bernoulli Differential Equations",
        "content": "Separable equations can be written in the form dy/dx = f(x)/g(y), while exact equations satisfy the condition \u2202M/\u2202y = \u2202N/\u2202x. Bernoulli equations are of the form dy/dx + P(x)y = Q(x)y^n.",
        "code": "import sympy as sp\nx = sp.symbols('x')\ny = sp.symbols('y')\neq = sp.Eq(y*(x**2 + 1), x*y**2)\nsol = sp.dsolve(eq)\nprint(sol)"
      },
      {
        "title": "Euler's Method",
        "content": "Euler's method is a numerical method used to approximate the solution of a differential equation. It works by iteratively applying the equation y'(x) = f(x,y) to find the next value of y.",
        "code": "import numpy as np\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        x[i+1] = x[i] + h\n        y[i+1] = y[i] + h * f(x[i], y[i])\n    return x, y"
      },
      {
        "title": "Higher-Order Linear ODEs",
        "content": "Higher-order linear ODEs are of the form a_n*y^(n) + a_(n-1)*y^(n-1) + ... + a_1*y' + a_0*y = f(x), where a_i are constants.",
        "code": "import sympy as sp\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x, 2) + 2*f(x).diff(x) + f(x), 0)\nsol = sp.dsolve(eq)\nprint(sol)"
      },
      {
        "title": "Laplace Transform and Power Series Methods",
        "content": "The Laplace transform is a method used to solve differential equations by transforming the equation into a simpler form. The power series method involves representing the solution as a power series and finding the coefficients.",
        "code": "import sympy as sp\nx = sp.symbols('x')\ns = sp.symbols('s')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x), -f(x))\nF = sp.laplace_transform(f(x), x, s)\nprint(F)"
      }
    ],
    [
      {
        "title": "Introduction to Separation of Variables",
        "content": "Separation of Variables is a method for solving differential equations by separating the variables. The general form of a separable differential equation is N(y)dy/dx = M(x), where N(y) is a function of y and M(x) is a function of x.",
        "code": ""
      },
      {
        "title": "General Form of Separable Differential Equations",
        "content": "To solve this type of equation, we integrate both sides with respect to x, which gives us \u222bN(y)dy = \u222bM(x)dx. This method is used to solve various types of first-order differential equations.",
        "code": ""
      },
      {
        "title": "Solution Process for Separable Differential Equations",
        "content": "The solution process involves rearranging the equation so that everything involving y appears on one side, and everything involving x appears on the other. The equation can then be integrated directly, yielding a solution for y(x).",
        "code": ""
      },
      {
        "title": "Example of Separable Differential Equation",
        "content": "Consider the differential equation dy/dx = 2x. We can separate the variables by multiplying both sides by dx and dividing by 2x, giving us dy/2x = dx. Integrating both sides, we get \u222bdy = 2\u222bxdx.",
        "code": "import sympy as sp\nx = sp.symbols('x')\ny = sp.symbols('y')\ndy_dx = 2*x\n# Separate the variables and integrate\ndy = sp.integrate(dy_dx, x)"
      },
      {
        "title": "First-Order Differential Equations",
        "content": "In addition to separable differential equations, the topic of First-Order Differential Equations covers other important concepts, such as intervals of validity, equilibrium solutions, and Euler's Method.",
        "code": ""
      },
      {
        "title": "Intervals of Validity and Equilibrium Solutions",
        "content": "Intervals of validity refer to the range of values for which a solution to a differential equation is valid, while equilibrium solutions are solutions that do not change over time.",
        "code": ""
      },
      {
        "title": "Euler's Method",
        "content": "Euler's Method is a numerical method for solving differential equations, which involves approximating the solution at a given point using the derivative of the function at that point.",
        "code": "import numpy as np\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        y[i+1] = y[i] + h * f(x[i], y[i])\n        x[i+1] = x[i] + h\n    return x, y"
      }
    ],
    [
      {
        "title": "Introduction to Higher-Order Linear Differential Equations",
        "content": "Higher-order linear differential equations are extensions of the concepts and methods used to solve 2nd order linear differential equations. These equations appear in various applications, although 2nd order equations are more common.",
        "code": ""
      },
      {
        "title": "Key Concepts and Methods",
        "content": "The basic results and concepts, such as the Principle of Superposition, linearly independent functions, and the Wronskian, are similar to those for 2nd order equations, but with some natural extensions.",
        "code": ""
      },
      {
        "title": "Method of Undetermined Coefficients",
        "content": "To solve higher-order linear differential equations, one can use methods such as undetermined coefficients, which is similar to the method used for 2nd order equations, with only one small natural extension.",
        "code": ""
      },
      {
        "title": "Solving Higher-Order Linear Homogeneous Differential Equations",
        "content": "The process of solving higher-order linear homogeneous differential equations is also similar to solving 2nd order equations, with some extensions to repeated real roots that occur more than twice and repeated complex roots.",
        "code": ""
      },
      {
        "title": "Example: Solving a 3rd Order Linear Homogeneous Differential Equation",
        "content": "Consider the differential equation y''' - 6y'' + 11y' - 6y = 0. We can use the method of undetermined coefficients to solve this equation.",
        "code": "import sympy as sp\ny = sp.Function('y')\neq = y(0).diff(3) - 6*y(0).diff(2) + 11*y(0).diff(1) - 6*y(0)\nprint(sp.dsolve(e, y(0)))"
      },
      {
        "title": "Finding Roots of the Characteristic Polynomial",
        "content": "The main difficulty in solving higher-order linear differential equations is finding all the roots of the characteristic polynomial.",
        "code": "import numpy as np\n# Define the coefficients of the characteristic polynomial\ncoeffs = [1, -6, 11, -6]\n# Find the roots of the polynomial\nroots = np.roots(coeffs)\nprint(roots)"
      }
    ],
    [
      {
        "title": "Introduction to Systems of Differential Equations and Phase Plane Analysis",
        "content": "Systems of Differential Equations and Phase Plane Analysis is a topic in applied mathematics that deals with the analysis of systems of differential equations and their behavior in the phase plane.",
        "code": ""
      },
      {
        "title": "Phase Plane",
        "content": "The phase plane is a graphical representation of the solutions of a system of differential equations, where the x and y axes represent the state variables of the system.",
        "code": ""
      },
      {
        "title": "Phase Portraits",
        "content": "Phase portraits are used to visualize the behavior of the system, including the stability of equilibrium solutions.",
        "code": ""
      },
      {
        "title": "Equilibrium Solutions",
        "content": "Equilibrium solutions are solutions for which the derivative of the state variables is zero, and they can be classified as asymptotically stable, unstable, or semi-stable.",
        "code": ""
      },
      {
        "title": "Stability of Equilibrium Solutions",
        "content": "The stability of equilibrium solutions can be determined by analyzing the eigenvalues of the system matrix.",
        "code": "import numpy as np\nA = np.array([[1, 2], [3, 4]])\neigenvalues, eigenvectors = np.linalg.eig(A)"
      },
      {
        "title": "Classification of Equilibrium Points",
        "content": "For a 2x2 linear system with distinct real eigenvalues, the equilibrium point can be a sink, source, or saddle point.",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef system(x, y):\n    return x + y, x - y\nx = np.linspace(-10, 10, 400)\ny = np.linspace(-10, 10, 400)\nX, Y = np.meshgrid(x, y)\nDX, DY = system(X, Y)\nplt.quiver(X, Y, DX, DY)\nplt.show()"
      },
      {
        "title": "Applications of Phase Plane Analysis",
        "content": "The phase plane can also be used to visualize the behavior of physical systems, including oscillatory systems such as predator-prey models and chemical reactions.",
        "code": "import numpy as np\nfrom scipy.integrate import odeint\n\ndef predator_prey(state, t):\n    x, y = state\n    dxdt = 0.5 * x - 0.02 * x * y\n    dydt = -0.3 * y + 0.01 * x * y\n    return dxdt, dydt\nstate0 = [40, 9]\nt = np.arange(0, 100, 0.1)\nstate = odeint(predator_prey, state0, t)\nimport matplotlib.pyplot as plt\nplt.plot(state[:, 0], state[:, 1])\nplt.show()"
      },
      {
        "title": "Conclusion",
        "content": "In summary, Systems of Differential Equations and Phase Plane Analysis is a powerful tool for analyzing and visualizing the behavior of complex systems, and it has numerous applications in physics, biology, chemistry, and other fields.",
        "code": ""
      }
    ],
    [
      {
        "title": "Introduction to Numerical Methods",
        "content": "Numerical methods for solving differential equations are essential tools in various fields, providing a way to approximate the solution of differential equations, which is crucial in understanding the behavior of complex systems.",
        "code": ""
      },
      {
        "title": "Euler's Method",
        "content": "Euler's method is the simplest example of a Runge-Kutta method, which approximates the solution of differential equations by approximating the slope of the secant line. It uses the formula: y(n+1) = y(n) + h * f(x(n), y(n))",
        "code": "def euler_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        y.append(y[i] + h * f(x[i], y[i]))\n    return x, y"
      },
      {
        "title": "Improved Euler's Method",
        "content": "The improved Euler's method is a variation of Euler's method that uses the average slope of the secant line to improve the accuracy of the approximation. It uses the formula: y(n+1) = y(n) + 0.5 * h * (f(x(n), y(n)) + f(x(n+1), y(n) + h * f(x(n), y(n))))",
        "code": "def improved_euler_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        y_pred = y[i] + h * f(x[i], y[i])\n        y.append(y[i] + 0.5 * h * (f(x[i], y[i]) + f(x[i+1], y_pred)))\n    return x, y"
      },
      {
        "title": "Third-Order Runge-Kutta Method",
        "content": "The third-order Runge-Kutta method is a popular method used to approximate the solution of differential equations. It uses the formula: y(n+1) = y(n) + (1/6) * h * (k1 + 4 * k2 + k3), where k1, k2, and k3 are the slopes of the secant line at different points",
        "code": "def third_order_runge_kutta_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        k1 = f(x[i], y[i])\n        k2 = f(x[i] + 0.5 * h, y[i] + 0.5 * h * k1)\n        k3 = f(x[i] + h, y[i] - k1 * h + 2 * k2 * h)\n        y.append(y[i] + (1/6) * h * (k1 + 4 * k2 + k3))\n    return x, y"
      },
      {
        "title": "Runge-Kutta-Fehlberg Methods",
        "content": "The Runge-Kutta-Fehlberg methods are a family of methods that use a combination of the Runge-Kutta method and the Fehlberg method to approximate the solution of differential equations. They are known for their high accuracy and stability",
        "code": "def runge_kutta_fehlberg_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        k1 = f(x[i], y[i])\n        k2 = f(x[i] + (1/4) * h, y[i] + (1/4) * h * k1)\n        k3 = f(x[i] + (3/8) * h, y[i] + (3/32) * h * k1 + (9/32) * h * k2)\n        k4 = f(x[i] + (12/13) * h, y[i] + (1932/2197) * h * k1 - (7200/2197) * h * k2 + (7296/2197) * h * k3)\n        k5 = f(x[i] + h, y[i] + (439/216) * h * k1 - 8 * h * k2 + (3680/513) * h * k3 - (845/4104) * h * k4)\n        k6 = f(x[i] + (1/2) * h, y[i] - (8/27) * h * k1 + 2 * h * k2 - (3544/2565) * h * k3 + (1859/4104) * h * k4 - (11/40) * h * k5)\n        y.append(y[i] + (25/216) * h * k1 + (1408/2565) * h * k3 + (2197/4104) * h * k4 - (1/5) * h * k5)\n    return x, y"
      }
    ]
  ],
  "lecture": [
    "**Differential Equations Teaching Script**\n==============================================\n\n### Introduction to Ordinary Differential Equations\n\nWelcome to the world of differential equations! Today, we're going to explore ordinary differential equations (ODEs), which are equations that involve an unknown function of one independent variable and its derivatives. ODEs are used to model various physical phenomena, such as population growth, chemical reactions, and mechanical vibrations.\n\nTo understand why ODEs are important, let's consider a simple example. Imagine you're a biologist studying the growth of a population of bacteria. You want to know how the population will change over time, given certain conditions such as food availability and environmental factors. An ODE can help you model this situation and make predictions about the future population size.\n\n### Classification of Differential Equations\n\nDifferential equations can be classified into two main categories: ordinary differential equations (ODEs) and partial differential equations (PDEs). ODEs involve functions of one independent variable, while PDEs involve functions of multiple independent variables.\n\nThink of it like this: ODEs are like a single-variable equation, where you have one input (x) and one output (y). PDEs, on the other hand, are like a multivariable equation, where you have multiple inputs (x, y, z) and one output.\n\n### First-Order Linear Equations\n\nA first-order linear equation is of the form dy/dx + P(x)y = Q(x), where P(x) and Q(x) are functions of x. These equations can be solved using an integrating factor.\n\nLet's look at an example:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x) + 2*x*f(x), 3*x)\nsol = sp.dsolve(eq)\nprint(sol)\n```\nThis code solves the differential equation f'(x) + 2xf(x) = 3x, where f(x) is the unknown function.\n\n### Separable, Exact, and Bernoulli Differential Equations\n\nSeparable equations can be written in the form dy/dx = f(x)/g(y), while exact equations satisfy the condition \u2202M/\u2202y = \u2202N/\u2202x. Bernoulli equations are of the form dy/dx + P(x)y = Q(x)y^n.\n\nLet's consider an example of a separable equation:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ny = sp.symbols('y')\neq = sp.Eq(y*(x**2 + 1), x*y**2)\nsol = sp.dsolve(eq)\nprint(sol)\n```\nThis code solves the differential equation y(x^2 + 1) = xy^2, which is a separable equation.\n\n### Euler's Method\n\nEuler's method is a numerical method used to approximate the solution of a differential equation. It works by iteratively applying the equation y'(x) = f(x,y) to find the next value of y.\n\nHere's an example of Euler's method in Python:\n```python\nimport numpy as np\n\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        x[i+1] = x[i] + h\n        y[i+1] = y[i] + h * f(x[i], y[i])\n    return x, y\n```\nThis code defines a function `euler_method` that takes in the function `f(x,y)`, the initial conditions `x0` and `y0`, the step size `h`, and the number of steps `n`. It returns the approximate solution `x` and `y`.\n\n### Higher-Order Linear ODEs\n\nHigher-order linear ODEs are of the form a_n*y^(n) + a_(n-1)*y^(n-1) + ... + a_1*y' + a_0*y = f(x), where a_i are constants.\n\nLet's look at an example:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x, 2) + 2*f(x).diff(x) + f(x), 0)\nsol = sp.dsolve(eq)\nprint(sol)\n```\nThis code solves the differential equation f''(x) + 2f'(x) + f(x) = 0, which is a second-order linear ODE.\n\n### Laplace Transform and Power Series Methods\n\nThe Laplace transform is a method used to solve differential equations by transforming the equation into a simpler form. The power series method involves representing the solution as a power series and finding the coefficients.\n\nLet's consider an example of the Laplace transform:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ns = sp.symbols('s')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x), -f(x))\nF = sp.laplace_transform(f(x), x, s)\nprint(F)\n```\nThis code applies the Laplace transform to the differential equation f'(x) = -f(x).\n\n### Introduction to Separation of Variables\n\nSeparation of Variables is a method for solving differential equations by separating the variables. The general form of a separable differential equation is N(y)dy/dx = M(x), where N(y) is a function of y and M(x) is a function of x.\n\nLet's look at an example:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ny = sp.symbols('y')\ndy_dx = 2*x\n# Separate the variables and integrate\ndy = sp.integrate(dy_dx, x)\n```\nThis code separates the variables and integrates the differential equation dy/dx = 2x.\n\n### General Form of Separable Differential Equations\n\nTo solve this type of equation, we integrate both sides with respect to x, which gives us \u222bN(y)dy = \u222bM(x)dx. This method is used to solve various types of first-order differential equations.\n\n### Solution Process for Separable Differential Equations\n\nThe solution process involves rearranging the equation so that everything involving y appears on one side, and everything involving x appears on the other. The equation can then be integrated directly, yielding a solution for y(x).\n\n### Example of Separable Differential Equation\n\nConsider the differential equation dy/dx = 2x. We can separate the variables by multiplying both sides by dx and dividing by 2x, giving us dy/2x = dx. Integrating both sides, we get \u222bdy = 2\u222bxdx.\n\n### First-Order Differential Equations\n\nIn addition to separable differential equations, the topic of First-Order Differential Equations covers other important concepts, such as intervals of validity, equilibrium solutions, and Euler's Method.\n\n### Intervals of Validity and Equilibrium Solutions\n\nIntervals of validity refer to the range of values for which a solution to a differential equation is valid, while equilibrium solutions are solutions that do not change over time.\n\n### Introduction to Higher-Order Linear Differential Equations\n\nHigher-order linear differential equations are extensions of the concepts and methods used to solve 2nd order linear differential equations. These equations appear in various applications, although 2nd order equations are more common.\n\n### Key Concepts and Methods\n\nThe basic results and concepts, such as the Principle of Superposition, linearly independent functions, and the Wronskian, are similar to those for 2nd order equations, but with some natural extensions.\n\n### Method of Undetermined Coefficients\n\nTo solve higher-order linear differential equations, one can use methods such as undetermined coefficients, which is similar to the method used for 2nd order equations, with only one small natural extension.\n\n### Solving Higher-Order Linear Homogeneous Differential Equations\n\nThe process of solving higher-order linear homogeneous differential equations is also similar to solving 2nd order equations, with some extensions to repeated real roots that occur more than twice and repeated complex roots.\n\n### Example: Solving a 3rd Order Linear Homogeneous Differential Equation\n\nConsider the differential equation y''' - 6y'' + 11y' - 6y = 0. We can use the method of undetermined coefficients to solve this equation.\n```python\nimport sympy as sp\n\ny = sp.Function('y')\neq = y(0).diff(3) - 6*y(0).diff(2) + 11*y(0).diff(1) - 6*y(0)\nprint(sp.dsolve(eq, y(0)))\n```\nThis code solves the differential equation y''' - 6y'' + 11y' - 6y = 0.\n\n### Finding Roots of the Characteristic Polynomial\n\nThe main difficulty in solving higher-order linear differential equations is finding all the roots of the characteristic polynomial.\n```python\nimport numpy as np\n\n# Define the coefficients of the characteristic polynomial\ncoeffs = [1, -6, 11, -6]\n# Find the roots of the polynomial\nroots = np.roots(coeffs)\nprint(roots)\n```\nThis code finds the roots of the characteristic polynomial.\n\n### Introduction to Systems of Differential Equations and Phase Plane Analysis\n\nSystems of Differential Equations and Phase Plane Analysis is a topic in applied mathematics that deals with the analysis of systems of differential equations and their behavior in the phase plane.\n\n### Phase Plane\n\nThe phase plane is a graphical representation of the solutions of a system of differential equations, where the x and y axes represent the state variables of the system.\n\n### Phase Portraits\n\nPhase portraits are used to visualize the behavior of the system, including the stability of equilibrium solutions.\n\n### Equilibrium Solutions\n\nEquilibrium solutions are solutions for which the derivative of the state variables is zero, and they can be classified as asymptotically stable, unstable, or semi-stable.\n\n### Stability of Equilibrium Solutions\n\nThe stability of equilibrium solutions can be determined by analyzing the eigenvalues of the system matrix.\n```python\nimport numpy as np\n\nA = np.array([[1, 2], [3, 4]])\neigenvalues, eigenvectors = np.linalg.eig(A)\n```\nThis code computes the eigenvalues and eigenvectors of the system matrix A.\n\n### Classification of Equilibrium Points\n\nFor a 2x2 linear system with distinct real eigenvalues, the equilibrium point can be a sink, source, or saddle point.\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef system(x, y):\n    return x + y, x - y\n\nx = np.linspace(-10, 10, 400)\ny = np.linspace(-10, 10, 400)\nX, Y = np.meshgrid(x, y)\nDX, DY = system(X, Y)\nplt.quiver(X, Y, DX, DY)\nplt.show()\n```\nThis code plots the phase portrait of the system.\n\n### Applications of Phase Plane Analysis\n\nThe phase plane can also be used to visualize the behavior of physical systems, including oscillatory systems such as predator-prey models and chemical reactions.\n```python\nimport numpy as np\nfrom scipy.integrate import odeint\n\ndef predator_prey(state, t):\n    x, y = state\n    dxdt = 0.5 * x - 0.02 * x * y\n    dydt = -0.3 * y + 0.01 * x * y\n    return dxdt, dydt\n\nstate0 = [40, 9]\nt = np.arange(0, 100, 0.1)\nstate = odeint(predator_prey, state0, t)\nimport matplotlib.pyplot as plt\nplt.plot(state[:, 0], state[:, 1])\nplt.show()\n```\nThis code plots the phase portrait of the predator-prey model.\n\n### Introduction to Numerical Methods\n\nNumerical methods for solving differential equations are essential tools in various fields, providing a way to approximate the solution of differential equations, which is crucial in understanding the behavior of complex systems.\n\n### Euler's Method\n\nEuler's method is the simplest example of a Runge-Kutta method, which approximates the solution of differential equations by approximating the slope of the secant line. It uses the formula: y(n+1) = y(n) + h * f(x(n), y(n))\n```python\ndef euler_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        y.append(y[i] + h * f(x[i], y[i]))\n    return x, y\n```\nThis code defines the Euler method.\n\n### Improved Euler's Method\n\nThe improved Euler's method is a variation of Euler's method that uses the average slope of the secant line to improve the accuracy of the approximation. It uses the formula: y(n+1) = y(n) + 0.5 * h * (f(x(n), y(n)) + f(x(n+1), y(n) + h * f(x(n), y(n))))\n```python\ndef improved_euler_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        y_pred = y[i] + h * f(x[i], y[i])\n        y.append(y[i] + 0.5 * h * (f(x[i], y[i]) + f(x[i+1], y_pred)))\n    return x, y\n```\nThis code defines the improved Euler method.\n\n### Third-Order Runge-Kutta Method\n\nThe third-order Runge-Kutta method is a popular method used to approximate the solution of differential equations. It uses the formula: y(n+1) = y(n) + (1/6) * h * (k1 + 4 * k2 + k3), where k1, k2, and k3 are the slopes of the secant line at different points\n```python\ndef third_order_runge_kutta_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        k1 = f(x[i], y[i])\n        k2 = f(x[i] + 0.5 * h, y[i] + 0.5 * h * k1)\n        k3 = f(x[i] + h, y[i] - k1 * h + 2 * k2 * h)\n        y.append(y[i] + (1/6) * h * (k1 + 4 * k2 + k3))\n    return x, y\n```\nThis code defines the third-order Runge-Kutta method.\n\n### Runge-Kutta-Fehlberg Methods\n\nThe Runge-Kutta-Fehlberg methods are a family of methods that use a combination of the Runge-Kutta method and the Fehlberg method to approximate the solution of differential equations. They are known for their high accuracy and stability\n```python\ndef runge_kutta_fehlberg_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        k1 = f(x[i], y[i])\n        k2 = f(x[i] + (1/4) * h, y[i] + (1/4) * h * k1)\n        k3 = f(x[i] + (3/8) * h, y[i] + (3/32) * h * k1 + (9/32) * h * k2)\n        k4 = f(x[i] + (12/13) * h, y[i] + (1932/2197) * h * k1 - (7200/2197) * h * k2 + (7296/2197) * h * k3)\n        k5 = f(x[i] + h, y[i] + (439/216) * h * k1 - 8 * h * k2 + (3680/513) * h * k3 - (845/4104) * h * k4)\n        k6 = f(x[i] + (1/2) * h, y[i] - (8/27) * h * k1 + 2 * h * k2 - (3544/2565) * h * k3 + (1859/4104) * h * k4 - (11/40) * h * k5)\n        y.append(y[i] + (25/216) * h * k1 + (1408/2565) * h * k3 + (2197/4104) * h * k4 - (1/5) * h * k5)\n    return x, y\n```\nThis code defines the Runge-Kutta-Fehlberg method.\n\nThat's a wrap! We've covered the basics of differential equations, including ordinary differential equations, classification, first-order linear equations, separable equations, and numerical methods. We've also explored higher-order linear differential equations, systems of differential equations, and phase plane analysis. Finally, we've introduced numerical methods for solving differential equations, including Euler's method, improved Euler's method, third-order Runge-Kutta method, and Runge-Kutta-Fehlberg methods.\n\nRemember, practice makes perfect! Try working through some examples and exercises to solidify your understanding of these concepts. Happy learning!",
    "**Differential Equations Teaching Script**\n=============================================\n\n### Introduction to Ordinary Differential Equations\n\nWelcome to our journey into the world of differential equations. Ordinary Differential Equations (ODEs) are equations that involve an unknown function of one independent variable and its derivatives. They are used to model various physical phenomena, such as population growth, chemical reactions, and mechanical vibrations.\n\nTo illustrate this concept, let's consider a simple example. Imagine a cup of hot coffee left on a table. As time passes, the coffee cools down to room temperature. We can model this situation using an ODE, where the temperature of the coffee is the unknown function, and time is the independent variable.\n\n### Classification of Differential Equations\n\nDifferential equations can be classified into two main categories: ordinary differential equations (ODEs) and partial differential equations (PDEs). ODEs involve functions of one independent variable, while PDEs involve functions of multiple independent variables.\n\nThink of it like a map. ODEs are like a one-dimensional map, where you can only move in one direction. PDEs, on the other hand, are like a two-dimensional map, where you can move in multiple directions.\n\n### First-Order Linear Equations\n\nA first-order linear equation is of the form dy/dx + P(x)y = Q(x), where P(x) and Q(x) are functions of x. These equations can be solved using an integrating factor.\n\nLet's take a look at an example. Suppose we have the equation dy/dx + 2xy = 3x. We can solve this equation using an integrating factor, which is a function that multiplies the equation to make it easier to integrate.\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x) + 2*x*f(x), 3*x)\nsol = sp.dsolve(eq)\nprint(sol)\n```\n\nThis code solves the differential equation using SymPy, a Python library for symbolic mathematics.\n\n### Separable, Exact, and Bernoulli Differential Equations\n\nSeparable equations can be written in the form dy/dx = f(x)/g(y), while exact equations satisfy the condition \u2202M/\u2202y = \u2202N/\u2202x. Bernoulli equations are of the form dy/dx + P(x)y = Q(x)y^n.\n\nLet's consider an example of a separable equation. Suppose we have the equation dy/dx = 2x/y. We can separate the variables by multiplying both sides by y and dividing by 2x, giving us y dy = 2x dx.\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ny = sp.symbols('y')\neq = sp.Eq(y*(x**2 + 1), x*y**2)\nsol = sp.dsolve(eq)\nprint(sol)\n```\n\nThis code solves the differential equation using SymPy.\n\n### Euler's Method\n\nEuler's method is a numerical method used to approximate the solution of a differential equation. It works by iteratively applying the equation y'(x) = f(x,y) to find the next value of y.\n\nLet's take a look at an example. Suppose we have the equation dy/dx = f(x,y) and we want to approximate the solution at x = 1, given the initial condition y(0) = 1.\n\n```python\nimport numpy as np\n\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        x[i+1] = x[i] + h\n        y[i+1] = y[i] + h * f(x[i], y[i])\n    return x, y\n```\n\nThis code implements Euler's method using NumPy.\n\n### Higher-Order Linear ODEs\n\nHigher-order linear ODEs are of the form a_n*y^(n) + a_(n-1)*y^(n-1) + ... + a_1*y' + a_0*y = f(x), where a_i are constants.\n\nLet's consider an example. Suppose we have the equation y'' + 2y' + y = 0. We can solve this equation using SymPy.\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x, 2) + 2*f(x).diff(x) + f(x), 0)\nsol = sp.dsolve(eq)\nprint(sol)\n```\n\nThis code solves the differential equation using SymPy.\n\n### Laplace Transform and Power Series Methods\n\nThe Laplace transform is a method used to solve differential equations by transforming the equation into a simpler form. The power series method involves representing the solution as a power series and finding the coefficients.\n\nLet's take a look at an example. Suppose we have the equation dy/dx = -y. We can solve this equation using the Laplace transform.\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ns = sp.symbols('s')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x), -f(x))\nF = sp.laplace_transform(f(x), x, s)\nprint(F)\n```\n\nThis code applies the Laplace transform to the differential equation using SymPy.\n\n### Introduction to Separation of Variables\n\nSeparation of Variables is a method for solving differential equations by separating the variables. The general form of a separable differential equation is N(y)dy/dx = M(x), where N(y) is a function of y and M(x) is a function of x.\n\nLet's consider an example. Suppose we have the equation dy/dx = 2x/y. We can separate the variables by multiplying both sides by y and dividing by 2x, giving us y dy = 2x dx.\n\n### General Form of Separable Differential Equations\n\nTo solve this type of equation, we integrate both sides with respect to x, which gives us \u222bN(y)dy = \u222bM(x)dx. This method is used to solve various types of first-order differential equations.\n\n### Solution Process for Separable Differential Equations\n\nThe solution process involves rearranging the equation so that everything involving y appears on one side, and everything involving x appears on the other. The equation can then be integrated directly, yielding a solution for y(x).\n\n### Example of Separable Differential Equation\n\nConsider the differential equation dy/dx = 2x. We can separate the variables by multiplying both sides by dx and dividing by 2x, giving us dy/2x = dx. Integrating both sides, we get \u222bdy = 2\u222bxdx.\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ny = sp.symbols('y')\ndy_dx = 2*x\n# Separate the variables and integrate\ndy = sp.integrate(dy_dx, x)\n```\n\nThis code separates the variables and integrates the equation using SymPy.\n\n### First-Order Differential Equations\n\nIn addition to separable differential equations, the topic of First-Order Differential Equations covers other important concepts, such as intervals of validity, equilibrium solutions, and Euler's Method.\n\n### Intervals of Validity and Equilibrium Solutions\n\nIntervals of validity refer to the range of values for which a solution to a differential equation is valid, while equilibrium solutions are solutions that do not change over time.\n\n### Euler's Method\n\nEuler's Method is a numerical method for solving differential equations, which involves approximating the solution at a given point using the derivative of the function at that point.\n\n```python\nimport numpy as np\n\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        y[i+1] = y[i] + h * f(x[i], y[i])\n        x[i+1] = x[i] + h\n    return x, y\n```\n\nThis code implements Euler's method using NumPy.\n\n### Introduction to Higher-Order Linear Differential Equations\n\nHigher-order linear differential equations are extensions of the concepts and methods used to solve 2nd order linear differential equations. These equations appear in various applications, although 2nd order equations are more common.\n\n### Key Concepts and Methods\n\nThe basic results and concepts, such as the Principle of Superposition, linearly independent functions, and the Wronskian, are similar to those for 2nd order equations, but with some natural extensions.\n\n### Method of Undetermined Coefficients\n\nTo solve higher-order linear differential equations, one can use methods such as undetermined coefficients, which is similar to the method used for 2nd order equations, with only one small natural extension.\n\n### Solving Higher-Order Linear Homogeneous Differential Equations\n\nThe process of solving higher-order linear homogeneous differential equations is also similar to solving 2nd order equations, with some extensions to repeated real roots that occur more than twice and repeated complex roots.\n\n### Example: Solving a 3rd Order Linear Homogeneous Differential Equation\n\nConsider the differential equation y''' - 6y'' + 11y' - 6y = 0. We can use the method of undetermined coefficients to solve this equation.\n\n```python\nimport sympy as sp\n\ny = sp.Function('y')\neq = y(0).diff(3) - 6*y(0).diff(2) + 11*y(0).diff(1) - 6*y(0)\nprint(sp.dsolve(eq, y(0)))\n```\n\nThis code solves the differential equation using SymPy.\n\n### Finding Roots of the Characteristic Polynomial\n\nThe main difficulty in solving higher-order linear differential equations is finding all the roots of the characteristic polynomial.\n\n```python\nimport numpy as np\n\n# Define the coefficients of the characteristic polynomial\ncoeffs = [1, -6, 11, -6]\n# Find the roots of the polynomial\nroots = np.roots(coeffs)\nprint(roots)\n```\n\nThis code finds the roots of the characteristic polynomial using NumPy.\n\n### Introduction to Systems of Differential Equations and Phase Plane Analysis\n\nSystems of Differential Equations and Phase Plane Analysis is a topic in applied mathematics that deals with the analysis of systems of differential equations and their behavior in the phase plane.\n\n### Phase Plane\n\nThe phase plane is a graphical representation of the solutions of a system of differential equations, where the x and y axes represent the state variables of the system.\n\n### Phase Portraits\n\nPhase portraits are used to visualize the behavior of the system, including the stability of equilibrium solutions.\n\n### Equilibrium Solutions\n\nEquilibrium solutions are solutions for which the derivative of the state variables is zero, and they can be classified as asymptotically stable, unstable, or semi-stable.\n\n### Stability of Equilibrium Solutions\n\nThe stability of equilibrium solutions can be determined by analyzing the eigenvalues of the system matrix.\n\n```python\nimport numpy as np\n\nA = np.array([[1, 2], [3, 4]])\neigenvalues, eigenvectors = np.linalg.eig(A)\n```\n\nThis code calculates the eigenvalues and eigenvectors of the system matrix using NumPy.\n\n### Classification of Equilibrium Points\n\nFor a 2x2 linear system with distinct real eigenvalues, the equilibrium point can be a sink, source, or saddle point.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef system(x, y):\n    return x + y, x - y\n\nx = np.linspace(-10, 10, 400)\ny = np.linspace(-10, 10, 400)\nX, Y = np.meshgrid(x, y)\nDX, DY = system(X, Y)\nplt.quiver(X, Y, DX, DY)\nplt.show()\n```\n\nThis code plots the phase portrait of the system using Matplotlib.\n\n### Applications of Phase Plane Analysis\n\nThe phase plane can also be used to visualize the behavior of physical systems, including oscillatory systems such as predator-prey models and chemical reactions.\n\n```python\nimport numpy as np\nfrom scipy.integrate import odeint\n\ndef predator_prey(state, t):\n    x, y = state\n    dxdt = 0.5 * x - 0.02 * x * y\n    dydt = -0.3 * y + 0.01 * x * y\n    return dxdt, dydt\n\nstate0 = [40, 9]\nt = np.arange(0, 100, 0.1)\nstate = odeint(predator_prey, state0, t)\nimport matplotlib.pyplot as plt\nplt.plot(state[:, 0], state[:, 1])\nplt.show()\n```\n\nThis code simulates the predator-prey model using SciPy and plots the phase portrait using Matplotlib.\n\n### Conclusion\n\nIn summary, Systems of Differential Equations and Phase Plane Analysis is a powerful tool for analyzing and visualizing the behavior of complex systems, and it has numerous applications in physics, biology, chemistry, and other fields.\n\n### Introduction to Numerical Methods\n\nNumerical methods for solving differential equations are essential tools in various fields, providing a way to approximate the solution of differential equations, which is crucial in understanding the behavior of complex systems.\n\n### Euler's Method\n\nEuler's method is the simplest example of a Runge-Kutta method, which approximates the solution of differential equations by approximating the slope of the secant line. It uses the formula: y(n+1) = y(n) + h * f(x(n), y(n))\n\n```python\ndef euler_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        y.append(y[i] + h * f(x[i], y[i]))\n    return x, y\n```\n\nThis code implements Euler's method.\n\n### Improved Euler's Method\n\nThe improved Euler's method is a variation of Euler's method that uses the average slope of the secant line to improve the accuracy of the approximation. It uses the formula: y(n+1) = y(n) + 0.5 * h * (f(x(n), y(n)) + f(x(n+1), y(n) + h * f(x(n), y(n))))\n\n```python\ndef improved_euler_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        y_pred = y[i] + h * f(x[i], y[i])\n        y.append(y[i] + 0.5 * h * (f(x[i], y[i]) + f(x[i+1], y_pred)))\n    return x, y\n```\n\nThis code implements the improved Euler's method.\n\n### Third-Order Runge-Kutta Method\n\nThe third-order Runge-Kutta method is a popular method used to approximate the solution of differential equations. It uses the formula: y(n+1) = y(n) + (1/6) * h * (k1 + 4 * k2 + k3), where k1, k2, and k3 are the slopes of the secant line at different points\n\n```python\ndef third_order_runge_kutta_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        k1 = f(x[i], y[i])\n        k2 = f(x[i] + 0.5 * h, y[i] + 0.5 * h * k1)\n        k3 = f(x[i] + h, y[i] - k1 * h + 2 * k2 * h)\n        y.append(y[i] + (1/6) * h * (k1 + 4 * k2 + k3))\n    return x, y\n```\n\nThis code implements the third-order Runge-Kutta method.\n\n### Runge-Kutta-Fehlberg Methods\n\nThe Runge-Kutta-Fehlberg methods are a family of methods that use a combination of the Runge-Kutta method and the Fehlberg method to approximate the solution of differential equations. They are known for their high accuracy and stability\n\n```python\ndef runge_kutta_fehlberg_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        k1 = f(x[i], y[i])\n        k2 = f(x[i] + (1/4) * h, y[i] + (1/4) * h * k1)\n        k3 = f(x[i] + (3/8) * h, y[i] + (3/32) * h * k1 + (9/32) * h * k2)\n        k4 = f(x[i] + (12/13) * h, y[i] + (1932/2197) * h * k1 - (7200/2197) * h * k2 + (7296/2197) * h * k3)\n        k5 = f(x[i] + h, y[i] + (439/216) * h * k1 - 8 * h * k2 + (3680/513) * h * k3 - (845/4104) * h * k4)\n        k6 = f(x[i] + (1/2) * h, y[i] - (8/27) * h * k1 + 2 * h * k2 - (3544/2565) * h * k3 + (1859/4104) * h * k4 - (11/40) * h * k5)\n        y.append(y[i] + (25/216) * h * k1 + (1408/2565) * h * k3 + (2197/4104) * h * k4 - (1/5) * h * k5)\n    return x, y\n```\n\nThis code implements the Runge-Kutta-Fehlberg method.",
    "# Introduction to Differential Equations\n## Introduction to Ordinary Differential Equations\n\nWelcome, students, to our lecture on differential equations. Today, we're going to explore the world of ordinary differential equations, or ODEs for short. An ODE is an equation that involves an unknown function of one independent variable and its derivatives. These equations are used to model various physical phenomena, such as population growth, chemical reactions, and mechanical vibrations.\n\nTo understand the importance of ODEs, let's consider a real-world example. Imagine you're a biologist studying the growth of a population of rabbits. You can model the growth of the population using an ODE, taking into account factors such as food supply, predators, and disease. By solving the ODE, you can predict the future growth of the population and make informed decisions about conservation efforts.\n\n## Classification of Differential Equations\n\nNow that we've introduced ODEs, let's talk about the different types of differential equations. There are two main categories: ordinary differential equations (ODEs) and partial differential equations (PDEs). ODEs involve functions of one independent variable, while PDEs involve functions of multiple independent variables.\n\nTo illustrate the difference, consider a simple example. The equation dy/dx = 2x is an ODE, because it involves only one independent variable, x. On the other hand, the equation \u2202u/\u2202x + \u2202u/\u2202y = 0 is a PDE, because it involves two independent variables, x and y.\n\n# First-Order Linear Equations\n\n## First-Order Linear Equations\n\nA first-order linear equation is of the form dy/dx + P(x)y = Q(x), where P(x) and Q(x) are functions of x. These equations can be solved using an integrating factor. The integrating factor is a function that, when multiplied by the differential equation, makes the left-hand side exact.\n\nLet's consider an example. Suppose we have the equation dy/dx + 2xy = 3x. We can solve this equation using an integrating factor. The integrating factor is e^(x^2), so we multiply both sides of the equation by e^(x^2) to get:\n\ne^(x^2)dy/dx + 2xe^(x^2)y = 3xe^(x^2)\n\nThis can be rewritten as:\n\nd(e^(x^2)y)/dx = 3xe^(x^2)\n\nNow we can integrate both sides with respect to x to get:\n\ne^(x^2)y = \u222b3xe^(x^2)dx\n\nWe can use Python to solve this equation and plot the solution. Here's an example code snippet:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x) + 2*x*f(x), 3*x)\nsol = sp.dsolve(eq)\nprint(sol)\n```\n\n# Separable, Exact, and Bernoulli Differential Equations\n\n## Separable, Exact, and Bernoulli Differential Equations\n\nSeparable equations can be written in the form dy/dx = f(x)/g(y), while exact equations satisfy the condition \u2202M/\u2202y = \u2202N/\u2202x. Bernoulli equations are of the form dy/dx + P(x)y = Q(x)y^n.\n\nLet's consider an example of a separable equation. Suppose we have the equation dy/dx = 2x/y. We can separate the variables by multiplying both sides by y and dividing by 2x, giving us:\n\nydy = 2xdx\n\nNow we can integrate both sides to get:\n\n\u222bydy = \u222b2xdx\n\ny^2 = x^2 + C\n\nWe can use Python to solve this equation and plot the solution. Here's an example code snippet:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ny = sp.symbols('y')\neq = sp.Eq(y*(x**2 + 1), x*y**2)\nsol = sp.dsolve(eq)\nprint(sol)\n```\n\n# Euler's Method\n\n## Euler's Method\n\nEuler's method is a numerical method used to approximate the solution of a differential equation. It works by iteratively applying the equation y'(x) = f(x,y) to find the next value of y.\n\nLet's consider an example. Suppose we have the equation dy/dx = 2x, and we want to find the solution at x = 1, given the initial condition y(0) = 0. We can use Euler's method to approximate the solution.\n\nHere's an example code snippet:\n```python\nimport numpy as np\n\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        x[i+1] = x[i] + h\n        y[i+1] = y[i] + h * f(x[i], y[i])\n    return x, y\n\ndef f(x, y):\n    return 2*x\n\nx0 = 0\ny0 = 0\nh = 0.1\nn = 10\n\nx, y = euler_method(f, x0, y0, h, n)\nprint(x, y)\n```\n\n# Higher-Order Linear ODEs\n\n## Higher-Order Linear ODEs\n\nHigher-order linear ODEs are of the form a_n*y^(n) + a_(n-1)*y^(n-1) + ... + a_1*y' + a_0*y = f(x), where a_i are constants.\n\nLet's consider an example. Suppose we have the equation y'' + 2y' + y = 0. We can solve this equation using the characteristic equation method.\n\nHere's an example code snippet:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x, 2) + 2*f(x).diff(x) + f(x), 0)\nsol = sp.dsolve(eq)\nprint(sol)\n```\n\n# Laplace Transform and Power Series Methods\n\n## Laplace Transform and Power Series Methods\n\nThe Laplace transform is a method used to solve differential equations by transforming the equation into a simpler form. The power series method involves representing the solution as a power series and finding the coefficients.\n\nLet's consider an example. Suppose we have the equation y' + y = 0. We can solve this equation using the Laplace transform method.\n\nHere's an example code snippet:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ns = sp.symbols('s')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x), -f(x))\nF = sp.laplace_transform(f(x), x, s)\nprint(F)\n```\n\n# Separation of Variables\n\n## Introduction to Separation of Variables\n\nSeparation of Variables is a method for solving differential equations by separating the variables. The general form of a separable differential equation is N(y)dy/dx = M(x), where N(y) is a function of y and M(x) is a function of x.\n\nLet's consider an example. Suppose we have the equation dy/dx = 2x/y. We can separate the variables by multiplying both sides by y and dividing by 2x, giving us:\n\nydy = 2xdx\n\nNow we can integrate both sides to get:\n\n\u222bydy = \u222b2xdx\n\ny^2 = x^2 + C\n\n## General Form of Separable Differential Equations\n\nTo solve this type of equation, we integrate both sides with respect to x, which gives us \u222bN(y)dy = \u222bM(x)dx. This method is used to solve various types of first-order differential equations.\n\n## Solution Process for Separable Differential Equations\n\nThe solution process involves rearranging the equation so that everything involving y appears on one side, and everything involving x appears on the other. The equation can then be integrated directly, yielding a solution for y(x).\n\n## Example of Separable Differential Equation\n\nConsider the differential equation dy/dx = 2x. We can separate the variables by multiplying both sides by dx and dividing by 2x, giving us:\n\ndy/2x = dx\n\nNow we can integrate both sides to get:\n\n\u222bdy = 2\u222bxdx\n\ny = x^2 + C\n\nHere's an example code snippet:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ny = sp.symbols('y')\ndy_dx = 2*x\ndy = sp.integrate(dy_dx, x)\nprint(dy)\n```\n\n# First-Order Differential Equations\n\n## First-Order Differential Equations\n\nIn addition to separable differential equations, the topic of First-Order Differential Equations covers other important concepts, such as intervals of validity, equilibrium solutions, and Euler's Method.\n\n## Intervals of Validity and Equilibrium Solutions\n\nIntervals of validity refer to the range of values for which a solution to a differential equation is valid, while equilibrium solutions are solutions that do not change over time.\n\n## Euler's Method\n\nEuler's Method is a numerical method for solving differential equations, which involves approximating the solution at a given point using the derivative of the function at that point.\n\nHere's an example code snippet:\n```python\nimport numpy as np\n\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        y[i+1] = y[i] + h * f(x[i], y[i])\n        x[i+1] = x[i] + h\n    return x, y\n```\n\n# Higher-Order Linear Differential Equations\n\n## Introduction to Higher-Order Linear Differential Equations\n\nHigher-order linear differential equations are extensions of the concepts and methods used to solve 2nd order linear differential equations. These equations appear in various applications, although 2nd order equations are more common.\n\n## Key Concepts and Methods\n\nThe basic results and concepts, such as the Principle of Superposition, linearly independent functions, and the Wronskian, are similar to those for 2nd order equations, but with some natural extensions.\n\n## Method of Undetermined Coefficients\n\nTo solve higher-order linear differential equations, one can use methods such as undetermined coefficients, which is similar to the method used for 2nd order equations, with only one small natural extension.\n\n## Solving Higher-Order Linear Homogeneous Differential Equations\n\nThe process of solving higher-order linear homogeneous differential equations is also similar to solving 2nd order equations, with some extensions to repeated real roots that occur more than twice and repeated complex roots.\n\n## Example: Solving a 3rd Order Linear Homogeneous Differential Equation\n\nConsider the differential equation y''' - 6y'' + 11y' - 6y = 0. We can use the method of undetermined coefficients to solve this equation.\n\nHere's an example code snippet:\n```python\nimport sympy as sp\n\ny = sp.Function('y')\neq = y(0).diff(3) - 6*y(0).diff(2) + 11*y(0).diff(1) - 6*y(0)\nprint(sp.dsolve(eq, y(0)))\n```\n\n# Systems of Differential Equations and Phase Plane Analysis\n\n## Introduction to Systems of Differential Equations and Phase Plane Analysis\n\nSystems of Differential Equations and Phase Plane Analysis is a topic in applied mathematics that deals with the analysis of systems of differential equations and their behavior in the phase plane.\n\n## Phase Plane\n\nThe phase plane is a graphical representation of the solutions of a system of differential equations, where the x and y axes represent the state variables of the system.\n\n## Phase Portraits\n\nPhase portraits are used to visualize the behavior of the system, including the stability of equilibrium solutions.\n\n## Equilibrium Solutions\n\nEquilibrium solutions are solutions for which the derivative of the state variables is zero, and they can be classified as asymptotically stable, unstable, or semi-stable.\n\n## Stability of Equilibrium Solutions\n\nThe stability of equilibrium solutions can be determined by analyzing the eigenvalues of the system matrix.\n\nHere's an example code snippet:\n```python\nimport numpy as np\n\nA = np.array([[1, 2], [3, 4]])\neigenvalues, eigenvectors = np.linalg.eig(A)\n```\n\n## Classification of Equilibrium Points\n\nFor a 2x2 linear system with distinct real eigenvalues, the equilibrium point can be a sink, source, or saddle point.\n\nHere's an example code snippet:\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef system(x, y):\n    return x + y, x - y\n\nx = np.linspace(-10, 10, 400)\ny = np.linspace(-10, 10, 400)\nX, Y = np.meshgrid(x, y)\nDX, DY = system(X, Y)\nplt.quiver(X, Y, DX, DY)\nplt.show()\n```\n\n# Numerical Methods\n\n## Introduction to Numerical Methods\n\nNumerical methods for solving differential equations are essential tools in various fields, providing a way to approximate the solution of differential equations, which is crucial in understanding the behavior of complex systems.\n\n## Euler's Method\n\nEuler's method is the simplest example of a Runge-Kutta method, which approximates the solution of differential equations by approximating the slope of the secant line. It uses the formula: y(n+1) = y(n) + h * f(x(n), y(n))\n\nHere's an example code snippet:\n```python\ndef euler_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        y.append(y[i] + h * f(x[i], y[i]))\n    return x, y\n```\n\n## Improved Euler's Method\n\nThe improved Euler's method is a variation of Euler's method that uses the average slope of the secant line to improve the accuracy of the approximation. It uses the formula: y(n+1) = y(n) + 0.5 * h * (f(x(n), y(n)) + f(x(n+1), y(n) + h * f(x(n), y(n))))\n\nHere's an example code snippet:\n```python\ndef improved_euler_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        y_pred = y[i] + h * f(x[i], y[i])\n        y.append(y[i] + 0.5 * h * (f(x[i], y[i]) + f(x[i+1], y_pred)))\n    return x, y\n```\n\n## Third-Order Runge-Kutta Method\n\nThe third-order Runge-Kutta method is a popular method used to approximate the solution of differential equations. It uses the formula: y(n+1) = y(n) + (1/6) * h * (k1 + 4 * k2 + k3), where k1, k2, and k3 are the slopes of the secant line at different points\n\nHere's an example code snippet:\n```python\ndef third_order_runge_kutta_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        k1 = f(x[i], y[i])\n        k2 = f(x[i] + 0.5 * h, y[i] + 0.5 * h * k1)\n        k3 = f(x[i] + h, y[i] - k1 * h + 2 * k2 * h)\n        y.append(y[i] + (1/6) * h * (k1 + 4 * k2 + k3))\n    return x, y\n```\n\n## Runge-Kutta-Fehlberg Methods\n\nThe Runge-Kutta-Fehlberg methods are a family of methods that use a combination of the Runge-Kutta method and the Fehlberg method to approximate the solution of differential equations. They are known for their high accuracy and stability.\n\nHere's an example code snippet:\n```python\ndef runge_kutta_fehlberg_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        k1 = f(x[i], y[i])\n        k2 = f(x[i] + (1/4) * h, y[i] + (1/4) * h * k1)\n        k3 = f(x[i] + (3/8) * h, y[i] + (3/32) * h * k1 + (9/32) * h * k2)\n        k4 = f(x[i] + (12/13) * h, y[i] + (1932/2197) * h * k1 - (7200/2197) * h * k2 + (7296/2197) * h * k3)\n        k5 = f(x[i] + h, y[i] + (439/216) * h * k1 - 8 * h * k2 + (3680/513) * h * k3 - (845/4104) * h * k4)\n        k6 = f(x[i] + (1/2) * h, y[i] - (8/27) * h * k1 + 2 * h * k2 - (3544/2565) * h * k3 + (1859/4104) * h * k4 - (11/40) * h * k5)\n        y.append(y[i] + (25/216) * h * k1 + (1408/2565) * h * k3 + (2197/4104) * h * k4 - (1/5) * h * k5)\n    return x, y\n```",
    "**Differential Equations Teaching Script**\n==============================================\n\n### Introduction to Ordinary Differential Equations\n\nWelcome, students, to the fascinating world of differential equations. Ordinary Differential Equations (ODEs) are equations that involve an unknown function of one independent variable and its derivatives. They are used to model various physical phenomena, such as population growth, chemical reactions, and mechanical vibrations. Can you think of a real-world scenario where ODEs are used to model a complex system?\n\nFor example, consider a population of rabbits living in a forest. The growth rate of the population can be modeled using an ODE, taking into account factors such as food availability, predation, and disease. By solving this ODE, we can predict the future population size and understand the dynamics of the ecosystem.\n\n### Classification of Differential Equations\n\nDifferential equations can be classified into two main categories: ordinary differential equations (ODEs) and partial differential equations (PDEs). ODEs involve functions of one independent variable, while PDEs involve functions of multiple independent variables. What are some examples of physical phenomena that can be modeled using ODEs versus PDEs?\n\nFor instance, the motion of a projectile under the influence of gravity can be modeled using an ODE, as it involves a single independent variable (time). On the other hand, the heat transfer in a building can be modeled using a PDE, as it involves multiple independent variables (space and time).\n\n### First-Order Linear Equations\n\nA first-order linear equation is of the form dy/dx + P(x)y = Q(x), where P(x) and Q(x) are functions of x. These equations can be solved using an integrating factor. Let's consider an example:\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x) + 2*x*f(x), 3*x)\nsol = sp.dsolve(eq)\nprint(sol)\n```\n\nThis code solves the differential equation y' + 2xy = 3x, where y is a function of x. The solution involves an integrating factor, which is used to simplify the equation and find the general solution.\n\n### Separable, Exact, and Bernoulli Differential Equations\n\nSeparable equations can be written in the form dy/dx = f(x)/g(y), while exact equations satisfy the condition \u2202M/\u2202y = \u2202N/\u2202x. Bernoulli equations are of the form dy/dx + P(x)y = Q(x)y^n. Let's consider an example of a separable equation:\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ny = sp.symbols('y')\neq = sp.Eq(y*(x**2 + 1), x*y**2)\nsol = sp.dsolve(eq)\nprint(sol)\n```\n\nThis code solves the differential equation y(x^2 + 1) = xy^2, which is a separable equation. By separating the variables and integrating, we can find the general solution.\n\n### Euler's Method\n\nEuler's method is a numerical method used to approximate the solution of a differential equation. It works by iteratively applying the equation y'(x) = f(x,y) to find the next value of y. Let's consider an example:\n\n```python\nimport numpy as np\n\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        x[i+1] = x[i] + h\n        y[i+1] = y[i] + h * f(x[i], y[i])\n    return x, y\n```\n\nThis code implements Euler's method to approximate the solution of a differential equation. By iteratively applying the equation, we can find an approximate solution at a given point.\n\n### Higher-Order Linear ODEs\n\nHigher-order linear ODEs are of the form a_n*y^(n) + a_(n-1)*y^(n-1) + ... + a_1*y' + a_0*y = f(x), where a_i are constants. Let's consider an example:\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x, 2) + 2*f(x).diff(x) + f(x), 0)\nsol = sp.dsolve(eq)\nprint(sol)\n```\n\nThis code solves the differential equation y'' + 2y' + y = 0, which is a second-order linear ODE. By finding the characteristic equation and solving for the roots, we can find the general solution.\n\n### Laplace Transform and Power Series Methods\n\nThe Laplace transform is a method used to solve differential equations by transforming the equation into a simpler form. The power series method involves representing the solution as a power series and finding the coefficients. Let's consider an example:\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ns = sp.symbols('s')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x), -f(x))\nF = sp.laplace_transform(f(x), x, s)\nprint(F)\n```\n\nThis code uses the Laplace transform to solve the differential equation y' = -y. By transforming the equation, we can find the solution in the s-domain and then transform back to the x-domain.\n\n### Introduction to Separation of Variables\n\nSeparation of Variables is a method for solving differential equations by separating the variables. The general form of a separable differential equation is N(y)dy/dx = M(x), where N(y) is a function of y and M(x) is a function of x. Can you think of a real-world scenario where separation of variables is used to model a complex system?\n\nFor example, consider a cup of hot coffee left on a table. The temperature of the coffee can be modeled using a separable differential equation, taking into account factors such as the initial temperature, the temperature of the surroundings, and the cooling rate. By separating the variables and integrating, we can find the temperature of the coffee at any given time.\n\n### General Form of Separable Differential Equations\n\nTo solve this type of equation, we integrate both sides with respect to x, which gives us \u222bN(y)dy = \u222bM(x)dx. This method is used to solve various types of first-order differential equations. Let's consider an example:\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ny = sp.symbols('y')\ndy_dx = 2*x\n# Separate the variables and integrate\ndy = sp.integrate(dy_dx, x)\nprint(dy)\n```\n\nThis code separates the variables and integrates to find the general solution of the differential equation dy/dx = 2x.\n\n### Solution Process for Separable Differential Equations\n\nThe solution process involves rearranging the equation so that everything involving y appears on one side, and everything involving x appears on the other. The equation can then be integrated directly, yielding a solution for y(x). Can you think of a real-world scenario where this solution process is used to model a complex system?\n\nFor example, consider a population of bacteria growing in a petri dish. The growth rate of the population can be modeled using a separable differential equation, taking into account factors such as the initial population size, the growth rate, and the carrying capacity. By separating the variables and integrating, we can find the population size at any given time.\n\n### Example of Separable Differential Equation\n\nConsider the differential equation dy/dx = 2x. We can separate the variables by multiplying both sides by dx and dividing by 2x, giving us dy/2x = dx. Integrating both sides, we get \u222bdy = 2\u222bxdx. Let's consider an example:\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ny = sp.symbols('y')\ndy_dx = 2*x\n# Separate the variables and integrate\ndy = sp.integrate(dy_dx, x)\nprint(dy)\n```\n\nThis code separates the variables and integrates to find the general solution of the differential equation dy/dx = 2x.\n\n### First-Order Differential Equations\n\nIn addition to separable differential equations, the topic of First-Order Differential Equations covers other important concepts, such as intervals of validity, equilibrium solutions, and Euler's Method. Can you think of a real-world scenario where these concepts are used to model a complex system?\n\nFor example, consider a simple pendulum consisting of a mass attached to a string. The motion of the pendulum can be modeled using a first-order differential equation, taking into account factors such as the initial angle, the length of the string, and the gravitational acceleration. By finding the equilibrium solutions and analyzing the stability, we can understand the behavior of the pendulum.\n\n### Intervals of Validity and Equilibrium Solutions\n\nIntervals of validity refer to the range of values for which a solution to a differential equation is valid, while equilibrium solutions are solutions that do not change over time. Can you think of a real-world scenario where these concepts are used to model a complex system?\n\nFor example, consider a population of rabbits living in a forest. The growth rate of the population can be modeled using a differential equation, taking into account factors such as food availability, predation, and disease. By finding the equilibrium solutions and analyzing the stability, we can understand the behavior of the population and predict the future population size.\n\n### Euler's Method\n\nEuler's Method is a numerical method for solving differential equations, which involves approximating the solution at a given point using the derivative of the function at that point. Let's consider an example:\n\n```python\nimport numpy as np\n\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        y[i+1] = y[i] + h * f(x[i], y[i])\n        x[i+1] = x[i] + h\n    return x, y\n```\n\nThis code implements Euler's method to approximate the solution of a differential equation. By iteratively applying the equation, we can find an approximate solution at a given point.\n\n### Introduction to Higher-Order Linear Differential Equations\n\nHigher-order linear differential equations are extensions of the concepts and methods used to solve 2nd order linear differential equations. These equations appear in various applications, although 2nd order equations are more common. Can you think of a real-world scenario where higher-order linear differential equations are used to model a complex system?\n\nFor example, consider a mechanical system consisting of a mass attached to a spring. The motion of the mass can be modeled using a higher-order linear differential equation, taking into account factors such as the initial displacement, the velocity, and the damping coefficient. By finding the general solution and analyzing the behavior, we can understand the motion of the mass and predict the future position.\n\n### Key Concepts and Methods\n\nThe basic results and concepts, such as the Principle of Superposition, linearly independent functions, and the Wronskian, are similar to those for 2nd order equations, but with some natural extensions. Can you think of a real-world scenario where these concepts are used to model a complex system?\n\nFor example, consider an electrical circuit consisting of resistors, capacitors, and inductors. The behavior of the circuit can be modeled using a higher-order linear differential equation, taking into account factors such as the initial voltage, the current, and the impedance. By finding the general solution and analyzing the behavior, we can understand the behavior of the circuit and predict the future voltage and current.\n\n### Method of Undetermined Coefficients\n\nTo solve higher-order linear differential equations, one can use methods such as undetermined coefficients, which is similar to the method used for 2nd order equations, with only one small natural extension. Let's consider an example:\n\n```python\nimport sympy as sp\n\ny = sp.Function('y')\neq = y(0).diff(3) - 6*y(0).diff(2) + 11*y(0).diff(1) - 6*y(0)\nprint(sp.dsolve(eq, y(0)))\n```\n\nThis code solves the differential equation y''' - 6y'' + 11y' - 6y = 0 using the method of undetermined coefficients.\n\n### Solving Higher-Order Linear Homogeneous Differential Equations\n\nThe process of solving higher-order linear homogeneous differential equations is also similar to solving 2nd order equations, with some extensions to repeated real roots that occur more than twice and repeated complex roots. Can you think of a real-world scenario where these concepts are used to model a complex system?\n\nFor example, consider a mechanical system consisting of a mass attached to a spring. The motion of the mass can be modeled using a higher-order linear homogeneous differential equation, taking into account factors such as the initial displacement, the velocity, and the damping coefficient. By finding the general solution and analyzing the behavior, we can understand the motion of the mass and predict the future position.\n\n### Example of Solving a 3rd Order Linear Homogeneous Differential Equation\n\nConsider the differential equation y''' - 6y'' + 11y' - 6y = 0. We can use the method of undetermined coefficients to solve this equation. Let's consider an example:\n\n```python\nimport sympy as sp\n\ny = sp.Function('y')\neq = y(0).diff(3) - 6*y(0).diff(2) + 11*y(0).diff(1) - 6*y(0)\nprint(sp.dsolve(eq, y(0)))\n```\n\nThis code solves the differential equation y''' - 6y'' + 11y' - 6y = 0 using the method of undetermined coefficients.\n\n### Finding Roots of the Characteristic Polynomial\n\nThe main difficulty in solving higher-order linear differential equations is finding all the roots of the characteristic polynomial. Can you think of a real-world scenario where finding the roots of the characteristic polynomial is crucial?\n\nFor example, consider an electrical circuit consisting of resistors, capacitors, and inductors. The behavior of the circuit can be modeled using a higher-order linear differential equation, taking into account factors such as the initial voltage, the current, and the impedance. By finding the roots of the characteristic polynomial, we can understand the behavior of the circuit and predict the future voltage and current.\n\n```python\nimport numpy as np\n\n# Define the coefficients of the characteristic polynomial\ncoeffs = [1, -6, 11, -6]\n# Find the roots of the polynomial\nroots = np.roots(coeffs)\nprint(roots)\n```\n\nThis code finds the roots of the characteristic polynomial using the numpy library.\n\n### Introduction to Systems of Differential Equations and Phase Plane Analysis\n\nSystems of Differential Equations and Phase Plane Analysis is a topic in applied mathematics that deals with the analysis of systems of differential equations and their behavior in the phase plane. Can you think of a real-world scenario where systems of differential equations are used to model a complex system?\n\nFor example, consider a population of rabbits and foxes living in a forest. The growth rate of the rabbit population and the fox population can be modeled using a system of differential equations, taking into account factors such as food availability, predation, and disease. By analyzing the behavior of the system in the phase plane, we can understand the dynamics of the ecosystem and predict the future population sizes.\n\n### Phase Plane\n\nThe phase plane is a graphical representation of the solutions of a system of differential equations, where the x and y axes represent the state variables of the system. Can you think of a real-world scenario where the phase plane is used to analyze a complex system?\n\nFor example, consider a mechanical system consisting of a mass attached to a spring. The motion of the mass can be modeled using a system of differential equations, taking into account factors such as the initial displacement, the velocity, and the damping coefficient. By analyzing the behavior of the system in the phase plane, we can understand the motion of the mass and predict the future position.\n\n### Phase Portraits\n\nPhase portraits are used to visualize the behavior of the system, including the stability of equilibrium solutions. Can you think of a real-world scenario where phase portraits are used to analyze a complex system?\n\nFor example, consider an electrical circuit consisting of resistors, capacitors, and inductors. The behavior of the circuit can be modeled using a system of differential equations, taking into account factors such as the initial voltage, the current, and the impedance. By analyzing the phase portrait of the system, we can understand the behavior of the circuit and predict the future voltage and current.\n\n### Equilibrium Solutions\n\nEquilibrium solutions are solutions for which the derivative of the state variables is zero, and they can be classified as asymptotically stable, unstable, or semi-stable. Can you think of a real-world scenario where equilibrium solutions are used to analyze a complex system?\n\nFor example, consider a population of rabbits living in a forest. The growth rate of the population can be modeled using a differential equation, taking into account factors such as food availability, predation, and disease. By finding the equilibrium solutions and analyzing the stability, we can understand the behavior of the population and predict the future population size.\n\n### Stability of Equilibrium Solutions\n\nThe stability of equilibrium solutions can be determined by analyzing the eigenvalues of the system matrix. Can you think of a real-world scenario where the stability of equilibrium solutions is crucial?\n\nFor example, consider a mechanical system consisting of a mass attached to a spring. The motion of the mass can be modeled using a system of differential equations, taking into account factors such as the initial displacement, the velocity, and the damping coefficient. By analyzing the stability of the equilibrium solutions, we can understand the motion of the mass and predict the future position.\n\n```python\nimport numpy as np\n\nA = np.array([[1, 2], [3, 4]])\neigenvalues, eigenvectors = np.linalg.eig(A)\n```\n\nThis code calculates the eigenvalues and eigenvectors of the system matrix using the numpy library.\n\n### Classification of Equilibrium Points\n\nFor a 2x2 linear system with distinct real eigenvalues, the equilibrium point can be a sink, source, or saddle point. Can you think of a real-world scenario where the classification of equilibrium points is used to analyze a complex system?\n\nFor example, consider a population of rabbits and foxes living in a forest. The growth rate of the rabbit population and the fox population can be modeled using a system of differential equations, taking into account factors such as food availability, predation, and disease. By classifying the equilibrium points, we can understand the dynamics of the ecosystem and predict the future population sizes.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef system(x, y):\n    return x + y, x - y\n\nx = np.linspace(-10, 10, 400)\ny = np.linspace(-10, 10, 400)\nX, Y = np.meshgrid(x, y)\nDX, DY = system(X, Y)\nplt.quiver(X, Y, DX, DY)\nplt.show()\n```\n\nThis code plots the phase portrait of the system using the matplotlib library.\n\n### Applications of Phase Plane Analysis\n\nThe phase plane can also be used to visualize the behavior of physical systems, including oscillatory systems such as predator-prey models and chemical reactions. Can you think of a real-world scenario where the phase plane is used to analyze a complex system?\n\nFor example, consider a population of rabbits and foxes living in a forest. The growth rate of the rabbit population and the fox population can be modeled using a system of differential equations, taking into account factors such as food availability, predation, and disease. By analyzing the behavior of the system in the phase plane, we can understand the dynamics of the ecosystem and predict the future population sizes.\n\n```python\nimport numpy as np\nfrom scipy.integrate import odeint\n\ndef predator_prey(state, t):\n    x, y = state\n    dxdt = 0.5 * x - 0.02 * x * y\n    dydt = -0.3 * y + 0.01 * x * y\n    return dxdt, dydt\n\nstate0 = [40, 9]\nt = np.arange(0, 100, 0.1)\nstate = odeint(predator_prey, state0, t)\nimport matplotlib.pyplot as plt\nplt.plot(state[:, 0], state[:, 1])\nplt.show()\n```\n\nThis code plots the phase portrait of the predator-prey model using the matplotlib library.\n\n### Conclusion\n\nIn summary, Systems of Differential Equations and Phase Plane Analysis is a powerful tool for analyzing and visualizing the behavior of complex systems, and it has numerous applications in physics, biology, chemistry, and other fields. By understanding the concepts and methods presented in this course, you will be able to model and analyze complex systems, and make predictions about their behavior.\n\n### Introduction to Numerical Methods\n\nNumerical methods for solving differential equations are essential tools in various fields, providing a way to approximate the solution of differential equations, which is crucial in understanding the behavior of complex systems. Can you think of a real-world scenario where numerical methods are used to solve a complex system?\n\nFor example, consider a population of rabbits living in a forest. The growth rate of the population can be modeled using a differential equation, taking into account factors such as food availability, predation, and disease. By using numerical methods, we can approximate the solution of the differential equation and predict the future population size.\n\n### Euler's Method\n\nEuler's method is the simplest example of a Runge-Kutta method, which approximates the solution of differential equations by approximating the slope of the secant line. It uses the formula: y(n+1) = y(n) + h * f(x(n), y(n)). Let's consider an example:\n\n```python\ndef euler_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        y.append(y[i] + h * f(x[i], y[i]))\n    return x, y\n```\n\nThis code implements Euler's method to approximate the solution of a differential equation.\n\n### Improved Euler's Method\n\nThe improved Euler's method is a variation of Euler's method that uses the average slope of the secant line to improve the accuracy of the approximation. It uses the formula: y(n+1) = y(n) + 0.5 * h * (f(x(n), y(n)) + f(x(n+1), y(n) + h * f(x(n), y(n)))). Let's consider an example:\n\n```python\ndef improved_euler_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        y_pred = y[i] + h * f(x[i], y[i])\n        y.append(y[i] + 0.5 * h * (f(x[i], y[i]) + f(x[i+1], y_pred)))\n    return x, y\n```\n\nThis code implements the improved Euler's method to approximate the solution of a differential equation.\n\n### Third-Order Runge-Kutta Method\n\nThe third-order Runge-Kutta method is a popular method used to approximate the solution of differential equations. It uses the formula: y(n+1) = y(n) + (1/6) * h * (k1 + 4 * k2 + k3), where k1, k2, and k3 are the slopes of the secant line at different points. Let's consider an example:\n\n```python\ndef third_order_runge_kutta_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        k1 = f(x[i], y[i])\n        k2 = f(x[i] + 0.5 * h, y[i] + 0.5 * h * k1)\n        k3 = f(x[i] + h, y[i] - k1 * h + 2 * k2 * h)\n        y.append(y[i] + (1/6) * h * (k1 + 4 * k2 + k3))\n    return x, y\n```\n\nThis code implements the third-order Runge-Kutta method to approximate the solution of a differential equation.\n\n### Runge-Kutta-Fehlberg Methods\n\nThe Runge-Kutta-Fehlberg methods are a family of methods that use a combination of the Runge-Kutta method and the Fehlberg method to approximate the solution of differential equations. They are known for their high accuracy and stability. Let's consider an example:\n\n```python\ndef runge_kutta_fehlberg_method(f, x0, y0, h, n):\n    x = [x0 + i * h for i in range(n+1)]\n    y = [y0]\n    for i in range(n):\n        k1 = f(x[i], y[i])\n        k2 = f(x[i] + (1/4) * h, y[i] + (1/4) * h * k1)\n        k3 = f(x[i] + (3/8) * h, y[i] + (3/32) * h * k1 + (9/32) * h * k2)\n        k4 = f(x[i] + (12/13) * h, y[i] + (1932/2197) * h * k1 - (7200/2197) * h * k2 + (7296/2197) * h * k3)\n        k5 = f(x[i] + h, y[i] + (439/216) * h * k1 - 8 * h * k2 + (3680/513) * h * k3 - (845/4104) * h * k4)\n        k6 = f(x[i] + (1/2) * h, y[i] - (8/27) * h * k1 + 2 * h * k2 - (3544/2565) * h * k3 + (1859/4104) * h * k4 - (11/40) * h * k5)\n        y.append(y[i] + (25/216) * h * k1 + (1408/2565) * h * k3 + (2197/4104) * h * k4 - (1/5) * h * k5)\n    return x, y\n```\n\nThis code implements the Runge-Kutta-Fehlberg method to approximate the solution of a differential equation.",
    "## Introduction to Differential Equations\nDifferential equations are a crucial part of mathematics and are used to model various physical phenomena. They involve an unknown function of one or more independent variables and their derivatives. In this section, we will introduce the concept of ordinary differential equations (ODEs) and their importance in modeling real-world problems.\n\nTo start with, let's consider a simple example. Imagine you are throwing a ball upwards from the ground. The ball's height at any given time can be modeled using a differential equation. The equation would involve the ball's initial velocity, the acceleration due to gravity, and the time it has been in the air.\n\n## Classification of Differential Equations\nDifferential equations can be classified into two main categories: ordinary differential equations (ODEs) and partial differential equations (PDEs). ODEs involve functions of one independent variable, while PDEs involve functions of multiple independent variables.\n\nFor instance, consider a population growth model. The population size at any given time can be modeled using an ODE, as it depends on the initial population size, the growth rate, and time. On the other hand, a model of heat transfer in a two-dimensional space would involve a PDE, as it depends on the temperature, the thermal conductivity of the material, and the spatial coordinates.\n\n## First-Order Linear Equations\nA first-order linear equation is of the form dy/dx + P(x)y = Q(x), where P(x) and Q(x) are functions of x. These equations can be solved using an integrating factor.\n\nTo illustrate this, let's consider the following example:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x) + 2*x*f(x), 3*x)\nsol = sp.dsolve(eq)\nprint(sol)\n```\nThis code solves the differential equation y' + 2xy = 3x using SymPy.\n\n## Separable, Exact, and Bernoulli Differential Equations\nSeparable equations can be written in the form dy/dx = f(x)/g(y), while exact equations satisfy the condition \u2202M/\u2202y = \u2202N/\u2202x. Bernoulli equations are of the form dy/dx + P(x)y = Q(x)y^n.\n\nFor example, consider the differential equation dy/dx = 2x. We can separate the variables by multiplying both sides by dx and dividing by 2x, giving us dy/2x = dx. Integrating both sides, we get \u222bdy = 2\u222bxdx.\n\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ny = sp.symbols('y')\neq = sp.Eq(y*(x**2 + 1), x*y**2)\nsol = sp.dsolve(eq)\nprint(sol)\n```\nThis code solves the differential equation y(x^2 + 1) = xy^2 using SymPy.\n\n## Euler's Method\nEuler's method is a numerical method used to approximate the solution of a differential equation. It works by iteratively applying the equation y'(x) = f(x,y) to find the next value of y.\n\nTo demonstrate this, let's consider the following example:\n```python\nimport numpy as np\n\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        x[i+1] = x[i] + h\n        y[i+1] = y[i] + h * f(x[i], y[i])\n    return x, y\n```\nThis code implements Euler's method to approximate the solution of a differential equation.\n\n## Higher-Order Linear ODEs\nHigher-order linear ODEs are of the form a_n*y^(n) + a_(n-1)*y^(n-1) + ... + a_1*y' + a_0*y = f(x), where a_i are constants.\n\nFor instance, consider the differential equation y'' + 2y' + y = 0. We can solve this equation using the following code:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x, 2) + 2*f(x).diff(x) + f(x), 0)\nsol = sp.dsolve(eq)\nprint(sol)\n```\nThis code solves the differential equation y'' + 2y' + y = 0 using SymPy.\n\n## Laplace Transform and Power Series Methods\nThe Laplace transform is a method used to solve differential equations by transforming the equation into a simpler form. The power series method involves representing the solution as a power series and finding the coefficients.\n\nTo illustrate this, let's consider the following example:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ns = sp.symbols('s')\nf = sp.Function('f')\neq = sp.Eq(f(x).diff(x), -f(x))\nF = sp.laplace_transform(f(x), x, s)\nprint(F)\n```\nThis code applies the Laplace transform to the differential equation y' = -y using SymPy.\n\n## Introduction to Separation of Variables\nSeparation of Variables is a method for solving differential equations by separating the variables. The general form of a separable differential equation is N(y)dy/dx = M(x), where N(y) is a function of y and M(x) is a function of x.\n\nFor example, consider the differential equation dy/dx = 2x. We can separate the variables by multiplying both sides by dx and dividing by 2x, giving us dy/2x = dx. Integrating both sides, we get \u222bdy = 2\u222bxdx.\n\n## General Form of Separable Differential Equations\nTo solve this type of equation, we integrate both sides with respect to x, which gives us \u222bN(y)dy = \u222bM(x)dx. This method is used to solve various types of first-order differential equations.\n\n## Solution Process for Separable Differential Equations\nThe solution process involves rearranging the equation so that everything involving y appears on one side, and everything involving x appears on the other. The equation can then be integrated directly, yielding a solution for y(x).\n\nTo demonstrate this, let's consider the following example:\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\ny = sp.symbols('y')\ndy_dx = 2*x\n# Separate the variables and integrate\ndy = sp.integrate(dy_dx, x)\nprint(dy)\n```\nThis code separates the variables and integrates the differential equation dy/dx = 2x using SymPy.\n\n## First-Order Differential Equations\nIn addition to separable differential equations, the topic of First-Order Differential Equations covers other important concepts, such as intervals of validity, equilibrium solutions, and Euler's Method.\n\nFor instance, consider the differential equation dy/dx = f(x,y). The equilibrium solutions are the values of y for which f(x,y) = 0.\n\n## Intervals of Validity and Equilibrium Solutions\nIntervals of validity refer to the range of values for which a solution to a differential equation is valid, while equilibrium solutions are solutions that do not change over time.\n\nTo illustrate this, let's consider the following example:\n```python\nimport numpy as np\n\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        y[i+1] = y[i] + h * f(x[i], y[i])\n        x[i+1] = x[i] + h\n    return x, y\n```\nThis code implements Euler's method to approximate the solution of a differential equation.\n\n## Introduction to Higher-Order Linear Differential Equations\nHigher-order linear differential equations are extensions of the concepts and methods used to solve 2nd order linear differential equations. These equations appear in various applications, although 2nd order equations are more common.\n\nFor example, consider the differential equation y'' + 2y' + y = 0. We can solve this equation using the following code:\n```python\nimport sympy as sp\n\ny = sp.Function('y')\neq = y(0).diff(3) - 6*y(0).diff(2) + 11*y(0).diff(1) - 6*y(0)\nprint(sp.dsolve(eq, y(0)))\n```\nThis code solves the differential equation y''' - 6y'' + 11y' - 6y = 0 using SymPy.\n\n## Key Concepts and Methods\nThe basic results and concepts, such as the Principle of Superposition, linearly independent functions, and the Wronskian, are similar to those for 2nd order equations, but with some natural extensions.\n\nTo illustrate this, let's consider the following example:\n```python\nimport numpy as np\n\n# Define the coefficients of the characteristic polynomial\ncoeffs = [1, -6, 11, -6]\n# Find the roots of the polynomial\nroots = np.roots(coeffs)\nprint(roots)\n```\nThis code finds the roots of the characteristic polynomial of a higher-order linear differential equation.\n\n## Introduction to Systems of Differential Equations and Phase Plane Analysis\nSystems of Differential Equations and Phase Plane Analysis is a topic in applied mathematics that deals with the analysis of systems of differential equations and their behavior in the phase plane.\n\nFor example, consider the system of differential equations dx/dt = x + y, dy/dt = x - y. We can analyze the behavior of this system using the following code:\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef system(x, y):\n    return x + y, x - y\n\nx = np.linspace(-10, 10, 400)\ny = np.linspace(-10, 10, 400)\nX, Y = np.meshgrid(x, y)\nDX, DY = system(X, Y)\n\nplt.quiver(X, Y, DX, DY)\nplt.show()\n```\nThis code visualizes the phase portrait of the system of differential equations.\n\n## Phase Portraits\nPhase portraits are used to visualize the behavior of the system, including the stability of equilibrium solutions.\n\nTo illustrate this, let's consider the following example:\n```python\nimport numpy as np\nfrom scipy.integrate import odeint\n\ndef predator_prey(state, t):\n    x, y = state\n    dxdt = 0.5 * x - 0.02 * x * y\n    dydt = -0.3 * y + 0.01 * x * y\n    return dxdt, dydt\n\nstate0 = [40, 9]\nt = np.arange(0, 100, 0.1)\nstate = odeint(predator_prey, state0, t)\n\nimport matplotlib.pyplot as plt\nplt.plot(state[:, 0], state[:, 1])\nplt.show()\n```\nThis code visualizes the phase portrait of the predator-prey model.\n\n## Conclusion\nIn summary, differential equations are a powerful tool for modeling and analyzing complex systems. The concepts and methods presented in this lecture, including separation of variables, Euler's method, and phase plane analysis, are essential for understanding and solving differential equations.\n\nTo further illustrate the concepts, let's consider the following example:\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef euler_method(f, x0, y0, h, n):\n    x = np.zeros(n+1)\n    y = np.zeros(n+1)\n    x[0] = x0\n    y[0] = y0\n    for i in range(n):\n        y[i+1] = y[i] + h * f(x[i], y[i])\n        x[i+1] = x[i] + h\n    return x, y\n\ndef f(x, y):\n    return x + y\n\nx0, y0 = 0, 1\nh = 0.1\nn = 100\n\nx, y = euler_method(f, x0, y0, h, n)\n\nplt.plot(x, y)\nplt.show()\n```\nThis code implements Euler's method to approximate the solution of a differential equation and visualizes the result."
  ]
}
